<!doctype html><html lang="zh-CN"><head><meta charset="utf-8" /><meta name="viewport" content="width=device-width,initial-scale=1" /><meta name="generator" content="VuePress 2.0.0-rc.26" /><meta name="theme" content="VuePress Theme Plume 1.0.0-rc.180" /><script id="check-mac-os">document.documentElement.classList.toggle('mac', /Mac|iPhone|iPod|iPad/i.test(navigator.platform))</script><script id="check-dark-mode">;(function () {const um= localStorage.getItem('vuepress-theme-appearance') || 'auto';const sm = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;const isDark = um === 'dark' || (um !== 'light' && sm);document.documentElement.dataset.theme = isDark ? 'dark' : 'light';})();</script><link rel="icon" type="image/svg+xml" href="/assests/jade_light.svg"><title>Inside vLLM Anatomy of a High-Throughput LLM Inference System | JinBlog</title><meta name="description" content="走在未知的道路上, 不许停也不能回头"><link rel="preload" href="/JinBlog/assets/style-Dud8xOD9.css" as="style"><link rel="stylesheet" href="/JinBlog/assets/style-Dud8xOD9.css"><link rel="modulepreload" href="/JinBlog/assets/app-AuyBpmKP.js"><link rel="modulepreload" href="/JinBlog/assets/index.html-DIJrlZuB.js"></head><body><div id="app"><!--[--><!--[--><div class="theme-plume vp-layout" vp-container data-v-6fcf39b7><!--[--><!--[--><!--]--><!--[--><span tabindex="-1" data-v-a1e390b2></span><a href="#VPContent" class="vp-skip-link visually-hidden" data-v-a1e390b2> Skip to content </a><!--]--><!----><header class="vp-nav" data-v-6fcf39b7 data-v-04ce2e5b><div class="vp-navbar" vp-navbar data-v-04ce2e5b data-v-43d9cf9e><div class="wrapper" data-v-43d9cf9e><div class="container" data-v-43d9cf9e><div class="title" data-v-43d9cf9e><div class="vp-navbar-title has-sidebar" data-v-43d9cf9e data-v-6c4c1c9c><a class="vp-link link no-icon title" href="/JinBlog/" data-v-6c4c1c9c><!--[--><!--[--><!--]--><!--[--><!--[--><!--[--><img class="vp-image dark logo" style="" src="/JinBlog/assests/jade_light.svg" alt data-v-264f5058><!--]--><!--[--><img class="vp-image light logo" style="" src="/JinBlog/assests/jade_light.svg" alt data-v-264f5058><!--]--><!--]--><!--]--><span data-v-6c4c1c9c>JinBlog</span><!--[--><!--]--><!--]--></a></div></div><div class="content" data-v-43d9cf9e><div class="content-body" data-v-43d9cf9e><!--[--><!--]--><div class="vp-navbar-search search" data-v-43d9cf9e><div class="search-wrapper" data-v-47ff7b1a><!----><div id="local-search" data-v-47ff7b1a><button type="button" class="mini-search mini-search-button" aria-label="搜索文档" data-v-47ff7b1a><span class="mini-search-button-container"><span class="mini-search-search-icon vpi-mini-search" aria-label="search icon"></span><span class="mini-search-button-placeholder">搜索文档</span></span><span class="mini-search-button-keys"><kbd class="mini-search-button-key"></kbd><kbd class="mini-search-button-key">K</kbd></span></button></div></div></div><!--[--><!--]--><nav aria-labelledby="main-nav-aria-label" class="vp-navbar-menu menu" data-v-43d9cf9e data-v-098c5d7d><span id="main-nav-aria-label" class="visually-hidden" data-v-098c5d7d>Main Navigation</span><!--[--><!--[--><a class="vp-link link navbar-menu-link" href="/JinBlog/" tabindex="0" data-v-098c5d7d data-v-6cfe1038><!--[--><!----><span data-v-6cfe1038>首页</span><!----><!--]--></a><!--]--><!--[--><a class="vp-link link navbar-menu-link" href="/JinBlog/blog/posts/" tabindex="0" data-v-098c5d7d data-v-6cfe1038><!--[--><!----><span data-v-6cfe1038>博客</span><!----><!--]--></a><!--]--><!--[--><a class="vp-link link navbar-menu-link" href="/JinBlog/fzu_cs_course/" tabindex="0" data-v-098c5d7d data-v-6cfe1038><!--[--><!----><span data-v-6cfe1038>FZU CS课程</span><!----><!--]--></a><!--]--><!--[--><a class="vp-link link navbar-menu-link active" href="/JinBlog/dl_llm/vllm-anatomy/" tabindex="0" data-v-098c5d7d data-v-6cfe1038><!--[--><!----><span data-v-6cfe1038>DL-LLM</span><!----><!--]--></a><!--]--><!--[--><a class="vp-link link navbar-menu-link" href="/JinBlog/csdiy/10-414/" tabindex="0" data-v-098c5d7d data-v-6cfe1038><!--[--><!----><span data-v-6cfe1038>CSDIY公开课</span><!----><!--]--></a><!--]--><!--[--><a class="vp-link link navbar-menu-link" href="/JinBlog/tools/linux-install/" tabindex="0" data-v-098c5d7d data-v-6cfe1038><!--[--><!----><span data-v-6cfe1038>工具分享</span><!----><!--]--></a><!--]--><!--]--></nav><!--[--><!--]--><!----><div class="vp-navbar-appearance appearance" data-v-43d9cf9e data-v-e59180ad><button class="vp-switch vp-switch-appearance" type="button" role="switch" title aria-checked="false" data-v-e59180ad data-v-3baa2a62 data-v-4dd2fff8><span class="check" data-v-4dd2fff8><span class="icon" data-v-4dd2fff8><!--[--><span class="vpi-sun sun" data-v-3baa2a62></span><span class="vpi-moon moon" data-v-3baa2a62></span><!--]--></span></span></button></div><div class="vp-social-links vp-navbar-social-links social-links" data-v-43d9cf9e data-v-967cd3fa data-v-4bcc4c24><!--[--><a class="vp-social-link no-icon" href="https://github.com/yJader/JinBlog" aria-label="github" target="_blank" rel="noopener" data-v-4bcc4c24 data-v-961e560d><span class="vpi-social-github" /></a><!--]--></div><div class="vp-flyout vp-navbar-extra extra" data-v-43d9cf9e data-v-3d0b13b6 data-v-d83556c7><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-d83556c7><span class="vpi-more-horizontal icon" data-v-d83556c7></span></button><div class="menu" data-v-d83556c7><div class="vp-menu" data-v-d83556c7 data-v-77899dba><!----><!--[--><!--[--><!----><div class="group" data-v-3d0b13b6><div class="item appearance" data-v-3d0b13b6><p class="label" data-v-3d0b13b6>外观</p><div class="appearance-action" data-v-3d0b13b6><button class="vp-switch vp-switch-appearance" type="button" role="switch" title aria-checked="false" data-v-3d0b13b6 data-v-3baa2a62 data-v-4dd2fff8><span class="check" data-v-4dd2fff8><span class="icon" data-v-4dd2fff8><!--[--><span class="vpi-sun sun" data-v-3baa2a62></span><span class="vpi-moon moon" data-v-3baa2a62></span><!--]--></span></span></button></div></div></div><div class="group" data-v-3d0b13b6><div class="item social-links" data-v-3d0b13b6><div class="vp-social-links social-links-list" data-v-3d0b13b6 data-v-4bcc4c24><!--[--><a class="vp-social-link no-icon" href="https://github.com/yJader/JinBlog" aria-label="github" target="_blank" rel="noopener" data-v-4bcc4c24 data-v-961e560d><span class="vpi-social-github" /></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="vp-navbar-hamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="nav-screen" data-v-43d9cf9e data-v-6c75f11b><span class="container" data-v-6c75f11b><span class="top" data-v-6c75f11b></span><span class="middle" data-v-6c75f11b></span><span class="bottom" data-v-6c75f11b></span></span></button></div></div></div></div><div class="divider" data-v-43d9cf9e><div class="divider-line" data-v-43d9cf9e></div></div></div><!----></header><div class="vp-local-nav reached-top" data-v-6fcf39b7 data-v-9d9c2ce3><button class="menu" aria-expanded="false" aria-controls="SidebarNav" data-v-9d9c2ce3><span class="vpi-align-left menu-icon" data-v-9d9c2ce3></span><span class="menu-text" data-v-9d9c2ce3>Menu</span></button><div class="vp-local-nav-outline-dropdown" style="--vp-vh:0px;" data-v-9d9c2ce3 data-v-f64fbe72><!----><!----></div></div><aside class="vp-sidebar" vp-sidebar data-v-6fcf39b7 data-v-f3fda0df><div class="curtain" data-v-f3fda0df></div><nav id="SidebarNav" class="nav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-f3fda0df><span id="sidebar-aria-label" class="visually-hidden" data-v-f3fda0df> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="no-transition group" data-v-87fea307><section class="vp-sidebar-item sidebar-item level-0 has-active" data-v-87fea307 data-v-23b58e2e><!----><div data-v-23b58e2e data-v-23b58e2e><div class="items" data-v-23b58e2e><!--[--><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-23b58e2e data-v-23b58e2e><div class="item" data-v-23b58e2e><div class="indicator" data-v-23b58e2e></div><!----><a class="vp-link link link" href="/JinBlog/dl_llm/vllm-anatomy/" data-v-23b58e2e><!--[--><p class="text" data-v-23b58e2e><span data-v-23b58e2e>vLLM: 高吞吐量大语言模型推理系统剖析</span><!----></p><!--]--></a><!----></div><!----></div><!--]--></div></div></section></div><div class="no-transition group" data-v-87fea307><section class="vp-sidebar-item sidebar-item level-0 is-link" data-v-87fea307 data-v-23b58e2e><div class="item" tabindex="0" data-v-23b58e2e><div class="indicator" data-v-23b58e2e></div><!----><a class="vp-link link link" href="/JinBlog/csdiy/10-414/" data-v-23b58e2e><!--[--><h2 class="text" data-v-23b58e2e><span data-v-23b58e2e>CMU 10-414 Deep Learning Systems</span><!----></h2><!--]--></a><!----></div><div data-v-23b58e2e data-v-23b58e2e><div class="items" data-v-23b58e2e><!--[--><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-23b58e2e data-v-23b58e2e><div class="item" data-v-23b58e2e><div class="indicator" data-v-23b58e2e></div><!----><a class="vp-link link link" href="/JinBlog/csdiy/10-414-homework/" data-v-23b58e2e><!--[--><p class="text" data-v-23b58e2e><span data-v-23b58e2e>Homework笔记</span><!----></p><!--]--></a><!----></div><!----></div><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-23b58e2e data-v-23b58e2e><div class="item" data-v-23b58e2e><div class="indicator" data-v-23b58e2e></div><!----><a class="vp-link link link" href="/JinBlog/csdiy/10-414-notes/" data-v-23b58e2e><!--[--><p class="text" data-v-23b58e2e><span data-v-23b58e2e>课程笔记</span><!----></p><!--]--></a><!----></div><!----></div><!--]--></div></div></section></div><!--]--><!--[--><!--]--></nav></aside><!--[--><div id="VPContent" vp-content class="vp-content has-sidebar" data-v-6fcf39b7 data-v-743d3d68><div class="vp-doc-container has-sidebar has-aside" data-v-743d3d68 data-v-50e98be9><!--[--><!--]--><div class="container" data-v-50e98be9><div class="aside" vp-outline data-v-50e98be9><div class="aside-curtain" data-v-50e98be9></div><div class="aside-container" data-v-50e98be9><div class="aside-content" data-v-50e98be9><div class="vp-doc-aside" data-v-50e98be9 data-v-0fdc8756><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="vp-doc-aside-outline" role="navigation" data-v-0fdc8756 data-v-fe5fe191><div class="content" data-v-fe5fe191><div class="outline-marker" data-v-fe5fe191></div><div id="doc-outline-aria-label" aria-level="2" class="outline-title" role="heading" data-v-fe5fe191><span data-v-fe5fe191>此页内容</span><span class="vpi-print icon" data-v-fe5fe191></span></div><ul class="root" data-v-fe5fe191 data-v-b4b17828><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-0fdc8756></div><!--[--><!--]--></div></div></div></div><div class="content" data-v-50e98be9><div class="content-container" data-v-50e98be9><!--[--><!--]--><main class="main" data-v-50e98be9><nav class="vp-breadcrumb" data-v-50e98be9 data-v-3472e66e><ol vocab="https://schema.org/" typeof="BreadcrumbList" data-v-3472e66e><!--[--><li property="itemListElement" typeof="ListItem" data-v-3472e66e><a class="vp-link link no-icon breadcrumb" href="/JinBlog/" property="item" typeof="WebPage" data-v-3472e66e><!--[-->首页<!--]--></a><span class="vpi-chevron-right" data-v-3472e66e></span><meta property="name" content="首页" data-v-3472e66e><meta property="position" content="1" data-v-3472e66e></li><li property="itemListElement" typeof="ListItem" data-v-3472e66e><span class="vp-link no-icon breadcrumb" property="item" typeof="WebPage" data-v-3472e66e><!--[-->DL-LLM<!--]--></span><span class="vpi-chevron-right" data-v-3472e66e></span><meta property="name" content="DL-LLM" data-v-3472e66e><meta property="position" content="2" data-v-3472e66e></li><li property="itemListElement" typeof="ListItem" data-v-3472e66e><a class="vp-link link no-icon breadcrumb current" href="/JinBlog/dl_llm/vllm-anatomy/" property="item" typeof="WebPage" data-v-3472e66e><!--[-->Inside vLLM Anatomy of a High-Throughput LLM Inference System<!--]--></a><!----><meta property="name" content="Inside vLLM Anatomy of a High-Throughput LLM Inference System" data-v-3472e66e><meta property="position" content="3" data-v-3472e66e></li><!--]--></ol></nav><!--[--><!--]--><!--[--><div class="vp-doc-title" data-v-1b67cc8d><!--[--><!--]--><h1 class="page-title" data-v-1b67cc8d><!----> Inside vLLM Anatomy of a High-Throughput LLM Inference System <!----></h1><!--[--><!--]--></div><div class="vp-doc-meta" data-v-1b67cc8d><!--[--><!--]--><p class="reading-time" data-v-1b67cc8d><span class="vpi-books icon" data-v-1b67cc8d></span><span data-v-1b67cc8d>约 12673 字</span><span data-v-1b67cc8d>大约 42 分钟</span></p><p data-v-1b67cc8d><span class="vpi-tag icon" data-v-1b67cc8d></span><!--[--><span class="vp-link tag vp-tag-q0mb" data-v-1b67cc8d><!--[-->LLM<!--]--></span><span class="vp-link tag vp-tag-g5dj" data-v-1b67cc8d><!--[-->Inference<!--]--></span><span class="vp-link tag vp-tag-vj4n" data-v-1b67cc8d><!--[-->Systems<!--]--></span><!--]--></p><!--[--><!--]--><p class="create-time" data-v-1b67cc8d><span class="vpi-clock icon" data-v-1b67cc8d></span><span data-v-1b67cc8d>2025-10-18</span></p></div><!--]--><!--[--><!--]--><div class="_dl_llm_vllm-anatomy_ external-link-icon-enabled vp-doc plume-content" vp-content data-v-50e98be9><!--[--><!--]--><div data-v-50e98be9><div class="hint-container note"><p class="hint-container-title">注</p><p>最初发布于 <a href="https://www.aleksagordic.com/blog/vllm" title="null" target="_blank" rel="noopener noreferrer">Aleksa Gordic 的网站</a></p><p>引用自<a href="https://blog.vllm.ai/2025/09/05/anatomy-of-vllm.html" target="_blank" rel="noopener noreferrer">Inside vLLM: Anatomy of a High-Throughput LLM Inference System | vLLM Blog</a> 我在AI的协助下进行了全文翻译</p></div><h3 id="从-paged-attention、continuous-batching、prefix-caching、speculative-decoding-等到多-gpu、多节点大规模动态服务" tabindex="-1"><a class="header-anchor" href="#从-paged-attention、continuous-batching、prefix-caching、speculative-decoding-等到多-gpu、多节点大规模动态服务"><span>从 Paged Attention、Continuous Batching、Prefix Caching、Speculative Decoding 等到多 GPU、多节点大规模动态服务</span></a></h3><p>在这篇文章中，我将逐步介绍构成现代高吞吐量 LLM 推理系统的所有核心系统组件和高级功能。我将特别详细分析 vLLM [1] 的工作原理。</p><p>这篇文章是一个系列的第一篇。它从宏观入手，然后层层深入细节（遵循倒金字塔方法），这样你可以在不陷入细枝末节的情况下，形成一个对整个系统准确的高层次心智模型。</p><p>后续的文章将深入探讨特定的子系统。</p><p>本文分为五个部分：</p><ol><li><strong>LLM Engine &amp; Engine Core</strong>：vLLM 的基础知识（调度、Paged Attention、Continuous Batching 等）</li><li><strong>高级功能</strong>：扩展核心引擎逻辑（Chunked Prefill、Prefix Caching、Guided &amp; Speculative Decoding、Disaggregated P/D）</li><li><strong>扩展</strong>：从单 GPU 到多 GPU 执行</li><li><strong>服务层</strong>：分布式/并发 Web 服务架构</li><li><strong>基准测试和自动调优</strong>：衡量延迟和吞吐量</li></ol><div class="hint-container note"><p class="hint-container-title">注</p><ul><li>分析基于 <a href="https://github.com/vllm-project/vllm/tree/42172ad" title="null" target="_blank" rel="noopener noreferrer">commit 42172ad</a>（2025 年 8 月 9 日）。</li><li>目标读者：对最先进的 LLM 引擎工作原理感到好奇的任何人，以及有兴趣为 vLLM、SGLang 等做出贡献的人。</li><li>我将重点关注 <a href="https://docs.vllm.ai/en/latest/usage/v1_guide.html" title="null" target="_blank" rel="noopener noreferrer">V1 引擎</a>。我也研究了 V0（现已<a href="https://github.com/vllm-project/vllm/issues/18571" title="null" target="_blank" rel="noopener noreferrer">弃用</a>），这对于理解项目的演变很有价值，许多概念仍然适用。</li><li>关于 LLM Engine / Engine Core 的第一部分可能有点让人不知所措/枯燥——但博客的其余部分有大量的例子和图示。😃</li></ul></div><h2 id="llm-engine-engine-core" tabindex="-1"><a class="header-anchor" href="#llm-engine-engine-core"><span>LLM Engine &amp; Engine Core</span></a></h2><p>LLM 引擎是 vLLM 的基本构建块。它本身就已经可以实现高吞吐量推理——但仅限于离线设置。你还不能通过网络向客户提供服务。</p><p>我们将使用以下离线推理代码片段作为我们的运行示例（改编自 <a href="https://github.com/vllm-project/vllm/blob/main/examples/offline_inference/basic/basic.py" title="null" target="_blank" rel="noopener noreferrer">basic.py</a>）。</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" style="--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212;"><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code class="language-python"><span class="line"><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">from</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> vllm </span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">import</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> LLM</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> SamplingParams</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">prompts </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#999999;--shiki-dark:#666666;"> [</span></span>
<span class="line"><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">    &quot;</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">Hello, my name is</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span></span>
<span class="line"><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">    &quot;</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">The president of the United States is</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span></span>
<span class="line"><span style="--shiki-light:#999999;--shiki-dark:#666666;">]</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">sampling_params </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> SamplingParams</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;">temperature</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">0.8</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;"> top_p</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">0.95</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">def</span><span style="--shiki-light:#59873A;--shiki-dark:#80A665;"> main</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">():</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">    llm </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> LLM</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;">model</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">TinyLlama/TinyLlama-1.1B-Chat-v1.0</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">    outputs </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> llm</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">generate</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">prompts</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> sampling_params</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">if</span><span style="--shiki-light:#998418;--shiki-dark:#B8A965;"> __name__</span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;"> ==</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;"> &quot;</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">__main__</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">:</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">    main</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">()</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="hint-container note"><p class="hint-container-title">注</p><p>环境变量：</p><ul><li>VLLM_USE_V1=”1” # 我们使用的是 V1 引擎</li><li>VLLM_ENABLE_V1_MULTIPROCESSING=”0” # 我们在单个进程中运行</li></ul></div><p>此配置为：</p><ul><li>离线（没有 Web/分布式系统架构）</li><li>同步（所有执行都在单个阻塞进程中进行）</li><li>单 GPU（没有数据/模型/流水线/专家并行；DP/TP/PP/EP = 1）</li><li>使用标准 Transformer [2]（支持像 Jamba 这样的混合模型需要更复杂的混合 KV-cache 内存分配器）</li></ul><p>从这里开始，我们将逐步构建一个在线、异步、多 GPU、多节点的推理系统——但仍然服务于一个标准的 Transformer。</p><p>在这个例子中，我们做了两件事：</p><ol><li>实例化一个引擎</li><li>调用它的 <code>generate</code> 方法，根据给定的prompts进行采样</li></ol><p>让我们从分析构造函数开始。</p><h3 id="llm-引擎构造函数" tabindex="-1"><a class="header-anchor" href="#llm-引擎构造函数"><span>LLM 引擎构造函数</span></a></h3><p>引擎的主要组件是：</p><ul><li><strong>vLLM 配置</strong>（包含配置模型、缓存、并行性等的所有选项）</li><li><strong>处理器</strong>（通过验证、分词和处理将原始输入转换为 <code>EngineCoreRequests</code>）</li><li><strong>Engine Core 客户端</strong>（在我们的运行示例中，我们使用的是 <code>InprocClient</code>，它基本上等同于 <code>EngineCore</code>；我们将逐步升级到 <code>DPLBAsyncMPClient</code>，它允许大规模服务）</li><li><strong>输出处理器</strong>（将原始的 <code>EngineCoreOutputs</code> 转换为用户看到的 <code>RequestOutput</code>）</li></ul><div class="hint-container note"><p class="hint-container-title">注</p><p>随着 V0 引擎被弃用，类名和细节可能会发生变化。我将强调核心思想而不是确切的签名。我会抽象掉一些但不是所有的细节。</p></div><p>Engine Core 本身由几个子组件组成：</p><ul><li><strong>Model Executor</strong>（驱动模型的前向传播，我们目前处理的是 <code>UniProcExecutor</code>，它在单个 GPU 上有一个 <code>Worker</code> 进程）。我们将逐步升级到支持多个 GPU 的 <code>MultiProcExecutor</code>。</li><li><strong>Structured Output Manager</strong>（用于 Guided Decoding - 我们稍后会介绍）</li><li><strong>Scheduler</strong>（决定哪些请求进入下一个引擎步骤）- 它进一步包含： <ul><li>策略设置 - 可以是 <strong>FCFS</strong>（先到先得）或 <strong>priority</strong>（高优先级的请求先被服务）</li><li><code>waiting</code> 和 <code>running</code> 队列</li><li><strong>KV Cache Manager</strong> - Paged Attention [3] 的核心</li></ul></li></ul><p>KV-cache 管理器维护一个 <code>free_block_queue</code> - 一个可用的 KV-cache 块池（通常有几十万个，取决于 VRAM 大小和块大小）。在 Paged Attention 期间，这些块作为索引结构，将 token 映射到其计算出的 KV cache 块。</p><p><img src="/JinBlog/assets/engine_constructor-DXQA2JWn.png" alt="图 1：本节描述的核心组件及其关系"></p><blockquote><p>[!NOTE]对于一个标准的 Transformer 层（非 MLA [4]），块大小计算如下： 2 (key/value) <em><code>block_size</code> (默认=16)</em> <code>num_kv_heads</code> <em><code>head_size</code></em> <code>dtype_num_bytes</code> (例如 bf16 为 2)</p></blockquote><p>在 Model Executor 构建期间，会创建一个 <code>Worker</code> 对象，并执行三个关键过程。（稍后，使用 <code>MultiProcExecutor</code> 时，这些相同的过程会在不同 GPU 上的每个 worker 进程中独立运行。）</p><ol><li><p><strong>初始化设备</strong>：</p><ul><li>将 CUDA 设备（例如 &quot;cuda:0&quot;）分配给 worker 并检查模型数据类型是否受支持（例如 bf16）</li><li>验证在给定的 <code>gpu_memory_utilization</code>（例如 0.8 -&gt; 80% 的总 VRAM）下是否有足够的 VRAM</li><li>设置分布式设置（DP / TP / PP / EP 等）</li><li>实例化一个 <code>model_runner</code>（包含采样器、KV cache 和前向传播缓冲区，如 <code>input_ids</code>、<code>positions</code> 等）</li><li>实例化一个 <code>InputBatch</code> 对象（包含 CPU 端的前向传播缓冲区、用于 KV-cache 索引的块表、采样元数据等）</li></ul></li><li><p><strong>加载模型</strong>：</p><ul><li>实例化模型架构</li><li>加载模型权重</li><li>调用 model.eval()（PyTorch 的推理模式）</li><li>可选：在模型上调用 torch.compile()</li></ul></li><li><p><strong>初始化 KV cache</strong></p><ul><li>获取每层的 KV-cache 规范。以前这总是 <code>FullAttentionSpec</code>（同构 Transformer），但随着混合模型（滑动窗口、像 Jamba 这样的 Transformer/SSM）的出现，它变得更加复杂（参见 Jenga [5]）</li><li>运行一个虚拟/性能分析的前向传播，并获取 GPU 内存快照，以计算可用 VRAM 中可以容纳多少 KV cache 块</li><li>分配、重塑并将 KV cache 张量绑定到注意力层</li><li>准备注意力元数据（例如将后端设置为 FlashAttention），稍后在 fwd pass 期间由 kernel 消耗</li><li>除非提供了 <code>--enforce-eager</code>，否则对于每个预热批次大小，进行一次<strong>虚拟运行</strong>并捕获 CUDA graphs。CUDA graphs 将整个 GPU 工作序列记录到一个 DAG 中。稍后在 fwd pass 期间，我们启动/重放预先准备好的 graphs，从而减少 kernel 启动开销，进而提高延迟。</li></ul></li></ol><p>我在这里抽象掉了许多底层细节——但这些是我现在要介绍的核心部分，因为我将在接下来的部分中反复引用它们。</p><p>现在我们已经初始化了引擎，让我们继续看 <code>generate</code> 函数。</p><h3 id="generate-函数" tabindex="-1"><a class="header-anchor" href="#generate-函数"><span><code>generate</code> 函数</span></a></h3><p>第一步是验证请求并将其输入引擎。对于每个prompt，我们：</p><ol><li>创建一个唯一的请求 ID 并捕获其到达时间</li><li>调用一个输入预处理器，对prompts进行分词并返回一个包含 <code>prompt</code>、<code>prompt_token_ids</code> 和 <code>type</code>（文本、token、嵌入等）的字典</li><li>将这些信息打包成一个 <code>EngineCoreRequest</code>，并添加优先级、采样参数和其他元数据</li><li>将请求传递到 Engine Core，Engine Core 将其包装在一个 <code>Request</code> 对象中，并将其状态设置为 <code>WAITING</code>。然后将此请求添加到调度程序的 <code>waiting</code> 队列中（如果是 FCFS 则追加，如果是优先级则push到堆中）</li></ol><p>此时，引擎已经接收了输入，可以开始执行。在同步引擎示例中，这些初始prompts是我们唯一要处理的——没有机制可以在运行中注入新的请求。相比之下，异步引擎支持这一点（也称为 <strong>continuous batching</strong> [6]）：在每个步骤之后，都会同时考虑新旧请求。</p><div class="hint-container note"><p class="hint-container-title">注</p><p>因为前向传播将批次展平为单个序列，并且自定义 kernel 可以高效地处理它，所以即使在同步引擎中，continuous batching 也从根本上得到了支持。</p></div><p>接下来，只要有请求需要处理，引擎就会重复调用其 <code>step()</code> 函数。每个步骤有三个阶段：</p><ol><li><strong>调度</strong>：选择在此步骤中运行的请求（decode 和/或 (chunked) prefill）</li><li><strong>前向传播</strong>：运行模型并采样 token</li><li><strong>后处理</strong>：将采样到的 token ID 附加到每个 <code>Request</code> 中，去分词，并检查停止条件。如果一个请求完成了，就进行清理（例如将其 KV-cache 块返回到 <code>free_block_queue</code>）并提前返回输出</li></ol><div class="hint-container note"><p class="hint-container-title">注</p><p>停止条件是：</p><ul><li>请求超出了其长度限制（<code>max_model_length</code> 或其自身的 <code>max_tokens</code>）</li><li>采样到的 token 是 EOS ID（除非启用了 <code>ignore_eos</code> -&gt; 这在基准测试中很有用，当我们想要强制生成一定数量的输出 token 时）</li><li>采样到的 token 与采样参数中指定的任何 <code>stop_token_ids</code> 匹配</li><li>输出中存在停止字符串 - 我们将输出截断到第一个停止字符串出现的位置，并在引擎中中止该请求（注意 <code>stop_token_ids</code> 会出现在输出中，但停止字符串不会）。</li></ul></div><p><img src="/JinBlog/assets/engine_loop-D9_TVB-L.png" alt="图 2：引擎循环"></p><div class="hint-container note"><p class="hint-container-title">注</p><p>在流式模式下，我们会在生成 token 时发送中间 token，但我们现在先忽略这一点。</p></div><p>接下来，我们将更详细地研究调度。</p><h3 id="调度器" tabindex="-1"><a class="header-anchor" href="#调度器"><span>调度器</span></a></h3><p>推理引擎处理两种主要类型的工作负载：</p><ol><li><strong>Prefill</strong> 请求 — 对所有prompt token 进行一次前向传播。这些通常是<strong>计算密集型</strong>的（阈值取决于硬件和prompt长度）。最后，我们从最后一个 token 位置的概率分布中采样一个 token。</li><li><strong>Decode</strong> 请求 — 仅对最新的一个 token 进行一次前向传播。所有早期的 KV 向量都已经被缓存。这些是<strong>内存带宽密集型</strong>的，因为我们仍然需要加载所有 LLM 权重（和 KV caches）才能计算一个 token。</li></ol><div class="hint-container note"><p class="hint-container-title">注</p><p>在<a href="#%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E5%92%8C%E8%87%AA%E5%8A%A8%E8%B0%83%E4%BC%98-%E5%BB%B6%E8%BF%9Fvs%E5%90%9E%E5%90%90%E9%87%8F">基准测试部分</a>，我们将分析所谓的 GPU 性能的 roofline 模型。这将更详细地探讨 prefill/decode 的性能概况。</p></div><p>V1 调度器可以在同一步骤中混合这两种类型的请求，这要归功于更智能的设计选择。相比之下，V0 引擎一次只能处理 prefill 或 decode。</p><p>调度器优先处理 decode 请求——即那些已经在 <code>running</code> 队列中的请求。对于每个这样的请求，它：</p><ol><li>计算要生成的新 token 的数量（由于 speculative decoding 和异步调度，不总是 1——稍后会详细介绍）。</li><li>调用 KV-cache 管理器的 <code>allocate_slots</code> 函数（详见下文）。</li><li>通过从第 1 步的 token 数量中减去来更新 token 预算。</li></ol><p>之后，它处理来自 <code>waiting</code> 队列的 prefill 请求，它：</p><ol><li>检索已计算块的数量（如果禁用了 prefix caching，则返回 0——我们稍后会介绍）。</li><li>调用 KV-cache 管理器的 <code>allocate_slots</code> 函数。</li><li>从 waiting 队列中弹出请求并将其移动到 running 队列，将其状态设置为 <code>RUNNING</code>。</li><li>更新 token 预算。</li></ol><p>现在让我们看看 <code>allocate_slots</code> 做了什么，它：</p><ol><li><strong>计算块数</strong> — 确定必须分配多少个新的 KV-cache 块 (<code>n</code>)。每个块默认存储 16 个 token。例如，如果一个 prefill 请求有 17 个新 token，我们需要 <code>ceil(17/16) = 2</code> 个块。</li><li><strong>检查可用性</strong> — 如果管理器池中没有足够的块，则<strong>提前退出</strong>。根据是 decode 还是 prefill 请求，引擎可能会尝试重新计算抢占（V0 中支持交换抢占），方法是驱逐低优先级请求（调用 <code>kv_cache_manager.free</code> 将 KV 块返回到块池），或者它可能会跳过调度并继续执行。</li><li><strong>分配块</strong> — 通过 KV-cache 管理器的协调器，从块池（前面提到的 <code>free_block_queue</code> 双向链表）中获取前 <code>n</code> 个块。存储到 <code>req_to_blocks</code>，这是一个将每个 <code>request_id</code> 映射到其 KV-cache 块列表的字典。</li></ol><p><img src="/JinBlog/assets/kv_cache_blocks-ikbpyv1b.png" alt="图 3：KV cache 块列表"></p><p>我们终于准备好进行一次前向传播了！</p><h3 id="运行前向传播" tabindex="-1"><a class="header-anchor" href="#运行前向传播"><span>运行前向传播</span></a></h3><p>我们调用 Model Executor 的 <code>execute_model</code>，它委托给 <code>Worker</code>，而 <code>Worker</code> 又委托给model runner。</p><p>以下是主要步骤：</p><ol><li><strong>更新状态</strong> — 从 <code>input_batch</code> 中修剪已完成的请求；更新与 fwd pass 相关的杂项元数据（例如，每个请求的 KV cache 块，将用于索引到 paged KV cache 内存中）。</li><li><strong>准备输入</strong> — 将缓冲区从 CPU 复制到 GPU；计算位置；构建 <code>slot_mapping</code>（示例中会详细介绍）；构造注意力元数据。</li><li><strong>前向传播</strong> — 使用自定义的 paged attn kernel 运行模型。所有序列都被展平并连接成一个长的“超级序列”。位置索引和注意力掩码确保每个序列只关注自己的 token，这使得 continuous batching 无需右填充。</li><li><strong>收集最后一个 token 的状态</strong> — 提取每个序列最后一个位置的隐藏状态并计算 logits。</li><li><strong>采样</strong> — 根据采样配置（贪婪、温度、top-p、top-k 等）从计算出的 logits 中采样 token。</li></ol><p>前向传播步骤本身有两种执行模式：</p><ol><li><strong>Eager 模式</strong> — 启用 eager execution 时运行标准的 PyTorch 前向传播。</li><li><strong>“捕获”模式</strong> — 当不强制执行 eager 时，执行/重放预先捕获的 CUDA Graph（请记住，我们在引擎构建期间的初始化 KV cache 过程中捕获了这些）。</li></ol><p>这里有一个具体的例子，应该可以清楚地说明 continuous batching 和 paged attention： <img src="/JinBlog/assets/fwd_pass-BMnHCkWW.png" alt="图 4：前向传播：continuous batching 和 paged attention"> slot_mapping: 将逻辑的token id映射到实际的KVCache的物理槽位(slot)位置</p><h2 id="高级功能-—-扩展核心引擎逻辑" tabindex="-1"><a class="header-anchor" href="#高级功能-—-扩展核心引擎逻辑"><span>高级功能 — 扩展核心引擎逻辑</span></a></h2><p>掌握了基本的引擎流程后，我们现在可以看看高级功能。</p><p>我们已经讨论了抢占、Paged Attention 和 Continuous Batching。</p><p>接下来，我们将深入探讨：</p><ol><li>Chunked Prefill</li><li>Prefix Caching</li><li>Guided Decoding（通过基于语法的有限状态机）</li><li>Speculative Decoding</li><li>Disaggregated P/D (prefill/decoding)</li></ol><h3 id="chunked-prefill" tabindex="-1"><a class="header-anchor" href="#chunked-prefill"><span>Chunked Prefill</span></a></h3><p>Chunked Prefill 是一种通过将其 prefill 步骤拆分成更小的块来处理长prompt的技术。如果没有它，我们可能会遇到单个非常长的请求独占一个引擎步骤，从而阻止其他 prefill 请求运行的情况。这会推迟所有其他请求并增加它们的延迟。</p><p>例如，让每个块包含 <code>n</code> (=8) 个 token，用小写字母标记，并用“-”分隔。一个长prompt <code>P</code> 可能看起来像 <code>x-y-z</code>，其中 <code>z</code> 是一个不完整的块（例如 2 个 token）。执行 <code>P</code> 的完整 prefill 将需要 ≥ 3 个引擎步骤（如果它在其中一个步骤中没有被调度执行，则可能 &gt; 3），并且只有在最后一个 chunked prefill 步骤中我们才会采样一个新的 token。</p><p>这是同一个例子的图示：</p><p><img src="/JinBlog/assets/chunked_pt1-JuMge1Wk.png" alt="图 5：Chunked Prefill"></p><p>实现很简单：限制每个步骤的新 token 数量。如果请求的数量超过 <code>long_prefill_token_threshold</code>，则将其重置为该确切值。底层的索引逻辑（前面描述的）会处理剩下的事情。</p><p>在 vLLM V1 中，您可以通过将 <code>long_prefill_token_threshold</code> 设置为正整数来启用 chunked prefill。（技术上讲，无论如何都可能发生，如果prompt长度超过了 token 预算，我们会截断它并运行 chunked prefill。）</p><h3 id="prefix-caching" tabindex="-1"><a class="header-anchor" href="#prefix-caching"><span>Prefix Caching</span></a></h3><p>为了解释 prefix caching 的工作原理，让我们对原始代码示例稍作调整：</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" style="--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212;"><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code class="language-python"><span class="line"><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">from</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> vllm </span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">import</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> LLM</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> SamplingParams</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">long_prefix </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;"> &quot;</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">&lt;a piece of text that is encoded into more than block_size tokens&gt;</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">prompts </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#999999;--shiki-dark:#666666;"> [</span></span>
<span class="line"><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">    &quot;</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">Hello, my name is</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span></span>
<span class="line"><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">    &quot;</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">The president of the United States is</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span></span>
<span class="line"><span style="--shiki-light:#999999;--shiki-dark:#666666;">]</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">sampling_params </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> SamplingParams</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;">temperature</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">0.8</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;"> top_p</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">0.95</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">def</span><span style="--shiki-light:#59873A;--shiki-dark:#80A665;"> main</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">():</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">    llm </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> LLM</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;">model</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">TinyLlama/TinyLlama-1.1B-Chat-v1.0</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">    outputs </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> llm</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">generate</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">long_prefix </span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">+</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> prompts</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">[</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">0</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">],</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> sampling_params</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">    outputs </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> llm</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">generate</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">long_prefix </span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">+</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> prompts</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">[</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">1</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">],</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> sampling_params</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">if</span><span style="--shiki-light:#998418;--shiki-dark:#B8A965;"> __name__</span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;"> ==</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;"> &quot;</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">__main__</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">:</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">    main</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">()</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>Prefix caching 避免了重新计算多个prompt在开头共享的 token——因此得名 <strong>prefix</strong>。</p><p>关键部分是 <code>long_prefix</code>：它被定义为任何长于一个 KV-cache 块（默认为 16 个 token）的前缀。为了简化我们的示例，我们假设 <code>long_prefix</code> 的长度正好是 <code>n x block_size</code>（其中 <code>n ≥ 1</code>）。</p><div class="hint-container note"><p class="hint-container-title">注</p><p>即它与块边界完美对齐——否则我们将不得不重新计算 <code>long_prefix_len % block_size</code> 个 token，因为我们无法缓存不完整的块。</p><p>Q: 那如果没有对齐, 是选择空出block, 还是每次拼接后都重算呢?</p></div><p>如果没有 prefix caching，每次我们处理一个具有相同 <code>long_prefix</code> 的新请求时，我们都会重新计算所有 <code>n x block_size</code> 个 token。</p><p>有了 prefix caching，这些 token 只计算一次（它们的 KV 存储在 KV cache paged memory 中），然后被重用，所以只需要处理新的prompt token。这加快了 prefill 请求的速度（尽管对 decode 没有帮助）。</p><p>这在 vLLM 中是如何工作的？</p><p>在第一次 <code>generate</code> 调用期间，在调度阶段，在 <code>kv_cache_manager.get_computed_blocks</code> 内部，引擎调用 <code>hash_request_tokens</code>：</p><ol><li><p>此函数将 <code>long_prefix + prompts[0]</code> 拆分成 16 个 token 的块。</p></li><li><p>对于每个完整的块，它计算一个哈希（使用内置哈希或 SHA-256，后者较慢但冲突较少）。该哈希结合了<strong>前一个块的哈希</strong>、<strong>当前 token</strong> 和可选的<strong>元数据</strong>。</p></li></ol><div class="hint-container note"><p class="hint-container-title">注</p><p>可选的元数据包括：</p><ul><li>MM hash: <strong>Multi-Model Hash 多模态Hash</strong>, 组合图片和文本进行特征编码, 得到总体的Hash</li><li>LoRA ID: <strong>Low-Rank Adapter</strong>, 使用不同的低秩矩阵进行参数微调, 实现模型定制和任务迁移</li><li>cache salt: 注入到<strong>第一个块</strong>的哈希中，确保只有具有此缓存盐的请求才能重用块</li></ul></div><ol start="3"><li>每个结果都存储为一个 <code>BlockHash</code> 对象，包含哈希及其 token ID。我们返回一个块哈希列表。</li></ol><p>该列表存储在 <code>self.req_to_block_hashes[request_id]</code> 中。</p><p>接下来，引擎调用 <code>find_longest_cache_hit</code> 来检查这些block hash中是否有任何一个已经存在于 <code>cached_block_hash_to_block</code> 中。在此处的第一个请求上，没有找到命中。</p><p><img src="/JinBlog/assets/prefix_pt1-9gFdh8Ir.png" alt="图 6：Prefix caching - 哈希函数"></p><p>然后我们调用 <code>allocate_slots</code>，它调用 <code>coordinator.cache_blocks</code>，将新的 <code>BlockHash</code> 条目与分配的 KV 块关联起来，并将它们记录在 <code>cached_block_hash_to_block</code> 中。</p><p>之后，前向传播将在 paged KV cache 内存中填充与我们上面分配的 KV cache 块相对应的 KV。</p><div class="hint-container note"><p class="hint-container-title">注</p><p>经过许多引擎步骤后，它会分配更多的 KV cache 块，但这对于我们的示例来说无关紧要，因为前缀在 <code>long_prefix</code> 之后立即分叉了。</p></div><p><img src="/JinBlog/assets/prefix_pt2-DKCDEr5Q.png" alt="图 7：Prefix caching - 在 paged memory 中填充 KV"></p><p>在第二次使用相同前缀的 <code>generate</code> 调用中，重复步骤 1-3，但现在 <code>find_longest_cache_hit</code> 为所有 <code>n</code> 个块找到匹配项（通过线性搜索）。引擎可以直接重用这些 KV 块。</p><p><img src="/JinBlog/assets/prefix_pt3-BbyyD-pg.png" alt="图 8：Prefix caching - 重用 KV"></p><p>如果原始请求仍然存在，这些块的引用计数将增加（例如增加到 2）。在这个例子中，第一个请求已经完成，所以这些块被释放回池中，它们的引用计数被重置为 0。因为我们能够从 <code>cached_block_hash_to_block</code> 中检索到它们，我们知道它们是有效的（KV cache 管理器的逻辑是这样设置的），所以我们只是再次将它们从 <code>free_block_queue</code> 中移除。</p><blockquote><p>[!NOTE] Advanced note: KV-cache 块只有在它们即将从 <code>free_block_queue</code>（从左侧弹出）重新分配时才会失效，并且我们发现该块仍然具有关联的哈希并且存在于 <code>cached_block_hash_to_block</code> 中。在那一刻，我们清除该块的哈希并将其条目从 <code>cached_block_hash_to_block</code> 中移除，确保它不能通过 prefix caching 重用（至少不能用于那个旧前缀）。</p></blockquote><p>这就是 prefix caching 的要点：不要重新计算你已经见过的 prefixes——只需重用它们的 KV cache！</p><p>如果你理解了这个例子，你也就理解了 paged attention 的工作原理。</p><p>Prefix caching 默认启用。要禁用它：<code>enable_prefix_caching = False</code>。</p><h3 id="guided-decoding-fsm" tabindex="-1"><a class="header-anchor" href="#guided-decoding-fsm"><span>Guided Decoding (FSM)</span></a></h3><p>Guided decoding 是一种技术，在每个 decoding 步骤中，logits 都受到基于语法的有限状态机的约束。<strong>这确保了只有语法允许的 token 才能被采样。</strong></p><p>这是一个强大的设置：你可以强制执行从正则语法（Chomsky type-3，例如任意正则表达式模式）到上下文无关语法（type-2，涵盖大多数编程语言）的任何内容。</p><p>为了让这不那么抽象，让我们从最简单的例子开始，在我们早期的代码上构建：</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" style="--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212;"><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code class="language-python"><span class="line"><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">from</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> vllm </span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">import</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> LLM</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> SamplingParams</span></span>
<span class="line"><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">from</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> vllm</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">sampling_params </span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">import</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> GuidedDecodingParams</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">prompts </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#999999;--shiki-dark:#666666;"> [</span></span>
<span class="line"><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">    &quot;</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">This sucks</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span></span>
<span class="line"><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">    &quot;</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">The weather is beautiful</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span></span>
<span class="line"><span style="--shiki-light:#999999;--shiki-dark:#666666;">]</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">guided_decoding_params </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> GuidedDecodingParams</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;">choice</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=[</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">Positive</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;"> &quot;</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">Negative</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">])</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">sampling_params </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> SamplingParams</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;">guided_decoding</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">guided_decoding_params</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">def</span><span style="--shiki-light:#59873A;--shiki-dark:#80A665;"> main</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">():</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">    llm </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> LLM</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;">model</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">TinyLlama/TinyLlama-1.1B-Chat-v1.0</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">    outputs </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> llm</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">generate</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">prompts</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> sampling_params</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">if</span><span style="--shiki-light:#998418;--shiki-dark:#B8A965;"> __name__</span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;"> ==</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;"> &quot;</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">__main__</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">:</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">    main</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">()</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>在我给出的玩具示例中（假设是字符级分词）：在 prefill 时，FSM 会屏蔽 logits，因此只有“P”或“N”是可行的。如果采样到“P”，FSM 会移动到“Positive”分支；下一步只允许“o”，依此类推。</p><p><img src="/JinBlog/assets/fsm-hLSmZNOv.png" alt="图 9：玩具示例 FSM"></p><p>这在 vLLM 中是如何工作的：</p><ol><li>在 LLM 引擎构建时，会创建一个 <code>StructuredOutputManager</code>；它可以访问 tokenizer 并维护一个 <code>_grammar_bitmask</code> 张量。</li><li>添加请求时，其状态设置为 <code>WAITING_FOR_FSM</code>，<code>grammar_init</code> 选择后端编译器（例如，<code>xgrammar</code> [7]；注意后端是第三方代码）。</li><li>此请求的语法是异步编译的。</li><li>在调度期间，如果异步编译已完成，状态将切换到 <code>WAITING</code>，并将 <code>request_id</code> 添加到 <code>structured_output_request_ids</code>；否则将其放入 <code>skipped_waiting_requests</code> 以在下一个引擎步骤中重试。</li><li>在调度循环之后（仍在调度内部），如果有 FSM 请求，<code>StructuredOutputManager</code> 会要求后端准备/更新 <code>_grammar_bitmask</code>。</li><li>在前向传播产生 logits 之后，xgr_torch_compile 的函数将位掩码扩展到词汇表大小（使用 32 位整数时扩展比为 32x）并将不允许的 logits 屏蔽为 –∞。</li><li>采样下一个 token 后，通过 <code>accept_tokens</code> 推进请求的 FSM。在 FSM 图上，我们视觉上移动到下一个状态。</li></ol><p>第 6 步值得进一步说明。</p><p>如果 <code>vocab_size = 32</code>，<code>_grammar_bitmask</code> 是一个整数；其二进制表示编码了哪些 token 是允许的（“1”）与不允许的（“0”）。例如，“101…001”扩展为一个长度为 32 的数组 <code>[1, 0, 1, ..., 0, 0, 1]</code>；位置为 0 的 logits 被设置为 –∞。对于更大的词汇表，使用多个 32 位字并相应地扩展/连接。后端（例如，<code>xgrammar</code>）负责使用当前的 FSM 状态生成这些位模式。</p><div class="hint-container note"><p class="hint-container-title">注</p><p>这里的大部分复杂性都隐藏在像 xgrammar 这样的第三方库中。</p></div><p>这是一个更简单的例子，词汇表大小为 8，使用 8 位整数（对于那些喜欢我的图示的人）：</p><p><img src="/JinBlog/assets/fsm2-BRaQA3jK.png" alt="图 10：玩具示例"></p><p>您可以在 vLLM 中通过传入所需的 <code>guided_decoding</code> 配置来启用此功能。</p><h3 id="speculative-decoding" tabindex="-1"><a class="header-anchor" href="#speculative-decoding"><span>Speculative Decoding</span></a></h3><p>在自回归生成中，每个新 token 都需要对大型 LM 进行一次前向传播。这很昂贵——每一步都要重新加载和应用所有模型权重，只为了计算一个 token！（假设 batch size == 1，通常是 <code>B</code>）</p><p>Speculative decoding(投机解码) [8] 通过引入一个较小的 draft LM 来加速这一过程。draft LM 可以廉价地提出 <em>(未来连续的)</em> <code>k</code> 个 token。但我们最终不想从较小的模型中采样——它只是用来猜测候选的延续。大型模型仍然决定什么是有效的。</p><p>以下是步骤：</p><ol><li><strong>Draft</strong>：在当前上下文中运行小型模型并提出 <code>k</code> 个 token</li><li><strong>Verify</strong>：在上下文 + <code>k</code> 个 draft token 上运行一次大型模型。这将为这 <code>k</code> 个位置加上一个额外的位置产生概率（所以我们得到 <code>k+1</code> 个候选）</li><li><strong>Accept/reject</strong>：从左到右遍历 <code>k</code> 个 draft token： <ul><li>如果大型模型对 draft token 的概率 ≥ draft 模型的概率，则接受它</li><li>否则，以 <code>p_large(token)/p_draft(token)</code> 的概率接受它</li><li>在第一次拒绝时停止，或接受所有 <code>k</code> 个 draft token <ul><li>如果所有 <code>k</code> 个 draft token 都被接受，也从大型模型中“免费”采样额外的第 <code>(k+1)</code> 个 token（我们已经计算了该分布）</li><li>如果发生拒绝，则在该位置创建一个新的重新平衡的分布（<code>p_large - p_draft</code>，将最小值钳位在 0，归一化以使总和为 1）并从中采样最后一个 token</li><li>注:</li><li>发生reject意味着Draft LM已经占据了概率质量, 需要在剩余的概率空间(<code>p_large - p_draft</code>)中采样</li><li>每个p都是vocab size维的概率向量</li></ul></li></ul></li></ol><p><strong>为什么这行得通</strong>：虽然我们使用小型模型来提出候选，但接受/拒绝规则保证了序列在期望上与我们逐个 token 从大型模型中采样完全相同。这意味着 speculative decoding 在统计上等同于标准的自回归 decoding——但可能快得多，因为一次大型模型的前向传播最多可以产生 <code>k+1</code> 个 token。</p><div class="hint-container note"><p class="hint-container-title">注</p><p>我建议查看 <a href="https://github.com/meta-pytorch/gpt-fast" title="null" target="_blank" rel="noopener noreferrer">gpt-fast</a> 以获取简单的实现，以及<a href="https://arxiv.org/abs/2302.01318" title="null" target="_blank" rel="noopener noreferrer">原始论文</a>以了解数学细节和与从完整模型采样等效的证明。</p></div><p>vLLM V1 不支持 LLM draft model 方法，而是实现了更快但不太准确的提议方案：n-gram、EAGLE [9] 和 Medusa [10]。</p><p>每个方案的一句话总结：</p><ul><li><strong>n-gram</strong>：取最后 <code>prompt_lookup_max</code> 个 token；在序列中找到一个先前的匹配项；如果找到，则提出该匹配项后面的 <code>k</code> 个 token；否则减少窗口并重试直到 <code>prompt_lookup_min</code></li></ul><div class="hint-container note"><p class="hint-container-title">注</p><p>当前的实现返回第一个匹配项后的 <code>k</code> 个 token。引入一个近因偏见并反转搜索方向似乎更自然？（即最后一个匹配项）</p></div><ul><li><strong>Eagle</strong>：对大型 LM 进行“模型手术”——保留 embeddings 和 LM head，用一个轻量级的 MLP 替换 Transformer 堆栈；将其微调为一个廉价的 draft</li><li><strong>Medusa</strong>：在大型模型的顶部（LM head 之前的 embeddings）训练辅助线性头，以并行预测接下来的 <code>k</code> 个 token；使用这些头比运行一个单独的小型 LM 更有效地提出 token</li></ul><p>以下是如何在 vLLM 中使用 <code>ngram</code> 作为 draft 方法来调用 speculative decoding：</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" style="--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212;"><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code class="language-python"><span class="line"><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">from</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> vllm </span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">import</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> LLM</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> SamplingParams</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">prompts </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#999999;--shiki-dark:#666666;"> [</span></span>
<span class="line"><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">    &quot;</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">Hello, my name is</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span></span>
<span class="line"><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">    &quot;</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">The president of the United States is</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span></span>
<span class="line"><span style="--shiki-light:#999999;--shiki-dark:#666666;">]</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">sampling_params </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> SamplingParams</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;">temperature</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">0.8</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;"> top_p</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">0.95</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">speculative_config</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">={</span></span>
<span class="line"><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">    &quot;</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">method</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">:</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;"> &quot;</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">ngram</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span></span>
<span class="line"><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">    &quot;</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">prompt_lookup_max</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">:</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;"> 5</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span></span>
<span class="line"><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">    &quot;</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">prompt_lookup_min</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">:</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;"> 3</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span></span>
<span class="line"><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">    &quot;</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">num_speculative_tokens</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">:</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;"> 3</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span></span>
<span class="line"><span style="--shiki-light:#999999;--shiki-dark:#666666;">}</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">def</span><span style="--shiki-light:#59873A;--shiki-dark:#80A665;"> main</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">():</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">    llm </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> LLM</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;">model</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">TinyLlama/TinyLlama-1.1B-Chat-v1.0</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;"> speculative_config</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">speculative_config</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">    outputs </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> llm</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">generate</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">prompts</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> sampling_params</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">if</span><span style="--shiki-light:#998418;--shiki-dark:#B8A965;"> __name__</span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;"> ==</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;"> &quot;</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">__main__</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">:</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">    main</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">()</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>这在 vLLM 中是如何工作的？</p><p><strong>设置（在引擎构建期间）：</strong></p><ol><li>初始化设备：创建一个 <code>drafter</code>（draft model，例如，<code>NgramProposer</code>）和一个 <code>rejection_sampler</code>（其部分是用 Triton 编写的）。</li><li>加载模型：加载 draft model 权重（对于 n-gram 是无操作）。</li></ol><p><strong>之后在 <code>generate</code> 函数中</strong>（假设我们得到一个全新的请求）：</p><ol><li>使用大型模型运行常规的 prefill 步骤。</li><li>在前向传播和标准采样之后，调用 <code>propose_draft_token_ids(k)</code> 从 draft model 中采样 <code>k</code> 个 draft token。</li><li>将这些存储在 <code>request.spec_token_ids</code> 中（更新请求元数据）。</li><li>在下一个引擎步骤中，当请求在 running 队列中时，将 <code>len(request.spec_token_ids)</code> 添加到“新 token”计数中，以便 <code>allocate_slots</code> 为 fwd pass 保留足够的 KV 块。</li><li>将 <code>spec_token_ids</code> 复制到 <code>input_batch.token_ids_cpu</code> 中以形成（上下文 + draft）token。</li><li>通过 <code>_calc_spec_decode_metadata</code> 计算元数据（这将从 <code>input_batch.token_ids_cpu</code> 复制 token，准备 logits 等），然后在 draft token 上运行大型模型的前向传播。</li><li>不从 logits 进行常规采样，而是使用 <code>rejection_sampler</code> 从左到右接受/拒绝并产生 <code>output_token_ids</code>。</li><li>重复步骤 2-7 直到满足停止条件。</li></ol><p>内化这一点的最佳方法是启动调试器并单步执行代码库，但希望本节能让您对此有所了解。这个也是：</p><p><img src="/JinBlog/assets/specdec_pt1-D0zB5GWl.png" alt="图 11：Speculative Decoding"></p><h3 id="disaggregated-p-d" tabindex="-1"><a class="header-anchor" href="#disaggregated-p-d"><span>Disaggregated P/D</span></a></h3><p>我之前已经暗示了 disaggregated P/D (prefill/decode) 背后的动机。</p><p>Prefill 和 decode 具有非常不同的性能概况（计算密集型 vs. 内存带宽密集型），因此将它们的执行分开是一种明智的设计。它对延迟提供了更严格的控制——包括 <code>TFTT</code>（首个 token 时间）和 <code>ITL</code>（token 间延迟）——更多内容将在<a href="#%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E5%92%8C%E8%87%AA%E5%8A%A8%E8%B0%83%E4%BC%98-%E5%BB%B6%E8%BF%9Fvs%E5%90%9E%E5%90%90%E9%87%8F">基准测试</a>部分讨论。</p><p>在实践中，我们运行 <code>N</code> 个 vLLM prefill 实例和 <code>M</code> 个 vLLM decode 实例，并根据实时请求组合自动扩展它们。Prefill worker 将 KV 写入专用的 KV-cache 服务；decode worker 从中读取。这将长的、突发性的 prefill 与稳定的、对延迟敏感的 decode 隔离开来。</p><p>这在 vLLM 中是如何工作的？</p><p>为清楚起见，下面的示例依赖于 <code>SharedStorageConnector</code>，这是一个用于说明机制的调试连接器实现。</p><div class="hint-container note"><p class="hint-container-title">注</p><p>Connector 是 vLLM 用于处理实例之间 KV 交换的抽象。Connector 接口尚不稳定，计划在近期进行一些改进，其中将涉及一些更改，其中一些可能是破坏性的。</p></div><p>我们启动 2 个 vLLM 实例（GPU 0 用于 prefill，GPU 1 用于 decode），然后在它们之间传输 KV cache：</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" style="--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212;"><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code class="language-python"><span class="line"><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">import</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> os</span></span>
<span class="line"><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">import</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> time</span></span>
<span class="line"><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">from</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> multiprocessing </span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">import</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> Event</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> Process</span></span>
<span class="line"><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">import</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> multiprocessing </span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">as</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> mp</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">from</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> vllm </span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">import</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> LLM</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> SamplingParams</span></span>
<span class="line"><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">from</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> vllm</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">config </span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">import</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> KVTransferConfig</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">prompts </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#999999;--shiki-dark:#666666;"> [</span></span>
<span class="line"><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">    &quot;</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">Hello, my name is</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span></span>
<span class="line"><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">    &quot;</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">The president of the United States is</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span></span>
<span class="line"><span style="--shiki-light:#999999;--shiki-dark:#666666;">]</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">def</span><span style="--shiki-light:#59873A;--shiki-dark:#80A665;"> run_prefill</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">prefill_done</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">):</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">  os</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">environ</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">[</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">CUDA_VISIBLE_DEVICES</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">]</span><span style="--shiki-light:#999999;--shiki-dark:#666666;"> =</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;"> &quot;</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">0</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">  sampling_params </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> SamplingParams</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;">temperature</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">0</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;"> top_p</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">0.95</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;"> max_tokens</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">1</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">  ktc</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">KVTransferConfig</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span></span>
<span class="line"><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;">      kv_connector</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">SharedStorageConnector</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span></span>
<span class="line"><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;">      kv_role</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">kv_both</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span></span>
<span class="line"><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;">      kv_connector_extra_config</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">={</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">shared_storage_path</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">:</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;"> &quot;</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">local_storage</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">},</span></span>
<span class="line"><span style="--shiki-light:#999999;--shiki-dark:#666666;">  )</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">  llm </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> LLM</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;">model</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">TinyLlama/TinyLlama-1.1B-Chat-v1.0</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;"> kv_transfer_config</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">ktc</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">  llm</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">generate</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">prompts</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> sampling_params</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">  prefill_done</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">set</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">()</span><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;">  # notify decode instance that KV cache is ready</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;">  # To keep the prefill node running in case the decode node is not done;</span></span>
<span class="line"><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;">  # otherwise, the script might exit prematurely, causing incomplete decoding.</span></span>
<span class="line"><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">  try</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">:</span></span>
<span class="line"><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">      while</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;"> True</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">:</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">          time</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">sleep</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">1</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">  except</span><span style="--shiki-light:#998418;--shiki-dark:#B8A965;"> KeyboardInterrupt</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">:</span></span>
<span class="line"><span style="--shiki-light:#998418;--shiki-dark:#B8A965;">      print</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">Script stopped by user.</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">def</span><span style="--shiki-light:#59873A;--shiki-dark:#80A665;"> run_decode</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">prefill_done</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">):</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">  os</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">environ</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">[</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">CUDA_VISIBLE_DEVICES</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">]</span><span style="--shiki-light:#999999;--shiki-dark:#666666;"> =</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;"> &quot;</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">1</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">  sampling_params </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> SamplingParams</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;">temperature</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">0</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;"> top_p</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">0.95</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">  ktc</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">KVTransferConfig</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span></span>
<span class="line"><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;">      kv_connector</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">SharedStorageConnector</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span></span>
<span class="line"><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;">      kv_role</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">kv_both</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span></span>
<span class="line"><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;">      kv_connector_extra_config</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">={</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">shared_storage_path</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">:</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;"> &quot;</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">local_storage</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">},</span></span>
<span class="line"><span style="--shiki-light:#999999;--shiki-dark:#666666;">  )</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">  llm </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> LLM</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;">model</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">TinyLlama/TinyLlama-1.1B-Chat-v1.0</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;"> kv_transfer_config</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">ktc</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">  prefill_done</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">wait</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">()</span><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;">  # block waiting for KV cache from prefill instance</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;">  # Internally it&#39;ll first fetch KV cache before starting the decoding loop</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">  outputs </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> llm</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">generate</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">prompts</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> sampling_params</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">if</span><span style="--shiki-light:#998418;--shiki-dark:#B8A965;"> __name__</span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;"> ==</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;"> &quot;</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">__main__</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">:</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">  prefill_done </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> Event</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">()</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">  prefill_process </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> Process</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;">target</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">run_prefill</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;"> args</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">prefill_done</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,))</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">  decode_process </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> Process</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;">target</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">run_decode</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;"> args</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">prefill_done</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,))</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">  prefill_process</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">start</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">()</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">  decode_process</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">start</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">()</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">  decode_process</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">join</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">()</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">  prefill_process</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">terminate</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">()</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="hint-container note"><p class="hint-container-title">注</p><p>我也尝试过 <code>LMCache</code> [11]，它是最快的生产级的连接器（使用 NVIDIA 的 NIXL 作为后端），但它仍处于前沿，我遇到了一些错误。由于其大部分复杂性存在于外部仓库中，因此 <code>SharedStorageConnector</code> 是一个更好的解释选择。</p></div><p>以下是 vLLM 中的步骤：</p><ol><li><p><strong>实例化</strong> — 在引擎构建期间，在两个地方创建连接器：</p><ul><li>在 worker 的 init device 过程中（在 init worker 分布式环境函数下），角色为“worker”。</li><li>在调度器构造函数中，角色为“scheduler”。</li></ul></li><li><p><strong>缓存查找</strong> — 当调度器处理来自 <code>waiting</code> 队列的 prefill 请求时（在本地 prefix-cache 检查之后），它会调用连接器的 <code>get_num_new_matched_tokens</code>。这会在 KV-cache 服务器中检查外部缓存的 token。Prefill 在这里总是看到 0；decode 可能会有缓存命中。在调用 <code>allocate_slots</code> 之前，将结果添加到本地计数中。</p></li><li><p><strong>状态更新</strong> — 调度器然后调用 <code>connector.update_state_after_alloc</code>，它记录了有缓存的请求（对 prefill 是无操作）。</p></li><li><p><strong>构建元数据对象</strong> — 在调度结束时，调度器调用 <code>meta = connector.build_connector_meta</code>：</p><ul><li>Prefill 添加所有 <code>is_store=True</code> 的请求（以上传 KV）。</li><li>Decode 添加 <code>is_store=False</code> 的请求（以获取 KV）。</li></ul></li><li><p><strong>上下文管理器</strong> — 在前向传播之前，引擎进入一个 KV-connector 上下文管理器：</p><ul><li>进入时：调用 <code>kv_connector.start_load_kv</code>。对于 decode，这将从外部服务器加载 KV 并将其注入到 paged memory 中。对于 prefill，这是一个无操作。</li><li>退出时：调用 <code>kv_connector.wait_for_save</code>。对于 prefill，这将阻塞直到 KV 上传到外部服务器。对于 decode，这是一个无操作。</li></ul></li></ol><p>这是一个图示示例：</p><p><img src="/JinBlog/assets/pd-CuOE7Sn8.png" alt="图 12：disaggregated P/D"></p><blockquote><p>[!NOTE] <strong>附加说明：</strong></p><ul><li>对于 <code>SharedStorageConnector</code>，“外部服务器”只是一个本地文件系统。</li><li>根据配置，KV 传输也可以逐层进行（在每个注意力层之前/之后）。</li><li>Decode 只在其请求的第一步加载一次外部 KV；之后它在本地计算/存储。</li></ul></blockquote><h2 id="从-uniprocexecutor-到-multiprocexecutor" tabindex="-1"><a class="header-anchor" href="#从-uniprocexecutor-到-multiprocexecutor"><span>从 UniprocExecutor 到 MultiProcExecutor</span></a></h2><p>掌握了核心技术后，我们现在可以讨论扩展了。</p><p>假设您的模型权重不再适合单个 GPU 的 VRAM。</p><p>第一个选项是使用 tensor parallelism（例如，<code>TP=8</code>）将模型分片到同一节点上的多个 GPU。如果模型仍然不适合，下一步是跨节点的 pipeline parallelism。</p><blockquote><p>[!NOTE] <strong>注意：</strong></p><ul><li>节点内带宽明显高于节点间带宽，这就是为什么 tensor parallelism (TP) 通常优于 pipeline parallelism (PP)。（PP 传输的数据比 TP 少也是事实。）</li><li>我没有涵盖 expert parallelism (EP)，因为我们专注于标准的 Transformer 而不是 MoE，也没有涵盖 sequence parallelism，因为 TP 和 PP 在实践中是最常用的。</li></ul></blockquote><p>在这个阶段，我们需要多个 GPU 进程（worker）和一个协调它们的编排层。这正是 <code>MultiProcExecutor</code> 所提供的。</p><p><img src="/JinBlog/assets/multiprocexecutor-B35vbogw.png" alt="图 13：TP=8 设置中的 MultiProcExecutor（驱动 worker 为 rank 0）"></p><p>这在 vLLM 中是如何工作的：</p><ol><li><code>MultiProcExecutor</code> 初始化一个 <code>rpc_broadcast_mq</code> 消息队列（在底层使用共享内存实现）。</li><li>构造函数遍历 <code>world_size</code>（例如 <code>TP=8 ⇒ world_size=8</code>）并通过 <code>WorkerProc.make_worker_process</code> 为每个 rank 派生一个守护进程。</li></ol><ul><li><strong>rank</strong>: 分布式进程组中, 每个进程的唯一编号</li></ul><ol start="3"><li>对于每个 worker，父进程首先创建一个读写管道。</li><li>新进程运行 <code>WorkerProc.worker_main</code>，它实例化一个 worker（经历与 <code>UniprocExecutor</code> 中相同的“初始化设备”、“加载模型”等过程）。</li><li>每个 worker 确定它是否是驱动程序（TP 组中的 rank 0）或普通 worker。每个 worker 设置两个队列： <ul><li><code>rpc_broadcast_mq</code>（与父进程共享）用于接收工作。</li><li><code>worker_response_mq</code> 用于发回响应。</li></ul></li><li>在初始化期间，每个子进程通过管道将其 <code>worker_response_mq</code> 句柄发送给父进程。一旦全部收到，父进程就会解除阻塞——这完成了协调。</li><li>Worker 然后进入一个忙碌循环，在 <code>rpc_broadcast_mq.dequeue</code> 上阻塞。当一个work item到达时，它们执行它（就像在 <code>UniprocExecutor</code> 中一样，但现在使用 TP/PP 特定的分区工作）。结果通过 <code>worker_response_mq.enqueue</code> 发回。</li></ol><ul><li>每个<code>rpc_broadcast_mq</code>的work item都被</li></ul><ol start="8"><li>在运行时，当一个请求到达时，<code>MultiProcExecutor</code> 将其排入所有子 worker 的 <code>rpc_broadcast_mq</code>（非阻塞）。然后它在指定的输出 rank 的 <code>worker_response_mq.dequeue</code> 上等待以收集最终结果。</li></ol><p>从引擎的角度来看，没有任何改变——所有这些多进程复杂性都通过调用 Model Executor 的 <code>execute_model</code> 抽象掉了。</p><ul><li>在 <code>UniProcExecutor</code> 的情况下：execute_model 直接导致在 worker 上调用 execute_model</li><li>在 <code>MultiProcExecutor</code> 的情况下：execute_model 间接导致通过 <code>rpc_broadcast_mq</code> 在每个 worker 上调用 execute_model</li></ul><p>此时，我们可以使用相同的引擎接口运行资源允许的任何大小的模型。</p><p>下一步是横向扩展：启用 data parallelism (<code>DP &gt; 1</code>)，跨节点复制模型，添加一个轻量级的 DP 协调层，在副本之间引入负载均衡，并在前面放置一个或多个 API 服务器来处理传入流量。</p><h2 id="分布式系统服务-vllm" tabindex="-1"><a class="header-anchor" href="#分布式系统服务-vllm"><span>分布式系统服务 vLLM</span></a></h2><blockquote><p>[!IMPORTANT] 记录: 这一段在没有上手代码的情况下有点难理解, 未来考虑重新阅读</p></blockquote><p>设置服务基础设施有很多方法，但为了保持具体，这里有一个例子：假设我们有两个 H100 节点，并希望在它们上面运行四个 vLLM 引擎。</p><p>如果模型需要 <code>TP=4</code>，我们可以这样配置节点。</p><p><img src="/JinBlog/assets/server_setup-C2NmGjhk.png" alt="图 14：具有 2 个 8xH100 节点的服务器配置（1 个无头，1 个 api 服务器）"></p><p>在第一个节点上，以无头模式运行引擎（无 API 服务器），并使用以下参数：</p><div class="language-bash line-numbers-mode" data-highlighter="shiki" data-ext="bash" style="--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212;"><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code class="language-bash"><span class="line"><span style="--shiki-light:#59873A;--shiki-dark:#80A665;">vllm</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;"> serve</span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;"> &lt;</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">model-nam</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">e</span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">&gt;</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> \</span></span>
<span class="line"><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">  --tensor-parallel-size</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;"> 4</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> \</span></span>
<span class="line"><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">  --data-parallel-size</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;"> 4</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> \</span></span>
<span class="line"><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">  --data-parallel-size-local</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;"> 2</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> \</span></span>
<span class="line"><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">  --data-parallel-start-rank</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;"> 0</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> \</span></span>
<span class="line"><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">  --data-parallel-address</span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;"> &lt;</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">master-i</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">p</span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">&gt;</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> \</span></span>
<span class="line"><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">  --data-parallel-rpc-port</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;"> 13345</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> \</span></span>
<span class="line"><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">  --headless</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>并在另一个节点上运行相同的命令，但稍作调整：</p><ul><li>无 <code>--headless</code></li><li>修改 DP 起始 rank</li></ul><div class="language-bash line-numbers-mode" data-highlighter="shiki" data-ext="bash" style="--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212;"><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code class="language-bash"><span class="line"><span style="--shiki-light:#59873A;--shiki-dark:#80A665;">vllm</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;"> serve</span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;"> &lt;</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">model-nam</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">e</span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">&gt;</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> \</span></span>
<span class="line"><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">  --tensor-parallel-size</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;"> 4</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> \</span></span>
<span class="line"><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">  --data-parallel-size</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;"> 4</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> \</span></span>
<span class="line"><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">  --data-parallel-size-local</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;"> 2</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> \</span></span>
<span class="line"><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">  --data-parallel-start-rank</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;"> 2</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> \</span></span>
<span class="line"><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">  --data-parallel-address</span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;"> &lt;</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">master-i</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">p</span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">&gt;</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> \</span></span>
<span class="line"><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">  --data-parallel-rpc-port</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;"> 13345</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="hint-container note"><p class="hint-container-title">注</p><p>这假设网络已配置，以便所有节点都可以访问指定的 IP 和端口。</p></div><p>这在 VLLM 中是如何工作的？</p><h3 id="在无头服务器节点上" tabindex="-1"><a class="header-anchor" href="#在无头服务器节点上"><span>在无头服务器节点上</span></a></h3><p>在无头节点上，一个 <code>CoreEngineProcManager</code> 启动 2 个进程（根据 <code>--data-parallel-size-local</code>），每个进程运行 <code>EngineCoreProc.run_engine_core</code>。这些函数中的每一个都创建一个 <code>DPEngineCoreProc</code>（引擎核心），然后进入其忙碌循环。</p><p><code>DPEngineCoreProc</code> 初始化其父 <code>EngineCoreProc</code>（<code>EngineCore</code> 的子类），它：</p><ol><li>创建一个 <code>input_queue</code> 和 <code>output_queue</code> (<code>queue.Queue</code>)。</li><li>使用 <code>DEALER</code> ZMQ 套接字（异步消息库）与另一节点上的前端进行初始握手，并接收协调地址信息。</li><li>初始化 DP 组（例如使用 NCCL 后端）。</li><li>使用 <code>MultiProcExecutor</code> (<code>TP=4</code> on 4 GPUs，如前所述）初始化 <code>EngineCore</code>。</li><li>创建一个 <code>ready_event</code> (<code>threading.Event</code>)。</li><li>启动一个运行 <code>process_input_sockets(…, ready_event)</code> 的输入守护线程 (<code>threading.Thread</code>)。类似地启动一个输出线程。</li><li>仍在主线程中，等待 <code>ready_event</code>，直到跨越 2 个节点的所有 4 个进程中的所有输入线程都完成了协调握手，最终执行 <code>ready_event.set()</code>。</li><li>一旦解除阻塞，就向前端发送一个“就绪”消息，其中包含元数据（例如，paged KV cache 内存中可用的 <code>num_gpu_blocks</code>）。</li><li>然后，主、输入和输出线程进入各自的忙碌循环。</li></ol><p>TL;DR: 我们最终得到 4 个子进程（每个 DP 副本一个），每个进程运行一个主、输入和输出线程。它们与 DP 协调器和前端完成协调握手，然后每个进程的三个线程都在稳态忙碌循环中运行。</p><p><img src="/JinBlog/assets/dpenginecoreproc-DZipxubK.png" alt="图 15：具有 4 个 DP 副本运行 4 个 DPEngineCoreProc 的分布式系统"></p><p><strong>当前稳态</strong>：</p><ul><li><strong>输入线程</strong> — 在输入套接字上阻塞，直到从 API 服务器路由一个请求；收到后，它解码有效负载，通过 <code>input_queue.put_nowait(...)</code> 将一个工作项排队，然后返回到在套接字上阻塞。</li><li><strong>主线程</strong> — 在 <code>input_queue.get(...)</code> 上唤醒，将请求馈送到引擎；<code>MultiProcExecutor</code> 运行前向传播并将结果排队到 <code>output_queue</code>。</li><li><strong>输出线程</strong> — 在 <code>output_queue.get(...)</code> 上唤醒，将结果发回 API 服务器，然后恢复阻塞。</li></ul><p><strong>附加机制</strong>：</p><ul><li><strong>DP wave counter</strong> — 系统跟踪“wave”；当所有引擎都变为空闲时，它们会静止下来，当新工作到达时，计数器会增加（对协调/指标有用）。</li><li><strong>控制消息</strong> — API 服务器可以发送的不仅仅是推理请求（例如，中止和实用程序/控制 RPC）。</li><li><strong>用于lockstep的虚拟步骤</strong> — 如果任何 DP 副本有工作，所有副本都执行一个前向步骤；没有请求的副本执行一个虚拟步骤以参与所需的同步点（避免阻塞活动副本）。</li></ul><div class="hint-container note"><p class="hint-container-title">注</p><p>Lockstep说明：这实际上只对 MoE 模型是必需的，其中 expert 层形成一个 EP 或 TP 组，而 attention 层仍然是 DP。目前总是与 DP 一起完成——这只是因为“内置”非 MoE DP 的用途有限，因为您可以只运行多个独立的 vLLM，并以正常方式在它们之间进行负载均衡。</p></div><p>现在是第二部分，API 服务器节点上发生了什么？</p><h3 id="在-api-服务器节点上" tabindex="-1"><a class="header-anchor" href="#在-api-服务器节点上"><span>在 API 服务器节点上</span></a></h3><p>我们实例化一个 <code>AsyncLLM</code> 对象（LLM 引擎的 asyncio 包装器）。在内部，这会创建一个 <code>DPLBAsyncMPClient</code>（data-parallel、load-balancing、asynchronous、multiprocessing client）。</p><p>在 <code>MPClient</code> 的父类中，<code>launch_core_engines</code> 函数运行并：</p><ol><li>创建用于启动握手的 ZMQ 地址（如在无头节点上所见）。</li><li>派生一个 <code>DPCoordinator</code> 进程。</li><li>创建一个 <code>CoreEngineProcManager</code>（与无头节点上相同）。</li></ol><p>在 <code>AsyncMPClient</code> (<code>MPClient</code> 的子类) 中，我们：</p><ol><li>创建一个 <code>outputs_queue</code> (<code>asyncio.Queue</code>)。</li><li>我们创建一个 asyncio 任务 <code>process_outputs_socket</code>，它（通过输出套接字）与所有 4 个 <code>DPEngineCoreProc</code> 的输出线程通信，并写入 <code>outputs_queue</code>。</li><li>随后，<code>AsyncLLM</code> 的另一个 asyncio 任务 <code>output_handler</code> 从此队列中读取，并最终将信息发送到 <code>create_completion</code> 函数。</li></ol><p>在 <code>DPAsyncMPClient</code> 中，我们创建一个 asyncio 任务 <code>run_engine_stats_update_task</code>，它与 DP 协调器通信。</p><p>DP 协调器在前端（API 服务器）和后端（引擎核心）之间进行协调。它：</p><ul><li>定期向前端的 <code>run_engine_stats_update_task</code> 发送负载均衡信息（队列大小、等待/运行中的请求）。</li><li>通过动态更改引擎数量来处理来自前端的 <code>SCALE_ELASTIC_EP</code> 命令（仅适用于 Ray 后端）。</li><li>向后端发送 <code>START_DP_WAVE</code> 事件（由前端触发）并报告 wave-state 更新。</li></ul><p>总而言之，前端 (<code>AsyncLLM</code>) 运行多个 asyncio 任务（请记住：并发，非并行）：</p><ul><li>一类任务通过 <code>generate</code> 路径处理输入请求（每个新的客户端请求都会派生一个新的 asyncio 任务）。</li><li>两个任务（<code>process_outputs_socket</code>、<code>output_handler</code>）处理来自底层引擎的输出消息。</li><li>一个任务（<code>run_engine_stats_update_task</code>）维护与 DP 协调器的通信：发送 wave 触发器、轮询 LB 状态以及处理动态扩展请求。</li></ul><p>最后，主服务器进程创建一个 FastAPI 应用程序并挂载诸如 <code>OpenAIServingCompletion</code> 和 <code>OpenAIServingChat</code> 之类的端点，这些端点公开 <code>/completion</code>、<code>/chat/completion</code> 等。然后通过 Uvicorn 提供堆栈服务。</p><p>那么，把所有东西放在一起，这就是完整的请求生命周期！</p><p>您从终端发送：</p><div class="language-bash line-numbers-mode" data-highlighter="shiki" data-ext="bash" style="--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212;"><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code class="language-bash"><span class="line"><span style="--shiki-light:#59873A;--shiki-dark:#80A665;">curl</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> -X</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;"> POST</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;"> http://localhost:8000/v1/completions</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> -H</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;"> &quot;</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">Content-Type: application/json</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> -d</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;"> &#39;</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">{</span></span>
<span class="line"><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">  &quot;model&quot;: &quot;TinyLlama/TinyLlama-1.1B-Chat-v1.0&quot;,</span></span>
<span class="line"><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">  &quot;prompt&quot;: &quot;The capital of France is&quot;,</span></span>
<span class="line"><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">  &quot;max_tokens&quot;: 50,</span></span>
<span class="line"><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">  &quot;temperature&quot;: 0.7</span></span>
<span class="line"><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">}</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&#39;</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>接下来会发生什么：</p><ol><li>请求到达 API 服务器上 <code>OpenAIServingCompletion</code> 的 <code>create_completion</code> 路由。</li><li>该函数异步地对prompt进行分词，并准备元数据（请求 ID、采样参数、时间戳等）。</li><li>然后它调用 <code>AsyncLLM.generate</code>，其流程与同步引擎相同，最终调用 <code>DPAsyncMPClient.add_request_async</code>。</li><li>这反过来又调用 <code>get_core_engine_for_request</code>，它根据 DP 协调器的状态在引擎之间进行负载均衡（选择得分最低/负载最低的那个：<code>score = len(waiting) * 4 + len(running)</code>）。</li><li><code>ADD</code> 请求被发送到所选引擎的 <code>input_socket</code>。</li><li>在该引擎处： <ul><li>输入线程 — 解除阻塞，从输入套接字解码数据，并将一个工作项放入主线程的 <code>input_queue</code> 中。</li><li>主线程 — 在 <code>input_queue</code> 上解除阻塞，将请求添加到引擎，并重复调用 <code>engine_core.step()</code>，将中间结果排队到 <code>output_queue</code>，直到满足停止条件。</li><li>输出线程 — 在 <code>output_queue</code> 上解除阻塞，并通过输出套接字发回结果。</li></ul></li></ol><div class="hint-container note"><p class="hint-container-title">注</p><p>提醒：<code>step()</code> 调用调度器、Model Executor（它本身可以是 <code>MultiProcExecutor</code>！）等。我们已经看到过这个了！</p></div><ol start="7"><li>这些结果触发了 <code>AsyncLLM</code> 输出 asyncio 任务（<code>process_outputs_socket</code> 和 <code>output_handler</code>），这些任务将 token 传播回 FastAPI 的 <code>create_completion</code> 路由。</li><li>FastAPI 附加元数据（完成原因、logprobs、使用信息等）并通过 Uvicorn 向您的终端返回一个 <code>JSONResponse</code>！</li></ol><p>就这样，您的补全回来了——整个分布式机制隐藏在一个简单的 <code>curl</code> 命令后面！😃 太有趣了！！！</p><blockquote><p><strong>附加说明：</strong></p><ul><li>当添加更多 API 服务器时，负载均衡在 OS/套接字级别处理。从应用程序的角度来看，没有重大变化——复杂性是隐藏的。</li><li>使用 Ray 作为 DP 后端，您可以公开一个 URL 端点（<code>/scale_elastic_ep</code>），该端点可以自动扩展引擎副本的数量。</li></ul></blockquote><h2 id="基准测试和自动调优-延迟vs吞吐量" tabindex="-1"><a class="header-anchor" href="#基准测试和自动调优-延迟vs吞吐量"><span>基准测试和自动调优-延迟vs吞吐量</span></a></h2><p>到目前为止，我们一直在分析“气体颗粒”——请求如何在引擎/系统中流动的内部原理。现在是时候放大并从整体上看待系统，并问：我们如何衡量一个推理系统的性能？</p><p>在最高层次上，有两个相互竞争的指标：</p><ol><li><strong>延迟</strong> — 从提交请求到返回 token 的时间</li><li><strong>吞吐量</strong> — 系统每秒可以生成/处理的 token/请求数</li></ol><p><strong>延迟</strong>对于交互式应用程序最重要，用户在这些应用程序中等待响应。</p><p><strong>吞吐量</strong>在离线工作负载中很重要，例如用于预/后训练运行的合成数据生成、数据清理/处理，以及通常——任何类型的离线批处理推理作业。</p><p>在解释为什么延迟和吞吐量相互竞争之前，让我们定义一些常见的推理指标：</p><table><thead><tr><th>指标</th><th>定义</th></tr></thead><tbody><tr><td><code>TTFT</code> (首个 token 时间)</td><td>从提交请求到收到第一个输出 token 的时间</td></tr><tr><td><code>ITL</code> (token 间延迟)</td><td>两个连续 token 之间的时间（例如，从 token i-1 到 token i）</td></tr><tr><td><code>TPOT</code> (每个输出 token 的时间)</td><td>请求中所有输出 token 的<strong>平均 ITL</strong></td></tr><tr><td><code>Latency / E2E</code> (端到端延迟)</td><td>处理请求的总时间，即 TTFT + 所有 ITL 的总和，或者等效地，提交请求和接收最后一个输出 token 之间的时间</td></tr><tr><td><code>Throughput</code></td><td>每秒处理的总 token 数（输入、输出或两者），或者每秒请求数</td></tr><tr><td><code>Goodput</code></td><td>满足服务级别目标 (SLO) 的吞吐量，例如最大 TTFT、TPOT 或 e2e 延迟。例如，只计算满足这些 SLO 的请求中的 token</td></tr></tbody></table><p><img src="/JinBlog/assets/latency_diagram-DDpsjyet.png" alt="图 16：ttft, itl, e2e 延迟"></p><p>这是一个解释这两个指标竞争性质的简化模型。</p><blockquote><p><strong>假设：</strong> 权重 I/O 而不是 KV cache I/O 占主导地位；即我们处理的是短序列。</p></blockquote><p>当观察批处理大小 <code>B</code> 如何影响单个 decode 步骤时，这种权衡变得清晰。当 <code>B ↓</code> 趋向于 1 时，ITL 下降：每一步的工作量减少，token 不会与其他 token “竞争”。当 <code>B ↑</code> 趋向于无穷大时，ITL 上升，因为我们每一步做更多的 FLOPs——但吞吐量提高了（直到我们达到峰值性能），因为权重 I/O 在更多的 token 上被摊销了。</p><p>一个 roofline 模型有助于理解这一点：在饱和批次 <code>B_sat</code> 以下，步骤时间由 HBM 带宽主导（逐层将权重流式传输到片上内存），因此步骤延迟几乎是平坦的——计算 1 个 vs 10 个 token 可能需要相似的时间。超过 <code>B_sat</code>，kernel 变得受计算限制，步骤时间大致随 <code>B</code> 增长；每个额外的 token 都会增加 ITL。</p><p><img src="/JinBlog/assets/roofline-CiYKDuRf.png" alt="图 17：roofline 性能模型"></p><blockquote><p><strong>注意：</strong> 为了更严谨地处理，我们必须考虑 kernel 自动调优：随着 <code>B</code> 的增长，运行时可能会切换到对该形状更有效的 kernel，从而改变实现的性能 <code>P_kernel</code>。步骤延迟为 <code>t = FLOPs_step / P_kernel</code>，其中 <code>FLOPs_step</code> 是该步骤中的工作量。您可以看到，当 <code>P_kernel</code> 达到 <code>P_peak</code> 时，每一步更多的计算将直接导致延迟增加。</p></blockquote><h3 id="如何在-vllm-中进行基准测试" tabindex="-1"><a class="header-anchor" href="#如何在-vllm-中进行基准测试"><span>如何在 vLLM 中进行基准测试</span></a></h3><p>vLLM 提供了一个 <code>vllm bench {serve,latency,throughput}</code> CLI，它包装了 vllm / benchmarks / {server,latency,throughput}.py。</p><p>以下是这些脚本的作用：</p><ul><li><strong>latency</strong> — 使用短输入（默认为 32 个 token）并以小批量（默认为 8）采样 128 个输出 token。它运行几次迭代并报告该批次的 e2e 延迟。</li><li><strong>throughput</strong> — 一次性提交一组固定的prompt（默认：1000 个 ShareGPT 样本）（也称为 <code>QPS=Inf</code> 模式），并报告整个运行过程中的输入/输出/总 token 数和每秒请求数。</li><li><strong>serve</strong> — 启动一个 vLLM 服务器，并通过从泊松（或更一般的，伽马）分布中采样请求的到达间隔时间来模拟真实世界的工作负载。它在一个时间窗口内发送请求，测量我们讨论过的所有指标，并且可以选择性地强制执行服务器端最大并发数（通过一个信号量，例如将服务器限制为 64 个并发请求）。</li></ul><p>以下是如何运行延迟脚本的示例：</p><div class="language-bash line-numbers-mode" data-highlighter="shiki" data-ext="bash" style="--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212;"><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code class="language-bash"><span class="line"><span style="--shiki-light:#59873A;--shiki-dark:#80A665;">vllm</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;"> bench</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;"> latency</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> \</span></span>
<span class="line"><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">  --model</span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;"> &lt;</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">model-nam</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">e</span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">&gt;</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> \</span></span>
<span class="line"><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">  --input-tokens</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;"> 32</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> \</span></span>
<span class="line"><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">  --output-tokens</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;"> 128</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> \</span></span>
<span class="line"><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">  --batch-size</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;"> 8</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="hint-container note"><p class="hint-container-title">注</p><p>CI 中使用的基准测试配置位于 <code>.buildkite/nightly-benchmarks/tests</code> 下。</p></div><p>还有一个自动调优脚本，它驱动 serve 基准测试来找到满足目标 SLO（例如，“在保持 p99 e2e &lt; 500 ms 的同时最大化吞吐量”）的参数设置，并返回一个建议的配置。</p><h2 id="结语" tabindex="-1"><a class="header-anchor" href="#结语"><span>结语</span></a></h2><p>我们从基本的引擎核心 (<code>UniprocExecutor</code>) 开始，添加了像 speculative decoding 和 prefix caching 这样的高级功能，扩展到 <code>MultiProcExecutor</code> (具有 <code>TP/PP &gt; 1</code>)，最后横向扩展，将所有东西包装在异步引擎和分布式服务堆栈中——最后讨论了如何衡量系统性能。</p><p>vLLM 还包括我跳过的专门处理。例如：</p><ul><li><strong>多样化的硬件后端</strong>：TPU、AWS Neuron (Trainium/Inferentia) 等。</li><li><strong>架构/技术</strong>：<code>MLA</code>、<code>MoE</code>、encoder-decoder (例如 Whisper)、pooling/embedding 模型、<code>EPLB</code>、<code>m-RoPE</code>、<code>LoRA</code>、<code>ALiBi</code>、无注意力变体、滑动窗口注意力、多模态 LM 和状态空间模型 (例如 Mamba/Mamba-2, Jamba)</li><li><strong>TP/PP/SP</strong></li><li><strong>混合 KV-cache 逻辑</strong> (Jenga)，更复杂的采样方法如 beam sampling 等</li><li><strong>实验性</strong>：异步调度</li></ul><p>好处是，这些中的大多数都与上面描述的主流程正交——你几乎可以将它们视为“插件”（当然，在实践中存在一些耦合）。</p><p>我喜欢理解系统。话虽如此，在这个高度上，分辨率肯定会受到影响。在接下来的文章中，我将放大到特定的子系统并深入探讨细节。</p><blockquote><p><strong>联系我：</strong> 如果您在帖子中发现任何错误，请私信我 - 欢迎在 <a href="https://x.com/gordic_aleksa" title="null" target="_blank" rel="noopener noreferrer">X</a> 或 <a href="https://www.linkedin.com/in/aleksagordic/" title="null" target="_blank" rel="noopener noreferrer">LinkedIn</a> 上给我留言，或通过<a href="https://docs.google.com/forms/d/1z1fEirrN2xtGxAsJvptpM7yV4ByT5SF25S-XiMPrXNA" title="null" target="_blank" rel="noopener noreferrer">匿名反馈</a>留言。</p></blockquote><h3 id="致谢" tabindex="-1"><a class="header-anchor" href="#致谢"><span>致谢</span></a></h3><p>非常感谢 <a href="https://www.hyperstack.cloud/" title="null" target="_blank" rel="noopener noreferrer">Hyperstack</a> 在过去一年中为我的实验提供了 H100！</p><p>感谢 <a href="https://www.linkedin.com/in/nickhillprofile/" title="null" target="_blank" rel="noopener noreferrer">Nick Hill</a> (vLLM 核心贡献者, RedHat), <a href="https://github.com/youkaichao" title="null" target="_blank" rel="noopener noreferrer">Kaichao You</a> (vLLM 核心贡献者), <a href="https://x.com/marksaroufim" title="null" target="_blank" rel="noopener noreferrer">Mark Saroufim</a> (PyTorch), <a href="https://www.linkedin.com/in/kyle-kranen/" title="null" target="_blank" rel="noopener noreferrer">Kyle Krannen</a> (NVIDIA, Dynamo), 和 <a href="https://www.linkedin.com/in/ashish-vaswani-99892181/" title="null" target="_blank" rel="noopener noreferrer">Ashish Vaswani</a> 阅读了这篇博文的预发布版本并提供了反馈！</p><h3 id="参考文献" tabindex="-1"><a class="header-anchor" href="#参考文献"><span>参考文献</span></a></h3><ol><li>vLLM <a href="https://github.com/vllm-project/vllm" title="null" target="_blank" rel="noopener noreferrer">https://github.com/vllm-project/vllm</a></li><li>“Attention Is All You Need” <a href="https://arxiv.org/abs/1706.03762" title="null" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/1706.03762</a></li><li>“Efficient Memory Management for Large Language Model Serving with PagedAttention” <a href="https://arxiv.org/abs/2309.06180" title="null" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2309.06180</a></li><li>“DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model” <a href="https://arxiv.org/abs/2405.04434" title="null" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2405.04434</a></li><li>“Jenga: Effective Memory Management for Serving LLM with Heterogeneity” <a href="https://arxiv.org/abs/2503.18292" title="null" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2503.18292</a></li><li>“Orca: A Distributed Serving System for Transformer-Based Generative Models” <a href="https://www.usenix.org/conference/osdi22/presentation/yu" title="null" target="_blank" rel="noopener noreferrer">https://www.usenix.org/conference/osdi22/presentation/yu</a></li><li>“XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models” <a href="https://arxiv.org/abs/2411.15100" title="null" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2411.15100</a></li><li>“Accelerating Large Language Model Decoding with Speculative Sampling” <a href="https://arxiv.org/abs/2302.01318" title="null" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2302.01318</a></li><li>“EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty” <a href="https://arxiv.org/abs/2401.15077" title="null" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2401.15077</a></li><li>“Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads” <a href="https://arxiv.org/abs/2401.10774" title="null" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2401.10774</a></li><li>LMCache <a href="https://github.com/LMCache/LMCache" title="null" target="_blank" rel="noopener noreferrer">https://github.com/LMCache/LMCache</a></li></ol></div><!----><!----><!----></div></main><footer class="vp-doc-footer" data-v-50e98be9 data-v-f5809d15><!--[--><!--]--><div class="edit-info" data-v-f5809d15><div class="edit-link" data-v-f5809d15><a class="vp-link link no-icon vp-external-link-icon edit-link-button" href="https://github.com/yJader/JinBlog/edit/main/docs/dl_llm/Inside vLLM Anatomy of a High-Throughput LLM Inference System.md" target="_blank" rel="noreferrer" data-v-f5809d15><!--[--><span class="vpi-square-pen edit-link-icon" aria-label="edit icon" data-v-f5809d15></span> 编辑此页<!--]--></a></div><!----></div><div class="contributors" aria-label="Contributors" data-v-f5809d15><span class="contributors-label" data-v-f5809d15>贡献者: </span><span class="contributors-info" data-v-f5809d15><!--[--><!--[--><span class="contributor" data-v-f5809d15>yJader@mba</span><!----><!--]--><!--]--></span></div><nav class="prev-next" data-v-f5809d15><div class="pager" data-v-f5809d15><!----></div><div class="pager" data-v-f5809d15><a class="vp-link link pager-link next" href="/JinBlog/csdiy/10-414/" data-v-f5809d15><!--[--><span class="desc" data-v-f5809d15>下一页</span><span class="title" data-v-f5809d15><!----><span data-v-f5809d15>CMU 10-414 Deep Learning Systems</span></span><!--]--></a></div></nav></footer><!----><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><!--]--><button type="button" class="vp-back-to-top" aria-label="back to top" data-v-6fcf39b7 style="display:none;" data-v-9ccec6f7><span class="percent" data-allow-mismatch data-v-9ccec6f7>0%</span><span class="show icon vpi-back-to-top" data-v-9ccec6f7></span><svg aria-hidden="true" data-v-9ccec6f7><circle cx="50%" cy="50%" data-allow-mismatch style="stroke-dasharray:calc(0% - 12.566370614359172px) calc(314.1592653589793% - 12.566370614359172px);" data-v-9ccec6f7></circle></svg></button><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="sign down" class="vp-sign-down" aria-hidden="true" data-v-6fcf39b7 style="display:none;" data-v-8cbe16af><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" data-v-8cbe16af><path d="m19 11l-7 6l-7-6" data-v-8cbe16af></path><path d="m19 5l-7 6l-7-6" opacity="0.6" data-v-8cbe16af></path></g></svg><footer class="vp-footer has-sidebar" vp-footer data-v-6fcf39b7 data-v-ec2d43d3><!--[--><div class="container" data-v-ec2d43d3><div class="message" data-v-ec2d43d3>Powered by <a target="_blank" href="https://v2.vuepress.vuejs.org/">VuePress</a> & <a target="_blank" href="https://theme-plume.vuejs.press">vuepress-theme-plume</a></div><!----></div><!--]--></footer><!--[--><!--]--><!--]--></div><!----><!--]--><!--[--><!--]--><!--]--></div><script type="module" src="/JinBlog/assets/app-AuyBpmKP.js" defer></script></body></html>