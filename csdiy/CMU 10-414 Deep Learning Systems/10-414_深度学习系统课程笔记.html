
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="yJin's Blog">
      
      
        <meta name="author" content="yJader">
      
      
      
        <link rel="prev" href="index.html">
      
      
        <link rel="next" href="10-414%20Homework%E7%AC%94%E8%AE%B0.html">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.22">
    
    
      
        <title>Course Notes - JinBlog</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.84d31ad4.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      
  
  
    
      
      
    
  
    
      
      
    
  
    
      
      
    
  
    
      
      
    
  
    
      
      
    
  
    
      
      
    
  
    
      
      
    
  
    
      
      
    
  
    
      
      
    
  
    
      
      
    
  
    
      
      
    
  
    
      
      
    
  
  
  <style>:root{--md-admonition-icon--note:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M1%207.775V2.75C1%201.784%201.784%201%202.75%201h5.025c.464%200%20.91.184%201.238.513l6.25%206.25a1.75%201.75%200%200%201%200%202.474l-5.026%205.026a1.75%201.75%200%200%201-2.474%200l-6.25-6.25A1.75%201.75%200%200%201%201%207.775m1.5%200c0%20.066.026.13.073.177l6.25%206.25a.25.25%200%200%200%20.354%200l5.025-5.025a.25.25%200%200%200%200-.354l-6.25-6.25a.25.25%200%200%200-.177-.073H2.75a.25.25%200%200%200-.25.25ZM6%205a1%201%200%201%201%200%202%201%201%200%200%201%200-2%22/%3E%3C/svg%3E');--md-admonition-icon--abstract:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M2.5%201.75v11.5c0%20.138.112.25.25.25h3.17a.75.75%200%200%201%200%201.5H2.75A1.75%201.75%200%200%201%201%2013.25V1.75C1%20.784%201.784%200%202.75%200h8.5C12.216%200%2013%20.784%2013%201.75v7.736a.75.75%200%200%201-1.5%200V1.75a.25.25%200%200%200-.25-.25h-8.5a.25.25%200%200%200-.25.25m13.274%209.537zl-4.557%204.45a.75.75%200%200%201-1.055-.008l-1.943-1.95a.75.75%200%200%201%201.062-1.058l1.419%201.425%204.026-3.932a.75.75%200%201%201%201.048%201.074M4.75%204h4.5a.75.75%200%200%201%200%201.5h-4.5a.75.75%200%200%201%200-1.5M4%207.75A.75.75%200%200%201%204.75%207h2a.75.75%200%200%201%200%201.5h-2A.75.75%200%200%201%204%207.75%22/%3E%3C/svg%3E');--md-admonition-icon--info:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M0%208a8%208%200%201%201%2016%200A8%208%200%200%201%200%208m8-6.5a6.5%206.5%200%201%200%200%2013%206.5%206.5%200%200%200%200-13M6.5%207.75A.75.75%200%200%201%207.25%207h1a.75.75%200%200%201%20.75.75v2.75h.25a.75.75%200%200%201%200%201.5h-2a.75.75%200%200%201%200-1.5h.25v-2h-.25a.75.75%200%200%201-.75-.75M8%206a1%201%200%201%201%200-2%201%201%200%200%201%200%202%22/%3E%3C/svg%3E');--md-admonition-icon--tip:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M3.499.75a.75.75%200%200%201%201.5%200v.996C5.9%202.903%206.793%203.65%207.662%204.376l.24.202c-.036-.694.055-1.422.426-2.163C9.1.873%2010.794-.045%2012.622.26%2014.408.558%2016%201.94%2016%204.25c0%201.278-.954%202.575-2.44%202.734l.146.508.065.22c.203.701.412%201.455.476%202.226.142%201.707-.4%203.03-1.487%203.898C11.714%2014.671%2010.27%2015%208.75%2015h-6a.75.75%200%200%201%200-1.5h1.376a4.5%204.5%200%200%201-.563-1.191%203.84%203.84%200%200%201-.05-2.063%204.65%204.65%200%200%201-2.025-.293.75.75%200%200%201%20.525-1.406c1.357.507%202.376-.006%202.698-.318l.009-.01a.747.747%200%200%201%201.06%200%20.75.75%200%200%201-.012%201.074c-.912.92-.992%201.835-.768%202.586.221.74.745%201.337%201.196%201.621H8.75c1.343%200%202.398-.296%203.074-.836.635-.507%201.036-1.31.928-2.602-.05-.603-.216-1.224-.422-1.93l-.064-.221c-.12-.407-.246-.84-.353-1.29a2.4%202.4%200%200%201-.507-.441%203.1%203.1%200%200%201-.633-1.248.75.75%200%200%201%201.455-.364c.046.185.144.436.31.627.146.168.353.305.712.305.738%200%201.25-.615%201.25-1.25%200-1.47-.95-2.315-2.123-2.51-1.172-.196-2.227.387-2.706%201.345-.46.92-.27%201.774.019%203.062l.042.19.01.05c.348.443.666.949.94%201.553a.75.75%200%201%201-1.365.62c-.553-1.217-1.32-1.94-2.3-2.768L6.7%205.527c-.814-.68-1.75-1.462-2.692-2.619a3.7%203.7%200%200%200-1.023.88c-.406.495-.663%201.036-.722%201.508.116.122.306.21.591.239.388.038.797-.06%201.032-.19a.75.75%200%200%201%20.728%201.31c-.515.287-1.23.439-1.906.373-.682-.067-1.473-.38-1.879-1.193L.75%205.677V5.5c0-.984.48-1.94%201.077-2.664.46-.559%201.05-1.055%201.673-1.353z%22/%3E%3C/svg%3E');--md-admonition-icon--success:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M13.78%204.22a.75.75%200%200%201%200%201.06l-7.25%207.25a.75.75%200%200%201-1.06%200L2.22%209.28a.75.75%200%200%201%20.018-1.042.75.75%200%200%201%201.042-.018L6%2010.94l6.72-6.72a.75.75%200%200%201%201.06%200%22/%3E%3C/svg%3E');--md-admonition-icon--question:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M0%208a8%208%200%201%201%2016%200A8%208%200%200%201%200%208m8-6.5a6.5%206.5%200%201%200%200%2013%206.5%206.5%200%200%200%200-13M6.92%206.085h.001a.749.749%200%201%201-1.342-.67c.169-.339.436-.701.849-.977C6.845%204.16%207.369%204%208%204a2.76%202.76%200%200%201%201.637.525c.503.377.863.965.863%201.725%200%20.448-.115.83-.329%201.15-.205.307-.47.513-.692.662-.109.072-.22.138-.313.195l-.006.004a6%206%200%200%200-.26.16%201%201%200%200%200-.276.245.75.75%200%200%201-1.248-.832c.184-.264.42-.489.692-.661q.154-.1.313-.195l.007-.004c.1-.061.182-.11.258-.161a1%201%200%200%200%20.277-.245C8.96%206.514%209%206.427%209%206.25a.61.61%200%200%200-.262-.525A1.27%201.27%200%200%200%208%205.5c-.369%200-.595.09-.74.187a1%201%200%200%200-.34.398M9%2011a1%201%200%201%201-2%200%201%201%200%200%201%202%200%22/%3E%3C/svg%3E');--md-admonition-icon--warning:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M6.457%201.047c.659-1.234%202.427-1.234%203.086%200l6.082%2011.378A1.75%201.75%200%200%201%2014.082%2015H1.918a1.75%201.75%200%200%201-1.543-2.575Zm1.763.707a.25.25%200%200%200-.44%200L1.698%2013.132a.25.25%200%200%200%20.22.368h12.164a.25.25%200%200%200%20.22-.368Zm.53%203.996v2.5a.75.75%200%200%201-1.5%200v-2.5a.75.75%200%200%201%201.5%200M9%2011a1%201%200%201%201-2%200%201%201%200%200%201%202%200%22/%3E%3C/svg%3E');--md-admonition-icon--failure:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M2.344%202.343za8%208%200%200%201%2011.314%2011.314A8.002%208.002%200%200%201%20.234%2010.089a8%208%200%200%201%202.11-7.746m1.06%2010.253a6.5%206.5%200%201%200%209.108-9.275%206.5%206.5%200%200%200-9.108%209.275M6.03%204.97%208%206.94l1.97-1.97a.749.749%200%200%201%201.275.326.75.75%200%200%201-.215.734L9.06%208l1.97%201.97a.749.749%200%200%201-.326%201.275.75.75%200%200%201-.734-.215L8%209.06l-1.97%201.97a.749.749%200%200%201-1.275-.326.75.75%200%200%201%20.215-.734L6.94%208%204.97%206.03a.75.75%200%200%201%20.018-1.042.75.75%200%200%201%201.042-.018%22/%3E%3C/svg%3E');--md-admonition-icon--danger:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M9.504.43a1.516%201.516%200%200%201%202.437%201.713L10.415%205.5h2.123c1.57%200%202.346%201.909%201.22%203.004l-7.34%207.142a1.25%201.25%200%200%201-.871.354h-.302a1.25%201.25%200%200%201-1.157-1.723L5.633%2010.5H3.462c-1.57%200-2.346-1.909-1.22-3.004zm1.047%201.074L3.286%208.571A.25.25%200%200%200%203.462%209H6.75a.75.75%200%200%201%20.694%201.034l-1.713%204.188%206.982-6.793A.25.25%200%200%200%2012.538%207H9.25a.75.75%200%200%201-.683-1.06l2.008-4.418.003-.006-.004-.009-.006-.006-.008-.001q-.005%200-.009.004%22/%3E%3C/svg%3E');--md-admonition-icon--bug:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M4.72.22a.75.75%200%200%201%201.06%200l1%20.999a3.5%203.5%200%200%201%202.441%200l.999-1a.748.748%200%200%201%201.265.332.75.75%200%200%201-.205.729l-.775.776c.616.63.995%201.493.995%202.444v.327q0%20.15-.025.292c.408.14.764.392%201.029.722l1.968-.787a.75.75%200%200%201%20.556%201.392L13%207.258V9h2.25a.75.75%200%200%201%200%201.5H13v.5q-.002.615-.141%201.186l2.17.868a.75.75%200%200%201-.557%201.392l-2.184-.873A5%205%200%200%201%208%2016a5%205%200%200%201-4.288-2.427l-2.183.873a.75.75%200%200%201-.558-1.392l2.17-.868A5%205%200%200%201%203%2011v-.5H.75a.75.75%200%200%201%200-1.5H3V7.258L.971%206.446a.75.75%200%200%201%20.558-1.392l1.967.787c.265-.33.62-.583%201.03-.722a1.7%201.7%200%200%201-.026-.292V4.5c0-.951.38-1.814.995-2.444L4.72%201.28a.75.75%200%200%201%200-1.06m.53%206.28a.75.75%200%200%200-.75.75V11a3.5%203.5%200%201%200%207%200V7.25a.75.75%200%200%200-.75-.75ZM6.173%205h3.654A.17.17%200%200%200%2010%204.827V4.5a2%202%200%201%200-4%200v.327c0%20.096.077.173.173.173%22/%3E%3C/svg%3E');--md-admonition-icon--example:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M5%205.782V2.5h-.25a.75.75%200%200%201%200-1.5h6.5a.75.75%200%200%201%200%201.5H11v3.282l3.666%205.76C15.619%2013.04%2014.543%2015%2012.767%2015H3.233c-1.776%200-2.852-1.96-1.899-3.458Zm-2.4%206.565a.75.75%200%200%200%20.633%201.153h9.534a.75.75%200%200%200%20.633-1.153L12.225%2010.5h-8.45ZM9.5%202.5h-3V6c0%20.143-.04.283-.117.403L4.73%209h6.54L9.617%206.403A.75.75%200%200%201%209.5%206Z%22/%3E%3C/svg%3E');--md-admonition-icon--quote:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M1.75%202.5h10.5a.75.75%200%200%201%200%201.5H1.75a.75.75%200%200%201%200-1.5m4%205h8.5a.75.75%200%200%201%200%201.5h-8.5a.75.75%200%200%201%200-1.5m0%205h8.5a.75.75%200%200%201%200%201.5h-8.5a.75.75%200%200%201%200-1.5M2.5%207.75v6a.75.75%200%200%201-1.5%200v-6a.75.75%200%200%201%201.5%200%22/%3E%3C/svg%3E');}</style>



    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="orange">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#lec2-softmax" class="md-skip">
          跳转至
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="页眉">
    <a href="../../index.html" title="JinBlog" class="md-header__button md-logo" aria-label="JinBlog" data-md-component="logo">
      
  <img src="../../assests/jade_light.svg" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            JinBlog
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Course Notes
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="orange"  aria-label="暗色模式"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="暗色模式" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="indigo"  aria-label="亮色模式"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="亮色模式" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="查找">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="分享" aria-label="分享" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="清空当前内容" aria-label="清空当前内容" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/yJader/JinBlog" title="前往仓库" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    yJader/JinBlog
  </div>
</a>
      </div>
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="标签" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../index.html" class="md-tabs__link">
        
  
  
    
  
  Jin Blog

      </a>
    </li>
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../fzu_cs_course/index.html" class="md-tabs__link">
          
  
  
    
  
  FZU CS课程

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../dl_llm/Inside%20vLLM%20Anatomy%20of%20a%20High-Throughput%20LLM%20Inference%20System.html" class="md-tabs__link">
          
  
  
    
  
  DL-LLM

        </a>
      </li>
    
  

    
  

      
        
  
  
  
    
  
  
    
    
      
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="index.html" class="md-tabs__link">
          
  
  
    
  
  CSDIY公开课

        </a>
      </li>
    
  

    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../tools/Windows%E8%A3%85%E6%9C%BA%E7%AC%94%E8%AE%B0%28%E9%99%84%E8%BD%AF%E4%BB%B6%E6%8E%A8%E8%8D%90%29.html" class="md-tabs__link">
          
  
  
    
  
  工具分享

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../blog/index.html" class="md-tabs__link">
          
  
  
    
  
  Blog

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="导航栏" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../index.html" title="JinBlog" class="md-nav__button md-logo" aria-label="JinBlog" data-md-component="logo">
      
  <img src="../../assests/jade_light.svg" alt="logo">

    </a>
    JinBlog
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/yJader/JinBlog" title="前往仓库" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    yJader/JinBlog
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../index.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Jin Blog
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../fzu_cs_course/index.html" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    FZU CS课程
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_2" id="__nav_2_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            FZU CS课程
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_2" >
        
          
          <label class="md-nav__link" for="__nav_2_2" id="__nav_2_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    数字电路与逻辑设计
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_2">
            <span class="md-nav__icon md-icon"></span>
            数字电路与逻辑设计
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../fzu_cs_course/%E6%95%B0%E5%AD%97%E7%94%B5%E8%B7%AF%E4%B8%8E%E9%80%BB%E8%BE%91%E8%AE%BE%E8%AE%A1/%E6%95%B0%E5%AD%97%E7%94%B5%E8%B7%AF%E4%B8%8E%E9%80%BB%E8%BE%91%E8%AE%BE%E8%AE%A1.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    数字电路与逻辑设计
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../fzu_cs_course/%E6%95%B0%E5%AD%97%E7%94%B5%E8%B7%AF%E4%B8%8E%E9%80%BB%E8%BE%91%E8%AE%BE%E8%AE%A1/%E5%A4%8D%E4%B9%A0%E7%BA%B2%E8%A6%81.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    复习纲要
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../fzu_cs_course/%E7%A6%BB%E6%95%A3%E6%95%B0%E5%AD%A6/%E7%A6%BB%E6%95%A3%E6%95%B0%E5%AD%A6.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    离散数学
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_4" >
        
          
          <label class="md-nav__link" for="__nav_2_4" id="__nav_2_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    数据结构
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_4">
            <span class="md-nav__icon md-icon"></span>
            数据结构
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../fzu_cs_course/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    数据结构
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../fzu_cs_course/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E8%80%83%E8%AF%95%E6%9D%90%E6%96%99.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    数据结构考试材料
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_5" >
        
          
          <label class="md-nav__link" for="__nav_2_5" id="__nav_2_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    计算机组成原理
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_5">
            <span class="md-nav__icon md-icon"></span>
            计算机组成原理
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../fzu_cs_course/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    计算机组成原理
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../fzu_cs_course/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86%E8%80%83%E7%82%B9.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    计算机组成原理考点
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_6" >
        
          
          <label class="md-nav__link" for="__nav_2_6" id="__nav_2_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    计算机网络
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_6">
            <span class="md-nav__icon md-icon"></span>
            计算机网络
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../fzu_cs_course/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    计算机网络
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../fzu_cs_course/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%20%E8%AF%BE%E5%90%8E%E9%A2%98.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    课后题
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_7" >
        
          
          <label class="md-nav__link" for="__nav_2_7" id="__nav_2_7_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    操作系统
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_7">
            <span class="md-nav__icon md-icon"></span>
            操作系统
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../fzu_cs_course/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    操作系统
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../fzu_cs_course/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/NJU%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    NJU操作系统
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../fzu_cs_course/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E7%AE%80%E7%AD%94%E9%A2%98%20MDver.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    简答题
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_8" >
        
          
          <label class="md-nav__link" for="__nav_2_8" id="__nav_2_8_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    软件工程
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_8">
            <span class="md-nav__icon md-icon"></span>
            软件工程
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../fzu_cs_course/%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B/%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    软件工程
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../fzu_cs_course/%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B/%E4%B8%80%E4%BA%9B%E6%95%B4%E7%90%86.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    知识点
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_9" >
        
          
          <label class="md-nav__link" for="__nav_2_9" id="__nav_2_9_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    数据库系统原理
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_9_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_9">
            <span class="md-nav__icon md-icon"></span>
            数据库系统原理
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../fzu_cs_course/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F%E5%8E%9F%E7%90%86/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F%E5%8E%9F%E7%90%86.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    数据库系统原理
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../fzu_cs_course/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F%E5%8E%9F%E7%90%86/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%80%E7%AD%94%E9%A2%98.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    简答题
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../fzu_cs_course/%E5%9B%BE%E5%BD%A2%E5%AD%A6/%E5%9B%BE%E5%BD%A2%E5%AD%A6.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    计算机图形学
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_11" >
        
          
          <label class="md-nav__link" for="__nav_2_11" id="__nav_2_11_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    汇编&接口
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_11_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_11">
            <span class="md-nav__icon md-icon"></span>
            汇编&接口
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../fzu_cs_course/%E6%B1%87%E7%BC%96%26%E6%8E%A5%E5%8F%A3/%E6%B1%87%E7%BC%96%26%E6%8E%A5%E5%8F%A3.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    汇编&接口
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../fzu_cs_course/%E6%B1%87%E7%BC%96%26%E6%8E%A5%E5%8F%A3/%E6%B1%87%E7%BC%96%E6%A8%A1%E6%9D%BF.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    汇编模板
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_12" >
        
          
          <label class="md-nav__link" for="__nav_2_12" id="__nav_2_12_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    劳动实践(BearPi)
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_12_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_12">
            <span class="md-nav__icon md-icon"></span>
            劳动实践(BearPi)
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../fzu_cs_course/%E5%8A%B3%E5%8A%A8%E5%AE%9E%E8%B7%B5%28BearPi%29/%E5%8A%B3%E5%8A%A8%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    劳动实践(BearPi)笔记
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_13" >
        
          
          <label class="md-nav__link" for="__nav_2_13" id="__nav_2_13_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    数据挖掘
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_13_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_13">
            <span class="md-nav__icon md-icon"></span>
            数据挖掘
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../fzu_cs_course/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    数据挖掘
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_14" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../fzu_cs_course/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E7%BB%93%E6%9E%84/index.html" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    计算机系统结构
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_2_14" id="__nav_2_14_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_14_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_14">
            <span class="md-nav__icon md-icon"></span>
            计算机系统结构
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../fzu_cs_course/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E7%BB%93%E6%9E%84/%E7%B3%BB%E7%BB%9F%E7%BB%93%E6%9E%84.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    系统结构
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../fzu_cs_course/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E7%BB%93%E6%9E%84/%E5%A4%8D%E4%B9%A0%E6%8F%90%E7%BA%B2MAP.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    复习提纲MAP
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../fzu_cs_course/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E7%BB%93%E6%9E%84/%E7%AE%80%E7%AD%94%E9%A2%98.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    简答题
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../fzu_cs_course/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E7%BB%93%E6%9E%84/%E5%B0%8F%E6%B5%8B%E9%80%89%E5%A1%AB.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    小测选填
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_15" >
        
          
          <label class="md-nav__link" for="__nav_2_15" id="__nav_2_15_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    编译原理
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_15_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_15">
            <span class="md-nav__icon md-icon"></span>
            编译原理
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../fzu_cs_course/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    编译原理
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_16" >
        
          
          <label class="md-nav__link" for="__nav_2_16" id="__nav_2_16_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    计算方法(数值分析)
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_16_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_16">
            <span class="md-nav__icon md-icon"></span>
            计算方法(数值分析)
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../fzu_cs_course/%E8%AE%A1%E7%AE%97%E6%96%B9%E6%B3%95%28%E6%95%B0%E5%80%BC%E5%88%86%E6%9E%90%29/%E8%AE%A1%E7%AE%97%E6%96%B9%E6%B3%95%28%E6%95%B0%E5%80%BC%E5%88%86%E6%9E%90%29.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    计算方法(数值分析)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../fzu_cs_course/%E8%AE%A1%E7%AE%97%E6%96%B9%E6%B3%95%28%E6%95%B0%E5%80%BC%E5%88%86%E6%9E%90%29/%E5%85%AC%E5%BC%8F%E6%95%B4%E7%90%86.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    公式整理
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_17" >
        
          
          <label class="md-nav__link" for="__nav_2_17" id="__nav_2_17_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    人机交互
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_17_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_17">
            <span class="md-nav__icon md-icon"></span>
            人机交互
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../fzu_cs_course/%E4%BA%BA%E6%9C%BA%E4%BA%A4%E4%BA%92/%E5%A4%8D%E4%B9%A0%E8%AF%BE%E4%BB%B6%20MD%20Ver.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    人机交互
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../fzu_cs_course/%E4%BA%BA%E6%9C%BA%E4%BA%A4%E4%BA%92/%E7%BC%A9%E5%87%8F%E7%89%88.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    人机交互考点
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_18" >
        
          
          <label class="md-nav__link" for="__nav_2_18" id="__nav_2_18_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    毕业设计
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_18_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_18">
            <span class="md-nav__icon md-icon"></span>
            毕业设计
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../fzu_cs_course/%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1/%E6%AF%95%E8%AE%BE%E8%A6%81%E6%B1%82%E6%95%B4%E7%90%86.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    毕业设计要求整理
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../fzu_cs_course/%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1/FZU-Undergraduate-Thesis.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    FZU-Undergraduate-Thesis
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../fzu_cs_course/%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1/FZU-SINTEF-Beamer-Template.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    FZU-SINTEF-Beamer-Template
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    DL-LLM
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            DL-LLM
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
      
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1" >
        
          
          <label class="md-nav__link" for="__nav_3_1" id="__nav_3_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    博客学习
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1">
            <span class="md-nav__icon md-icon"></span>
            博客学习
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../dl_llm/Inside%20vLLM%20Anatomy%20of%20a%20High-Throughput%20LLM%20Inference%20System.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    vLLM深入研究：一个高吞吐量 LLM 推理系统的剖析
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="index.html" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    CMU10-414/714: Deep Learning Systems
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3_2" id="__nav_3_2_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2">
            <span class="md-nav__icon md-icon"></span>
            CMU10-414/714: Deep Learning Systems
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Course Notes
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.html" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Course Notes
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#lec2-softmax" class="md-nav__link">
    <span class="md-ellipsis">
      Lec2 机器学习回顾/Softmax回归
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lec2 机器学习回顾/Softmax回归">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21" class="md-nav__link">
    <span class="md-ellipsis">
      2.1 机器学习基础
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.1 机器学习基础">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    <span class="md-ellipsis">
      数据驱动编程范式
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    <span class="md-ellipsis">
      机器学习三要素
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22-softmax" class="md-nav__link">
    <span class="md-ellipsis">
      2.2 Softmax回归示例
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.2 Softmax回归示例">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    <span class="md-ellipsis">
      多类分类问题设定
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    <span class="md-ellipsis">
      线性假设函数
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_5" class="md-nav__link">
    <span class="md-ellipsis">
      损失函数设计
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#23" class="md-nav__link">
    <span class="md-ellipsis">
      2.3 优化方法
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.3 优化方法">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_6" class="md-nav__link">
    <span class="md-ellipsis">
      梯度下降算法
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sgd" class="md-nav__link">
    <span class="md-ellipsis">
      随机梯度下降 (SGD)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#softmax" class="md-nav__link">
    <span class="md-ellipsis">
      softmax的梯度
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lec3" class="md-nav__link">
    <span class="md-ellipsis">
      Lec3 手动构建神经网络
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lec3 手动构建神经网络">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31" class="md-nav__link">
    <span class="md-ellipsis">
      3.1 从线性到非线性假设类
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3.1 从线性到非线性假设类">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_7" class="md-nav__link">
    <span class="md-ellipsis">
      线性分类器的局限性
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_8" class="md-nav__link">
    <span class="md-ellipsis">
      非线性特征
    </span>
  </a>
  
    <nav class="md-nav" aria-label="非线性特征">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_9" class="md-nav__link">
    <span class="md-ellipsis">
      核心思想
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_10" class="md-nav__link">
    <span class="md-ellipsis">
      特征构造方法对比
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_11" class="md-nav__link">
    <span class="md-ellipsis">
      关键非线性变换示例
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32" class="md-nav__link">
    <span class="md-ellipsis">
      3.2 神经网络
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3.2 神经网络">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_12" class="md-nav__link">
    <span class="md-ellipsis">
      基本定义
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_13" class="md-nav__link">
    <span class="md-ellipsis">
      神经网络结构
    </span>
  </a>
  
    <nav class="md-nav" aria-label="神经网络结构">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_14" class="md-nav__link">
    <span class="md-ellipsis">
      两层神经网络
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mlp" class="md-nav__link">
    <span class="md-ellipsis">
      多层感知机（MLP）
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_15" class="md-nav__link">
    <span class="md-ellipsis">
      深度网络优势分析
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#33" class="md-nav__link">
    <span class="md-ellipsis">
      3.3 反向传播
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3.3 反向传播">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_16" class="md-nav__link">
    <span class="md-ellipsis">
      梯度计算框架
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_17" class="md-nav__link">
    <span class="md-ellipsis">
      两层网络梯度推导
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_18" class="md-nav__link">
    <span class="md-ellipsis">
      通用反向传播算法
    </span>
  </a>
  
    <nav class="md-nav" aria-label="通用反向传播算法">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_19" class="md-nav__link">
    <span class="md-ellipsis">
      推导过程
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_20" class="md-nav__link">
    <span class="md-ellipsis">
      前向传播
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_21" class="md-nav__link">
    <span class="md-ellipsis">
      反向传播
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_22" class="md-nav__link">
    <span class="md-ellipsis">
      计算图视角
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_23" class="md-nav__link">
    <span class="md-ellipsis">
      关键公式总结
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lec4" class="md-nav__link">
    <span class="md-ellipsis">
      Lec4 自动微分
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lec4 自动微分">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#40" class="md-nav__link">
    <span class="md-ellipsis">
      4.0 微分在机器学习中的应用
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#41" class="md-nav__link">
    <span class="md-ellipsis">
      4.1 手动微分
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4.1 手动微分">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_24" class="md-nav__link">
    <span class="md-ellipsis">
      数值微分
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_25" class="md-nav__link">
    <span class="md-ellipsis">
      符号微分
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#42" class="md-nav__link">
    <span class="md-ellipsis">
      4.2 自动微分
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4.2 自动微分">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_26" class="md-nav__link">
    <span class="md-ellipsis">
      计算图
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#forward-ad" class="md-nav__link">
    <span class="md-ellipsis">
      前向模式自动微分 (Forward AD)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="前向模式自动微分 (Forward AD)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ad" class="md-nav__link">
    <span class="md-ellipsis">
      前向模式 AD 的局限性
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reverse-ad" class="md-nav__link">
    <span class="md-ellipsis">
      反向模式自动微分 (Reverse AD)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="反向模式自动微分 (Reverse AD)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_27" class="md-nav__link">
    <span class="md-ellipsis">
      多路径情况的推导
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ad_1" class="md-nav__link">
    <span class="md-ellipsis">
      反向 AD 算法
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ad_2" class="md-nav__link">
    <span class="md-ellipsis">
      通过扩展计算图实现反向模式 AD
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ad_3" class="md-nav__link">
    <span class="md-ellipsis">
      反向模式 AD 与反向传播的比较
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ad_4" class="md-nav__link">
    <span class="md-ellipsis">
      张量上的反向模式 AD
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_28" class="md-nav__link">
    <span class="md-ellipsis">
      梯度的梯度
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ad_5" class="md-nav__link">
    <span class="md-ellipsis">
      数据结构上的反向模式 AD
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lec6" class="md-nav__link">
    <span class="md-ellipsis">
      Lec6 全连接网络、优化与初始化
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lec6 全连接网络、优化与初始化">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#61-fully-connected-networks" class="md-nav__link">
    <span class="md-ellipsis">
      6.1 全连接网络（Fully Connected Networks）
    </span>
  </a>
  
    <nav class="md-nav" aria-label="6.1 全连接网络（Fully Connected Networks）">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mlp_1" class="md-nav__link">
    <span class="md-ellipsis">
      多层感知机（MLP）的数学定义
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#broadcasting" class="md-nav__link">
    <span class="md-ellipsis">
      矩阵形式与广播（Broadcasting）
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_29" class="md-nav__link">
    <span class="md-ellipsis">
      训练前必须回答的关键问题
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#62-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      6.2 优化算法（Optimization）
    </span>
  </a>
  
    <nav class="md-nav" aria-label="6.2 优化算法（Optimization）">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#gradient-descent" class="md-nav__link">
    <span class="md-ellipsis">
      梯度下降（Gradient Descent）
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#newtons-method" class="md-nav__link">
    <span class="md-ellipsis">
      牛顿法（Newton’s Method）
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#momentum" class="md-nav__link">
    <span class="md-ellipsis">
      动量法（Momentum）
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nesterov" class="md-nav__link">
    <span class="md-ellipsis">
      Nesterov 动量
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adam" class="md-nav__link">
    <span class="md-ellipsis">
      Adam 优化器
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sgd_1" class="md-nav__link">
    <span class="md-ellipsis">
      随机梯度下降（SGD）
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#63-initialization" class="md-nav__link">
    <span class="md-ellipsis">
      6.3 权重初始化（Initialization）
    </span>
  </a>
  
    <nav class="md-nav" aria-label="6.3 权重初始化（Initialization）">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#0" class="md-nav__link">
    <span class="md-ellipsis">
      为什么不能初始化为 0？
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_30" class="md-nav__link">
    <span class="md-ellipsis">
      随机初始化的影响
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_31" class="md-nav__link">
    <span class="md-ellipsis">
      权重移动并不大
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#kaiming" class="md-nav__link">
    <span class="md-ellipsis">
      方差推导与 Kaiming 初始化
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#kaiming_1" class="md-nav__link">
    <span class="md-ellipsis">
      Kaiming 初始化的推导
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Kaiming 初始化的推导">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1" class="md-nav__link">
    <span class="md-ellipsis">
      1. 基本设定
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2" class="md-nav__link">
    <span class="md-ellipsis">
      2. 方差推导
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-relu" class="md-nav__link">
    <span class="md-ellipsis">
      3. ReLU 对输入分布的影响
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4" class="md-nav__link">
    <span class="md-ellipsis">
      4. 保持方差一致的条件
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#64" class="md-nav__link">
    <span class="md-ellipsis">
      6.4 小结与后续
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lec7" class="md-nav__link">
    <span class="md-ellipsis">
      Lec7 神经网络库抽象
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lec7 神经网络库抽象">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#71-programming-abstractions" class="md-nav__link">
    <span class="md-ellipsis">
      7.1 编程抽象（Programming Abstractions）
    </span>
  </a>
  
    <nav class="md-nav" aria-label="7.1 编程抽象（Programming Abstractions）">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#case-studies" class="md-nav__link">
    <span class="md-ellipsis">
      案例研究（Case Studies）
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_32" class="md-nav__link">
    <span class="md-ellipsis">
      三种核心抽象模式
    </span>
  </a>
  
    <nav class="md-nav" aria-label="三种核心抽象模式">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-forward-and-backward-layer-interface" class="md-nav__link">
    <span class="md-ellipsis">
      1. 前向与反向层接口（Forward and backward layer interface）
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-computational-graph-and-declarative-programming" class="md-nav__link">
    <span class="md-ellipsis">
      2. 计算图与声明式编程（Computational graph and declarative programming）
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-imperative-automatic-differentiation" class="md-nav__link">
    <span class="md-ellipsis">
      3. 命令式自动微分（Imperative automatic differentiation）
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#discussions" class="md-nav__link">
    <span class="md-ellipsis">
      讨论（Discussions）
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#72-high-level-modular-library-components" class="md-nav__link">
    <span class="md-ellipsis">
      7.2 高级模块化库组件（High Level Modular Library Components）
    </span>
  </a>
  
    <nav class="md-nav" aria-label="7.2 高级模块化库组件（High Level Modular Library Components）">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#elements-of-machine-learning" class="md-nav__link">
    <span class="md-ellipsis">
      机器学习核心元素（Elements of Machine Learning）
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_33" class="md-nav__link">
    <span class="md-ellipsis">
      深度学习的模块化本质
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_34" class="md-nav__link">
    <span class="md-ellipsis">
      模块化组件设计
    </span>
  </a>
  
    <nav class="md-nav" aria-label="模块化组件设计">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-nnmodule" class="md-nav__link">
    <span class="md-ellipsis">
      1. nn.Module：组合模块
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2_1" class="md-nav__link">
    <span class="md-ellipsis">
      2. 损失函数：特殊模块
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-optimizer" class="md-nav__link">
    <span class="md-ellipsis">
      3. 优化器（Optimizer）
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4_1" class="md-nav__link">
    <span class="md-ellipsis">
      4. 正则化与优化器
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5-initialization" class="md-nav__link">
    <span class="md-ellipsis">
      5. 参数初始化（Initialization）
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#6-data-loader-and-preprocessing" class="md-nav__link">
    <span class="md-ellipsis">
      6. 数据加载与预处理（Data Loader and Preprocessing）
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#discussions_1" class="md-nav__link">
    <span class="md-ellipsis">
      讨论（Discussions）
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#revisit-programming-abstraction" class="md-nav__link">
    <span class="md-ellipsis">
      Revisit Programming Abstraction
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lec9" class="md-nav__link">
    <span class="md-ellipsis">
      Lec9 深度学习系统：归一化与正则化
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lec9 深度学习系统：归一化与正则化">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#91" class="md-nav__link">
    <span class="md-ellipsis">
      9.1 引言
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#92" class="md-nav__link">
    <span class="md-ellipsis">
      9.2 初始化与优化的交互
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#93-normalization" class="md-nav__link">
    <span class="md-ellipsis">
      9.3 归一化（Normalization）
    </span>
  </a>
  
    <nav class="md-nav" aria-label="9.3 归一化（Normalization）">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_35" class="md-nav__link">
    <span class="md-ellipsis">
      动机
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#931-layer-normalization" class="md-nav__link">
    <span class="md-ellipsis">
      9.3.1 层归一化（Layer Normalization）
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#932" class="md-nav__link">
    <span class="md-ellipsis">
      9.3.2 层归一化效果
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#933-batch-normalization" class="md-nav__link">
    <span class="md-ellipsis">
      9.3.3 批归一化（Batch Normalization）
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#934-minibatch-dependence" class="md-nav__link">
    <span class="md-ellipsis">
      9.3.4 小批量依赖性（Minibatch Dependence）
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#94-regularization" class="md-nav__link">
    <span class="md-ellipsis">
      9.4 正则化（Regularization）
    </span>
  </a>
  
    <nav class="md-nav" aria-label="9.4 正则化（Regularization）">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_36" class="md-nav__link">
    <span class="md-ellipsis">
      背景
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#941" class="md-nav__link">
    <span class="md-ellipsis">
      9.4.1 正则化的定义与分类
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#942-ell_2-weight-decay" class="md-nav__link">
    <span class="md-ellipsis">
      9.4.2 \(\ell_2\) 正则化（Weight Decay）
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#943-ell_2" class="md-nav__link">
    <span class="md-ellipsis">
      9.4.3 \(\ell_2\) 正则化的局限性
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#944-dropout" class="md-nav__link">
    <span class="md-ellipsis">
      9.4.4 Dropout
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#945-dropout" class="md-nav__link">
    <span class="md-ellipsis">
      9.4.5 Dropout 作为随机近似
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#95" class="md-nav__link">
    <span class="md-ellipsis">
      9.5 各因素的交互作用
    </span>
  </a>
  
    <nav class="md-nav" aria-label="9.5 各因素的交互作用">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#batchnorm-ali-rahimi" class="md-nav__link">
    <span class="md-ellipsis">
      BatchNorm 的反思（Ali Rahimi 演讲引用）
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#batchnorm" class="md-nav__link">
    <span class="md-ellipsis">
      BatchNorm 的其他观察
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#98" class="md-nav__link">
    <span class="md-ellipsis">
      9.8 总结与启示
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lec10-convolutional-networks" class="md-nav__link">
    <span class="md-ellipsis">
      Lec10 卷积网络 (Convolutional Networks)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lec10 卷积网络 (Convolutional Networks)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#101-convolutional-operators-in-deep-networks" class="md-nav__link">
    <span class="md-ellipsis">
      10.1 深度网络中的卷积算子 (Convolutional operators in deep networks)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="10.1 深度网络中的卷积算子 (Convolutional operators in deep networks)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#fully-connected-networks" class="md-nav__link">
    <span class="md-ellipsis">
      全连接网络 (fully connected networks) 的问题
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_37" class="md-nav__link">
    <span class="md-ellipsis">
      卷积如何“简化”深度网络
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_38" class="md-nav__link">
    <span class="md-ellipsis">
      卷积的优势
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_39" class="md-nav__link">
    <span class="md-ellipsis">
      卷积详解
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_40" class="md-nav__link">
    <span class="md-ellipsis">
      图像处理中的卷积
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_41" class="md-nav__link">
    <span class="md-ellipsis">
      深度网络中的卷积
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#-" class="md-nav__link">
    <span class="md-ellipsis">
      多通道卷积的矩阵-向量形式
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#102-elements-of-practical-convolutions" class="md-nav__link">
    <span class="md-ellipsis">
      10.2 实用卷积的要素 (Elements of practical convolutions)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="10.2 实用卷积的要素 (Elements of practical convolutions)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#padding" class="md-nav__link">
    <span class="md-ellipsis">
      Padding
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#strided-convolutions-pooling" class="md-nav__link">
    <span class="md-ellipsis">
      步幅卷积 (Strided Convolutions) / 池化 (Pooling)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#grouped-convolutions" class="md-nav__link">
    <span class="md-ellipsis">
      分组卷积 (Grouped Convolutions)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dilations" class="md-nav__link">
    <span class="md-ellipsis">
      扩张卷积 (Dilations)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#103-differentiating-convolutions" class="md-nav__link">
    <span class="md-ellipsis">
      10.3 卷积的微分 (Differentiating convolutions)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="10.3 卷积的微分 (Differentiating convolutions)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_42" class="md-nav__link">
    <span class="md-ellipsis">
      微分需要什么？
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_43" class="md-nav__link">
    <span class="md-ellipsis">
      回顾矩阵乘法的微分
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1_1" class="md-nav__link">
    <span class="md-ellipsis">
      将卷积视为矩阵乘法：版本 1
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adjoint" class="md-nav__link">
    <span class="md-ellipsis">
      卷积的伴随算子 (Adjoint)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2_2" class="md-nav__link">
    <span class="md-ellipsis">
      将卷积视为矩阵乘法：版本 2
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lec11" class="md-nav__link">
    <span class="md-ellipsis">
      Lec11 硬件加速
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lec11 硬件加速">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#111" class="md-nav__link">
    <span class="md-ellipsis">
      11.1 通用加速技术
    </span>
  </a>
  
    <nav class="md-nav" aria-label="11.1 通用加速技术">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_44" class="md-nav__link">
    <span class="md-ellipsis">
      机器学习框架中的层次
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vectorization" class="md-nav__link">
    <span class="md-ellipsis">
      Vectorization (向量化)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#data-layout-strides" class="md-nav__link">
    <span class="md-ellipsis">
      数据布局 (Data layout) 和步幅 (strides)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#strides" class="md-nav__link">
    <span class="md-ellipsis">
      关于 Strides 的讨论
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#parallelization" class="md-nav__link">
    <span class="md-ellipsis">
      Parallelization (并行化)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#112" class="md-nav__link">
    <span class="md-ellipsis">
      11.2 案例学习：矩阵乘法
    </span>
  </a>
  
    <nav class="md-nav" aria-label="11.2 案例学习：矩阵乘法">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vanilla-matrix-multiplication" class="md-nav__link">
    <span class="md-ellipsis">
      朴素矩阵乘法 (Vanilla matrix multiplication)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cpu-memory-hierarchy" class="md-nav__link">
    <span class="md-ellipsis">
      现代 CPU 的内存层次结构 (Memory hierarchy)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#architecture-aware-analysis" class="md-nav__link">
    <span class="md-ellipsis">
      体系结构感知分析 (Architecture aware analysis)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#register-tiled-matrix-multiplication" class="md-nav__link">
    <span class="md-ellipsis">
      寄存器分块矩阵乘法 (Register tiled matrix multiplication)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cache-line-aware-tiling" class="md-nav__link">
    <span class="md-ellipsis">
      缓存行感知分块 (Cache line aware tiling)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_45" class="md-nav__link">
    <span class="md-ellipsis">
      整合两种分块方法
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#memory-load-reuse" class="md-nav__link">
    <span class="md-ellipsis">
      核心思想：内存加载复用 (Memory load reuse)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_46" class="md-nav__link">
    <span class="md-ellipsis">
      常见的复用模式
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_47" class="md-nav__link">
    <span class="md-ellipsis">
      讨论：卷积中可能的复用模式
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lec12-gpu" class="md-nav__link">
    <span class="md-ellipsis">
      Lec12 GPU 加速
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lec12 GPU 加速">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#121-gpu" class="md-nav__link">
    <span class="md-ellipsis">
      12.1 GPU 编程
    </span>
  </a>
  
    <nav class="md-nav" aria-label="12.1 GPU 编程">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#gpu" class="md-nav__link">
    <span class="md-ellipsis">
      什么是 GPU
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpu-simt" class="md-nav__link">
    <span class="md-ellipsis">
      GPU 编程模型: SIMT
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_48" class="md-nav__link">
    <span class="md-ellipsis">
      示例: 向量加法
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpu_1" class="md-nav__link">
    <span class="md-ellipsis">
      其他 GPU 编程模型示例
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpu_2" class="md-nav__link">
    <span class="md-ellipsis">
      GPU 内存层级
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#window-sum" class="md-nav__link">
    <span class="md-ellipsis">
      示例: 窗口求和 (Window Sum)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_49" class="md-nav__link">
    <span class="md-ellipsis">
      核心要点
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#122-gpu" class="md-nav__link">
    <span class="md-ellipsis">
      12.2 案例学习: GPU 上的矩阵乘法
    </span>
  </a>
  
    <nav class="md-nav" aria-label="12.2 案例学习: GPU 上的矩阵乘法">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#register-tiling" class="md-nav__link">
    <span class="md-ellipsis">
      线程级别优化: Register Tiling
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#shared-memory-tiling" class="md-nav__link">
    <span class="md-ellipsis">
      块级别优化: Shared Memory Tiling
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpu_3" class="md-nav__link">
    <span class="md-ellipsis">
      更多 GPU 优化技巧
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="10-414%20Homework%E7%AC%94%E8%AE%B0.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Homework Notes
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
      
        
      
    
    
    
      
        
        
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" checked>
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    CSDIY公开课
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            CSDIY公开课
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    
    
      
        
          
        
      
        
      
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_1" checked>
        
          
          <div class="md-nav__link md-nav__container">
            <a href="index.html" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    CMU10-414/714: Deep Learning Systems
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_4_1" id="__nav_4_1_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_1_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_4_1">
            <span class="md-nav__icon md-icon"></span>
            CMU10-414/714: Deep Learning Systems
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Course Notes
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.html" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Course Notes
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#lec2-softmax" class="md-nav__link">
    <span class="md-ellipsis">
      Lec2 机器学习回顾/Softmax回归
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lec2 机器学习回顾/Softmax回归">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21" class="md-nav__link">
    <span class="md-ellipsis">
      2.1 机器学习基础
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.1 机器学习基础">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    <span class="md-ellipsis">
      数据驱动编程范式
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    <span class="md-ellipsis">
      机器学习三要素
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22-softmax" class="md-nav__link">
    <span class="md-ellipsis">
      2.2 Softmax回归示例
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.2 Softmax回归示例">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    <span class="md-ellipsis">
      多类分类问题设定
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    <span class="md-ellipsis">
      线性假设函数
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_5" class="md-nav__link">
    <span class="md-ellipsis">
      损失函数设计
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#23" class="md-nav__link">
    <span class="md-ellipsis">
      2.3 优化方法
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.3 优化方法">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_6" class="md-nav__link">
    <span class="md-ellipsis">
      梯度下降算法
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sgd" class="md-nav__link">
    <span class="md-ellipsis">
      随机梯度下降 (SGD)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#softmax" class="md-nav__link">
    <span class="md-ellipsis">
      softmax的梯度
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lec3" class="md-nav__link">
    <span class="md-ellipsis">
      Lec3 手动构建神经网络
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lec3 手动构建神经网络">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31" class="md-nav__link">
    <span class="md-ellipsis">
      3.1 从线性到非线性假设类
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3.1 从线性到非线性假设类">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_7" class="md-nav__link">
    <span class="md-ellipsis">
      线性分类器的局限性
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_8" class="md-nav__link">
    <span class="md-ellipsis">
      非线性特征
    </span>
  </a>
  
    <nav class="md-nav" aria-label="非线性特征">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_9" class="md-nav__link">
    <span class="md-ellipsis">
      核心思想
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_10" class="md-nav__link">
    <span class="md-ellipsis">
      特征构造方法对比
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_11" class="md-nav__link">
    <span class="md-ellipsis">
      关键非线性变换示例
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32" class="md-nav__link">
    <span class="md-ellipsis">
      3.2 神经网络
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3.2 神经网络">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_12" class="md-nav__link">
    <span class="md-ellipsis">
      基本定义
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_13" class="md-nav__link">
    <span class="md-ellipsis">
      神经网络结构
    </span>
  </a>
  
    <nav class="md-nav" aria-label="神经网络结构">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_14" class="md-nav__link">
    <span class="md-ellipsis">
      两层神经网络
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mlp" class="md-nav__link">
    <span class="md-ellipsis">
      多层感知机（MLP）
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_15" class="md-nav__link">
    <span class="md-ellipsis">
      深度网络优势分析
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#33" class="md-nav__link">
    <span class="md-ellipsis">
      3.3 反向传播
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3.3 反向传播">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_16" class="md-nav__link">
    <span class="md-ellipsis">
      梯度计算框架
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_17" class="md-nav__link">
    <span class="md-ellipsis">
      两层网络梯度推导
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_18" class="md-nav__link">
    <span class="md-ellipsis">
      通用反向传播算法
    </span>
  </a>
  
    <nav class="md-nav" aria-label="通用反向传播算法">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_19" class="md-nav__link">
    <span class="md-ellipsis">
      推导过程
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_20" class="md-nav__link">
    <span class="md-ellipsis">
      前向传播
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_21" class="md-nav__link">
    <span class="md-ellipsis">
      反向传播
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_22" class="md-nav__link">
    <span class="md-ellipsis">
      计算图视角
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_23" class="md-nav__link">
    <span class="md-ellipsis">
      关键公式总结
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lec4" class="md-nav__link">
    <span class="md-ellipsis">
      Lec4 自动微分
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lec4 自动微分">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#40" class="md-nav__link">
    <span class="md-ellipsis">
      4.0 微分在机器学习中的应用
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#41" class="md-nav__link">
    <span class="md-ellipsis">
      4.1 手动微分
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4.1 手动微分">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_24" class="md-nav__link">
    <span class="md-ellipsis">
      数值微分
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_25" class="md-nav__link">
    <span class="md-ellipsis">
      符号微分
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#42" class="md-nav__link">
    <span class="md-ellipsis">
      4.2 自动微分
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4.2 自动微分">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_26" class="md-nav__link">
    <span class="md-ellipsis">
      计算图
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#forward-ad" class="md-nav__link">
    <span class="md-ellipsis">
      前向模式自动微分 (Forward AD)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="前向模式自动微分 (Forward AD)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ad" class="md-nav__link">
    <span class="md-ellipsis">
      前向模式 AD 的局限性
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reverse-ad" class="md-nav__link">
    <span class="md-ellipsis">
      反向模式自动微分 (Reverse AD)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="反向模式自动微分 (Reverse AD)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_27" class="md-nav__link">
    <span class="md-ellipsis">
      多路径情况的推导
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ad_1" class="md-nav__link">
    <span class="md-ellipsis">
      反向 AD 算法
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ad_2" class="md-nav__link">
    <span class="md-ellipsis">
      通过扩展计算图实现反向模式 AD
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ad_3" class="md-nav__link">
    <span class="md-ellipsis">
      反向模式 AD 与反向传播的比较
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ad_4" class="md-nav__link">
    <span class="md-ellipsis">
      张量上的反向模式 AD
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_28" class="md-nav__link">
    <span class="md-ellipsis">
      梯度的梯度
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ad_5" class="md-nav__link">
    <span class="md-ellipsis">
      数据结构上的反向模式 AD
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lec6" class="md-nav__link">
    <span class="md-ellipsis">
      Lec6 全连接网络、优化与初始化
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lec6 全连接网络、优化与初始化">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#61-fully-connected-networks" class="md-nav__link">
    <span class="md-ellipsis">
      6.1 全连接网络（Fully Connected Networks）
    </span>
  </a>
  
    <nav class="md-nav" aria-label="6.1 全连接网络（Fully Connected Networks）">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mlp_1" class="md-nav__link">
    <span class="md-ellipsis">
      多层感知机（MLP）的数学定义
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#broadcasting" class="md-nav__link">
    <span class="md-ellipsis">
      矩阵形式与广播（Broadcasting）
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_29" class="md-nav__link">
    <span class="md-ellipsis">
      训练前必须回答的关键问题
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#62-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      6.2 优化算法（Optimization）
    </span>
  </a>
  
    <nav class="md-nav" aria-label="6.2 优化算法（Optimization）">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#gradient-descent" class="md-nav__link">
    <span class="md-ellipsis">
      梯度下降（Gradient Descent）
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#newtons-method" class="md-nav__link">
    <span class="md-ellipsis">
      牛顿法（Newton’s Method）
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#momentum" class="md-nav__link">
    <span class="md-ellipsis">
      动量法（Momentum）
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nesterov" class="md-nav__link">
    <span class="md-ellipsis">
      Nesterov 动量
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adam" class="md-nav__link">
    <span class="md-ellipsis">
      Adam 优化器
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sgd_1" class="md-nav__link">
    <span class="md-ellipsis">
      随机梯度下降（SGD）
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#63-initialization" class="md-nav__link">
    <span class="md-ellipsis">
      6.3 权重初始化（Initialization）
    </span>
  </a>
  
    <nav class="md-nav" aria-label="6.3 权重初始化（Initialization）">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#0" class="md-nav__link">
    <span class="md-ellipsis">
      为什么不能初始化为 0？
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_30" class="md-nav__link">
    <span class="md-ellipsis">
      随机初始化的影响
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_31" class="md-nav__link">
    <span class="md-ellipsis">
      权重移动并不大
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#kaiming" class="md-nav__link">
    <span class="md-ellipsis">
      方差推导与 Kaiming 初始化
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#kaiming_1" class="md-nav__link">
    <span class="md-ellipsis">
      Kaiming 初始化的推导
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Kaiming 初始化的推导">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1" class="md-nav__link">
    <span class="md-ellipsis">
      1. 基本设定
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2" class="md-nav__link">
    <span class="md-ellipsis">
      2. 方差推导
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-relu" class="md-nav__link">
    <span class="md-ellipsis">
      3. ReLU 对输入分布的影响
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4" class="md-nav__link">
    <span class="md-ellipsis">
      4. 保持方差一致的条件
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#64" class="md-nav__link">
    <span class="md-ellipsis">
      6.4 小结与后续
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lec7" class="md-nav__link">
    <span class="md-ellipsis">
      Lec7 神经网络库抽象
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lec7 神经网络库抽象">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#71-programming-abstractions" class="md-nav__link">
    <span class="md-ellipsis">
      7.1 编程抽象（Programming Abstractions）
    </span>
  </a>
  
    <nav class="md-nav" aria-label="7.1 编程抽象（Programming Abstractions）">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#case-studies" class="md-nav__link">
    <span class="md-ellipsis">
      案例研究（Case Studies）
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_32" class="md-nav__link">
    <span class="md-ellipsis">
      三种核心抽象模式
    </span>
  </a>
  
    <nav class="md-nav" aria-label="三种核心抽象模式">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-forward-and-backward-layer-interface" class="md-nav__link">
    <span class="md-ellipsis">
      1. 前向与反向层接口（Forward and backward layer interface）
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-computational-graph-and-declarative-programming" class="md-nav__link">
    <span class="md-ellipsis">
      2. 计算图与声明式编程（Computational graph and declarative programming）
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-imperative-automatic-differentiation" class="md-nav__link">
    <span class="md-ellipsis">
      3. 命令式自动微分（Imperative automatic differentiation）
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#discussions" class="md-nav__link">
    <span class="md-ellipsis">
      讨论（Discussions）
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#72-high-level-modular-library-components" class="md-nav__link">
    <span class="md-ellipsis">
      7.2 高级模块化库组件（High Level Modular Library Components）
    </span>
  </a>
  
    <nav class="md-nav" aria-label="7.2 高级模块化库组件（High Level Modular Library Components）">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#elements-of-machine-learning" class="md-nav__link">
    <span class="md-ellipsis">
      机器学习核心元素（Elements of Machine Learning）
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_33" class="md-nav__link">
    <span class="md-ellipsis">
      深度学习的模块化本质
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_34" class="md-nav__link">
    <span class="md-ellipsis">
      模块化组件设计
    </span>
  </a>
  
    <nav class="md-nav" aria-label="模块化组件设计">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-nnmodule" class="md-nav__link">
    <span class="md-ellipsis">
      1. nn.Module：组合模块
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2_1" class="md-nav__link">
    <span class="md-ellipsis">
      2. 损失函数：特殊模块
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-optimizer" class="md-nav__link">
    <span class="md-ellipsis">
      3. 优化器（Optimizer）
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4_1" class="md-nav__link">
    <span class="md-ellipsis">
      4. 正则化与优化器
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5-initialization" class="md-nav__link">
    <span class="md-ellipsis">
      5. 参数初始化（Initialization）
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#6-data-loader-and-preprocessing" class="md-nav__link">
    <span class="md-ellipsis">
      6. 数据加载与预处理（Data Loader and Preprocessing）
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#discussions_1" class="md-nav__link">
    <span class="md-ellipsis">
      讨论（Discussions）
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#revisit-programming-abstraction" class="md-nav__link">
    <span class="md-ellipsis">
      Revisit Programming Abstraction
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lec9" class="md-nav__link">
    <span class="md-ellipsis">
      Lec9 深度学习系统：归一化与正则化
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lec9 深度学习系统：归一化与正则化">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#91" class="md-nav__link">
    <span class="md-ellipsis">
      9.1 引言
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#92" class="md-nav__link">
    <span class="md-ellipsis">
      9.2 初始化与优化的交互
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#93-normalization" class="md-nav__link">
    <span class="md-ellipsis">
      9.3 归一化（Normalization）
    </span>
  </a>
  
    <nav class="md-nav" aria-label="9.3 归一化（Normalization）">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_35" class="md-nav__link">
    <span class="md-ellipsis">
      动机
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#931-layer-normalization" class="md-nav__link">
    <span class="md-ellipsis">
      9.3.1 层归一化（Layer Normalization）
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#932" class="md-nav__link">
    <span class="md-ellipsis">
      9.3.2 层归一化效果
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#933-batch-normalization" class="md-nav__link">
    <span class="md-ellipsis">
      9.3.3 批归一化（Batch Normalization）
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#934-minibatch-dependence" class="md-nav__link">
    <span class="md-ellipsis">
      9.3.4 小批量依赖性（Minibatch Dependence）
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#94-regularization" class="md-nav__link">
    <span class="md-ellipsis">
      9.4 正则化（Regularization）
    </span>
  </a>
  
    <nav class="md-nav" aria-label="9.4 正则化（Regularization）">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_36" class="md-nav__link">
    <span class="md-ellipsis">
      背景
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#941" class="md-nav__link">
    <span class="md-ellipsis">
      9.4.1 正则化的定义与分类
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#942-ell_2-weight-decay" class="md-nav__link">
    <span class="md-ellipsis">
      9.4.2 \(\ell_2\) 正则化（Weight Decay）
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#943-ell_2" class="md-nav__link">
    <span class="md-ellipsis">
      9.4.3 \(\ell_2\) 正则化的局限性
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#944-dropout" class="md-nav__link">
    <span class="md-ellipsis">
      9.4.4 Dropout
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#945-dropout" class="md-nav__link">
    <span class="md-ellipsis">
      9.4.5 Dropout 作为随机近似
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#95" class="md-nav__link">
    <span class="md-ellipsis">
      9.5 各因素的交互作用
    </span>
  </a>
  
    <nav class="md-nav" aria-label="9.5 各因素的交互作用">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#batchnorm-ali-rahimi" class="md-nav__link">
    <span class="md-ellipsis">
      BatchNorm 的反思（Ali Rahimi 演讲引用）
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#batchnorm" class="md-nav__link">
    <span class="md-ellipsis">
      BatchNorm 的其他观察
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#98" class="md-nav__link">
    <span class="md-ellipsis">
      9.8 总结与启示
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lec10-convolutional-networks" class="md-nav__link">
    <span class="md-ellipsis">
      Lec10 卷积网络 (Convolutional Networks)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lec10 卷积网络 (Convolutional Networks)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#101-convolutional-operators-in-deep-networks" class="md-nav__link">
    <span class="md-ellipsis">
      10.1 深度网络中的卷积算子 (Convolutional operators in deep networks)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="10.1 深度网络中的卷积算子 (Convolutional operators in deep networks)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#fully-connected-networks" class="md-nav__link">
    <span class="md-ellipsis">
      全连接网络 (fully connected networks) 的问题
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_37" class="md-nav__link">
    <span class="md-ellipsis">
      卷积如何“简化”深度网络
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_38" class="md-nav__link">
    <span class="md-ellipsis">
      卷积的优势
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_39" class="md-nav__link">
    <span class="md-ellipsis">
      卷积详解
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_40" class="md-nav__link">
    <span class="md-ellipsis">
      图像处理中的卷积
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_41" class="md-nav__link">
    <span class="md-ellipsis">
      深度网络中的卷积
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#-" class="md-nav__link">
    <span class="md-ellipsis">
      多通道卷积的矩阵-向量形式
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#102-elements-of-practical-convolutions" class="md-nav__link">
    <span class="md-ellipsis">
      10.2 实用卷积的要素 (Elements of practical convolutions)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="10.2 实用卷积的要素 (Elements of practical convolutions)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#padding" class="md-nav__link">
    <span class="md-ellipsis">
      Padding
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#strided-convolutions-pooling" class="md-nav__link">
    <span class="md-ellipsis">
      步幅卷积 (Strided Convolutions) / 池化 (Pooling)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#grouped-convolutions" class="md-nav__link">
    <span class="md-ellipsis">
      分组卷积 (Grouped Convolutions)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dilations" class="md-nav__link">
    <span class="md-ellipsis">
      扩张卷积 (Dilations)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#103-differentiating-convolutions" class="md-nav__link">
    <span class="md-ellipsis">
      10.3 卷积的微分 (Differentiating convolutions)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="10.3 卷积的微分 (Differentiating convolutions)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_42" class="md-nav__link">
    <span class="md-ellipsis">
      微分需要什么？
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_43" class="md-nav__link">
    <span class="md-ellipsis">
      回顾矩阵乘法的微分
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1_1" class="md-nav__link">
    <span class="md-ellipsis">
      将卷积视为矩阵乘法：版本 1
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adjoint" class="md-nav__link">
    <span class="md-ellipsis">
      卷积的伴随算子 (Adjoint)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2_2" class="md-nav__link">
    <span class="md-ellipsis">
      将卷积视为矩阵乘法：版本 2
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lec11" class="md-nav__link">
    <span class="md-ellipsis">
      Lec11 硬件加速
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lec11 硬件加速">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#111" class="md-nav__link">
    <span class="md-ellipsis">
      11.1 通用加速技术
    </span>
  </a>
  
    <nav class="md-nav" aria-label="11.1 通用加速技术">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_44" class="md-nav__link">
    <span class="md-ellipsis">
      机器学习框架中的层次
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vectorization" class="md-nav__link">
    <span class="md-ellipsis">
      Vectorization (向量化)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#data-layout-strides" class="md-nav__link">
    <span class="md-ellipsis">
      数据布局 (Data layout) 和步幅 (strides)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#strides" class="md-nav__link">
    <span class="md-ellipsis">
      关于 Strides 的讨论
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#parallelization" class="md-nav__link">
    <span class="md-ellipsis">
      Parallelization (并行化)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#112" class="md-nav__link">
    <span class="md-ellipsis">
      11.2 案例学习：矩阵乘法
    </span>
  </a>
  
    <nav class="md-nav" aria-label="11.2 案例学习：矩阵乘法">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vanilla-matrix-multiplication" class="md-nav__link">
    <span class="md-ellipsis">
      朴素矩阵乘法 (Vanilla matrix multiplication)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cpu-memory-hierarchy" class="md-nav__link">
    <span class="md-ellipsis">
      现代 CPU 的内存层次结构 (Memory hierarchy)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#architecture-aware-analysis" class="md-nav__link">
    <span class="md-ellipsis">
      体系结构感知分析 (Architecture aware analysis)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#register-tiled-matrix-multiplication" class="md-nav__link">
    <span class="md-ellipsis">
      寄存器分块矩阵乘法 (Register tiled matrix multiplication)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cache-line-aware-tiling" class="md-nav__link">
    <span class="md-ellipsis">
      缓存行感知分块 (Cache line aware tiling)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_45" class="md-nav__link">
    <span class="md-ellipsis">
      整合两种分块方法
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#memory-load-reuse" class="md-nav__link">
    <span class="md-ellipsis">
      核心思想：内存加载复用 (Memory load reuse)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_46" class="md-nav__link">
    <span class="md-ellipsis">
      常见的复用模式
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_47" class="md-nav__link">
    <span class="md-ellipsis">
      讨论：卷积中可能的复用模式
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lec12-gpu" class="md-nav__link">
    <span class="md-ellipsis">
      Lec12 GPU 加速
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lec12 GPU 加速">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#121-gpu" class="md-nav__link">
    <span class="md-ellipsis">
      12.1 GPU 编程
    </span>
  </a>
  
    <nav class="md-nav" aria-label="12.1 GPU 编程">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#gpu" class="md-nav__link">
    <span class="md-ellipsis">
      什么是 GPU
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpu-simt" class="md-nav__link">
    <span class="md-ellipsis">
      GPU 编程模型: SIMT
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_48" class="md-nav__link">
    <span class="md-ellipsis">
      示例: 向量加法
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpu_1" class="md-nav__link">
    <span class="md-ellipsis">
      其他 GPU 编程模型示例
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpu_2" class="md-nav__link">
    <span class="md-ellipsis">
      GPU 内存层级
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#window-sum" class="md-nav__link">
    <span class="md-ellipsis">
      示例: 窗口求和 (Window Sum)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_49" class="md-nav__link">
    <span class="md-ellipsis">
      核心要点
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#122-gpu" class="md-nav__link">
    <span class="md-ellipsis">
      12.2 案例学习: GPU 上的矩阵乘法
    </span>
  </a>
  
    <nav class="md-nav" aria-label="12.2 案例学习: GPU 上的矩阵乘法">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#register-tiling" class="md-nav__link">
    <span class="md-ellipsis">
      线程级别优化: Register Tiling
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#shared-memory-tiling" class="md-nav__link">
    <span class="md-ellipsis">
      块级别优化: Shared Memory Tiling
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpu_3" class="md-nav__link">
    <span class="md-ellipsis">
      更多 GPU 优化技巧
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="10-414%20Homework%E7%AC%94%E8%AE%B0.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Homework Notes
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_2" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../CS224w%E5%9B%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.html" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    CS224W图机器学习
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_4_2" id="__nav_4_2_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_2">
            <span class="md-nav__icon md-icon"></span>
            CS224W图机器学习
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../CS224w%E5%9B%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/CS224w.pdf" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    CS224w Notes
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../CS224w%E5%9B%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/CS224W%20%E5%9B%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    CS224w 图机器学习笔记
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    工具分享
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            工具分享
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tools/Windows%E8%A3%85%E6%9C%BA%E7%AC%94%E8%AE%B0%28%E9%99%84%E8%BD%AF%E4%BB%B6%E6%8E%A8%E8%8D%90%29.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Windows装机笔记(附软件推荐)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tools/Linux%E8%A3%85%E6%9C%BA%E7%AC%94%E8%AE%B0.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Linux装机笔记
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../blog/index.html" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    Blog
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_6" id="__nav_6_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Blog
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    
    
      
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6_2" >
        
          
          <label class="md-nav__link" for="__nav_6_2" id="__nav_6_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    归档
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_6_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6_2">
            <span class="md-nav__icon md-icon"></span>
            归档
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../blog/archive/2024.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2024
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6_3" >
        
          
          <label class="md-nav__link" for="__nav_6_3" id="__nav_6_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    分类
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_6_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6_3">
            <span class="md-nav__icon md-icon"></span>
            分类
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../blog/category/%E6%9D%82%E9%A1%B9.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    杂项
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../blog/category/%E8%B8%A9%E5%9D%91%E8%AE%B0%E5%BD%95.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    踩坑记录
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#lec2-softmax" class="md-nav__link">
    <span class="md-ellipsis">
      Lec2 机器学习回顾/Softmax回归
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lec2 机器学习回顾/Softmax回归">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21" class="md-nav__link">
    <span class="md-ellipsis">
      2.1 机器学习基础
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.1 机器学习基础">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    <span class="md-ellipsis">
      数据驱动编程范式
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    <span class="md-ellipsis">
      机器学习三要素
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22-softmax" class="md-nav__link">
    <span class="md-ellipsis">
      2.2 Softmax回归示例
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.2 Softmax回归示例">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    <span class="md-ellipsis">
      多类分类问题设定
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    <span class="md-ellipsis">
      线性假设函数
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_5" class="md-nav__link">
    <span class="md-ellipsis">
      损失函数设计
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#23" class="md-nav__link">
    <span class="md-ellipsis">
      2.3 优化方法
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.3 优化方法">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_6" class="md-nav__link">
    <span class="md-ellipsis">
      梯度下降算法
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sgd" class="md-nav__link">
    <span class="md-ellipsis">
      随机梯度下降 (SGD)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#softmax" class="md-nav__link">
    <span class="md-ellipsis">
      softmax的梯度
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lec3" class="md-nav__link">
    <span class="md-ellipsis">
      Lec3 手动构建神经网络
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lec3 手动构建神经网络">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31" class="md-nav__link">
    <span class="md-ellipsis">
      3.1 从线性到非线性假设类
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3.1 从线性到非线性假设类">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_7" class="md-nav__link">
    <span class="md-ellipsis">
      线性分类器的局限性
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_8" class="md-nav__link">
    <span class="md-ellipsis">
      非线性特征
    </span>
  </a>
  
    <nav class="md-nav" aria-label="非线性特征">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_9" class="md-nav__link">
    <span class="md-ellipsis">
      核心思想
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_10" class="md-nav__link">
    <span class="md-ellipsis">
      特征构造方法对比
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_11" class="md-nav__link">
    <span class="md-ellipsis">
      关键非线性变换示例
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32" class="md-nav__link">
    <span class="md-ellipsis">
      3.2 神经网络
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3.2 神经网络">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_12" class="md-nav__link">
    <span class="md-ellipsis">
      基本定义
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_13" class="md-nav__link">
    <span class="md-ellipsis">
      神经网络结构
    </span>
  </a>
  
    <nav class="md-nav" aria-label="神经网络结构">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_14" class="md-nav__link">
    <span class="md-ellipsis">
      两层神经网络
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mlp" class="md-nav__link">
    <span class="md-ellipsis">
      多层感知机（MLP）
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_15" class="md-nav__link">
    <span class="md-ellipsis">
      深度网络优势分析
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#33" class="md-nav__link">
    <span class="md-ellipsis">
      3.3 反向传播
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3.3 反向传播">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_16" class="md-nav__link">
    <span class="md-ellipsis">
      梯度计算框架
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_17" class="md-nav__link">
    <span class="md-ellipsis">
      两层网络梯度推导
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_18" class="md-nav__link">
    <span class="md-ellipsis">
      通用反向传播算法
    </span>
  </a>
  
    <nav class="md-nav" aria-label="通用反向传播算法">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_19" class="md-nav__link">
    <span class="md-ellipsis">
      推导过程
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_20" class="md-nav__link">
    <span class="md-ellipsis">
      前向传播
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_21" class="md-nav__link">
    <span class="md-ellipsis">
      反向传播
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_22" class="md-nav__link">
    <span class="md-ellipsis">
      计算图视角
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_23" class="md-nav__link">
    <span class="md-ellipsis">
      关键公式总结
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lec4" class="md-nav__link">
    <span class="md-ellipsis">
      Lec4 自动微分
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lec4 自动微分">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#40" class="md-nav__link">
    <span class="md-ellipsis">
      4.0 微分在机器学习中的应用
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#41" class="md-nav__link">
    <span class="md-ellipsis">
      4.1 手动微分
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4.1 手动微分">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_24" class="md-nav__link">
    <span class="md-ellipsis">
      数值微分
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_25" class="md-nav__link">
    <span class="md-ellipsis">
      符号微分
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#42" class="md-nav__link">
    <span class="md-ellipsis">
      4.2 自动微分
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4.2 自动微分">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_26" class="md-nav__link">
    <span class="md-ellipsis">
      计算图
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#forward-ad" class="md-nav__link">
    <span class="md-ellipsis">
      前向模式自动微分 (Forward AD)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="前向模式自动微分 (Forward AD)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ad" class="md-nav__link">
    <span class="md-ellipsis">
      前向模式 AD 的局限性
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reverse-ad" class="md-nav__link">
    <span class="md-ellipsis">
      反向模式自动微分 (Reverse AD)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="反向模式自动微分 (Reverse AD)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_27" class="md-nav__link">
    <span class="md-ellipsis">
      多路径情况的推导
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ad_1" class="md-nav__link">
    <span class="md-ellipsis">
      反向 AD 算法
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ad_2" class="md-nav__link">
    <span class="md-ellipsis">
      通过扩展计算图实现反向模式 AD
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ad_3" class="md-nav__link">
    <span class="md-ellipsis">
      反向模式 AD 与反向传播的比较
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ad_4" class="md-nav__link">
    <span class="md-ellipsis">
      张量上的反向模式 AD
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_28" class="md-nav__link">
    <span class="md-ellipsis">
      梯度的梯度
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ad_5" class="md-nav__link">
    <span class="md-ellipsis">
      数据结构上的反向模式 AD
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lec6" class="md-nav__link">
    <span class="md-ellipsis">
      Lec6 全连接网络、优化与初始化
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lec6 全连接网络、优化与初始化">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#61-fully-connected-networks" class="md-nav__link">
    <span class="md-ellipsis">
      6.1 全连接网络（Fully Connected Networks）
    </span>
  </a>
  
    <nav class="md-nav" aria-label="6.1 全连接网络（Fully Connected Networks）">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mlp_1" class="md-nav__link">
    <span class="md-ellipsis">
      多层感知机（MLP）的数学定义
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#broadcasting" class="md-nav__link">
    <span class="md-ellipsis">
      矩阵形式与广播（Broadcasting）
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_29" class="md-nav__link">
    <span class="md-ellipsis">
      训练前必须回答的关键问题
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#62-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      6.2 优化算法（Optimization）
    </span>
  </a>
  
    <nav class="md-nav" aria-label="6.2 优化算法（Optimization）">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#gradient-descent" class="md-nav__link">
    <span class="md-ellipsis">
      梯度下降（Gradient Descent）
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#newtons-method" class="md-nav__link">
    <span class="md-ellipsis">
      牛顿法（Newton’s Method）
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#momentum" class="md-nav__link">
    <span class="md-ellipsis">
      动量法（Momentum）
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nesterov" class="md-nav__link">
    <span class="md-ellipsis">
      Nesterov 动量
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adam" class="md-nav__link">
    <span class="md-ellipsis">
      Adam 优化器
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sgd_1" class="md-nav__link">
    <span class="md-ellipsis">
      随机梯度下降（SGD）
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#63-initialization" class="md-nav__link">
    <span class="md-ellipsis">
      6.3 权重初始化（Initialization）
    </span>
  </a>
  
    <nav class="md-nav" aria-label="6.3 权重初始化（Initialization）">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#0" class="md-nav__link">
    <span class="md-ellipsis">
      为什么不能初始化为 0？
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_30" class="md-nav__link">
    <span class="md-ellipsis">
      随机初始化的影响
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_31" class="md-nav__link">
    <span class="md-ellipsis">
      权重移动并不大
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#kaiming" class="md-nav__link">
    <span class="md-ellipsis">
      方差推导与 Kaiming 初始化
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#kaiming_1" class="md-nav__link">
    <span class="md-ellipsis">
      Kaiming 初始化的推导
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Kaiming 初始化的推导">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1" class="md-nav__link">
    <span class="md-ellipsis">
      1. 基本设定
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2" class="md-nav__link">
    <span class="md-ellipsis">
      2. 方差推导
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-relu" class="md-nav__link">
    <span class="md-ellipsis">
      3. ReLU 对输入分布的影响
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4" class="md-nav__link">
    <span class="md-ellipsis">
      4. 保持方差一致的条件
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#64" class="md-nav__link">
    <span class="md-ellipsis">
      6.4 小结与后续
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lec7" class="md-nav__link">
    <span class="md-ellipsis">
      Lec7 神经网络库抽象
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lec7 神经网络库抽象">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#71-programming-abstractions" class="md-nav__link">
    <span class="md-ellipsis">
      7.1 编程抽象（Programming Abstractions）
    </span>
  </a>
  
    <nav class="md-nav" aria-label="7.1 编程抽象（Programming Abstractions）">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#case-studies" class="md-nav__link">
    <span class="md-ellipsis">
      案例研究（Case Studies）
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_32" class="md-nav__link">
    <span class="md-ellipsis">
      三种核心抽象模式
    </span>
  </a>
  
    <nav class="md-nav" aria-label="三种核心抽象模式">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-forward-and-backward-layer-interface" class="md-nav__link">
    <span class="md-ellipsis">
      1. 前向与反向层接口（Forward and backward layer interface）
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-computational-graph-and-declarative-programming" class="md-nav__link">
    <span class="md-ellipsis">
      2. 计算图与声明式编程（Computational graph and declarative programming）
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-imperative-automatic-differentiation" class="md-nav__link">
    <span class="md-ellipsis">
      3. 命令式自动微分（Imperative automatic differentiation）
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#discussions" class="md-nav__link">
    <span class="md-ellipsis">
      讨论（Discussions）
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#72-high-level-modular-library-components" class="md-nav__link">
    <span class="md-ellipsis">
      7.2 高级模块化库组件（High Level Modular Library Components）
    </span>
  </a>
  
    <nav class="md-nav" aria-label="7.2 高级模块化库组件（High Level Modular Library Components）">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#elements-of-machine-learning" class="md-nav__link">
    <span class="md-ellipsis">
      机器学习核心元素（Elements of Machine Learning）
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_33" class="md-nav__link">
    <span class="md-ellipsis">
      深度学习的模块化本质
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_34" class="md-nav__link">
    <span class="md-ellipsis">
      模块化组件设计
    </span>
  </a>
  
    <nav class="md-nav" aria-label="模块化组件设计">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-nnmodule" class="md-nav__link">
    <span class="md-ellipsis">
      1. nn.Module：组合模块
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2_1" class="md-nav__link">
    <span class="md-ellipsis">
      2. 损失函数：特殊模块
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-optimizer" class="md-nav__link">
    <span class="md-ellipsis">
      3. 优化器（Optimizer）
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4_1" class="md-nav__link">
    <span class="md-ellipsis">
      4. 正则化与优化器
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5-initialization" class="md-nav__link">
    <span class="md-ellipsis">
      5. 参数初始化（Initialization）
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#6-data-loader-and-preprocessing" class="md-nav__link">
    <span class="md-ellipsis">
      6. 数据加载与预处理（Data Loader and Preprocessing）
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#discussions_1" class="md-nav__link">
    <span class="md-ellipsis">
      讨论（Discussions）
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#revisit-programming-abstraction" class="md-nav__link">
    <span class="md-ellipsis">
      Revisit Programming Abstraction
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lec9" class="md-nav__link">
    <span class="md-ellipsis">
      Lec9 深度学习系统：归一化与正则化
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lec9 深度学习系统：归一化与正则化">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#91" class="md-nav__link">
    <span class="md-ellipsis">
      9.1 引言
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#92" class="md-nav__link">
    <span class="md-ellipsis">
      9.2 初始化与优化的交互
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#93-normalization" class="md-nav__link">
    <span class="md-ellipsis">
      9.3 归一化（Normalization）
    </span>
  </a>
  
    <nav class="md-nav" aria-label="9.3 归一化（Normalization）">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_35" class="md-nav__link">
    <span class="md-ellipsis">
      动机
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#931-layer-normalization" class="md-nav__link">
    <span class="md-ellipsis">
      9.3.1 层归一化（Layer Normalization）
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#932" class="md-nav__link">
    <span class="md-ellipsis">
      9.3.2 层归一化效果
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#933-batch-normalization" class="md-nav__link">
    <span class="md-ellipsis">
      9.3.3 批归一化（Batch Normalization）
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#934-minibatch-dependence" class="md-nav__link">
    <span class="md-ellipsis">
      9.3.4 小批量依赖性（Minibatch Dependence）
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#94-regularization" class="md-nav__link">
    <span class="md-ellipsis">
      9.4 正则化（Regularization）
    </span>
  </a>
  
    <nav class="md-nav" aria-label="9.4 正则化（Regularization）">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_36" class="md-nav__link">
    <span class="md-ellipsis">
      背景
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#941" class="md-nav__link">
    <span class="md-ellipsis">
      9.4.1 正则化的定义与分类
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#942-ell_2-weight-decay" class="md-nav__link">
    <span class="md-ellipsis">
      9.4.2 \(\ell_2\) 正则化（Weight Decay）
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#943-ell_2" class="md-nav__link">
    <span class="md-ellipsis">
      9.4.3 \(\ell_2\) 正则化的局限性
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#944-dropout" class="md-nav__link">
    <span class="md-ellipsis">
      9.4.4 Dropout
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#945-dropout" class="md-nav__link">
    <span class="md-ellipsis">
      9.4.5 Dropout 作为随机近似
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#95" class="md-nav__link">
    <span class="md-ellipsis">
      9.5 各因素的交互作用
    </span>
  </a>
  
    <nav class="md-nav" aria-label="9.5 各因素的交互作用">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#batchnorm-ali-rahimi" class="md-nav__link">
    <span class="md-ellipsis">
      BatchNorm 的反思（Ali Rahimi 演讲引用）
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#batchnorm" class="md-nav__link">
    <span class="md-ellipsis">
      BatchNorm 的其他观察
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#98" class="md-nav__link">
    <span class="md-ellipsis">
      9.8 总结与启示
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lec10-convolutional-networks" class="md-nav__link">
    <span class="md-ellipsis">
      Lec10 卷积网络 (Convolutional Networks)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lec10 卷积网络 (Convolutional Networks)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#101-convolutional-operators-in-deep-networks" class="md-nav__link">
    <span class="md-ellipsis">
      10.1 深度网络中的卷积算子 (Convolutional operators in deep networks)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="10.1 深度网络中的卷积算子 (Convolutional operators in deep networks)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#fully-connected-networks" class="md-nav__link">
    <span class="md-ellipsis">
      全连接网络 (fully connected networks) 的问题
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_37" class="md-nav__link">
    <span class="md-ellipsis">
      卷积如何“简化”深度网络
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_38" class="md-nav__link">
    <span class="md-ellipsis">
      卷积的优势
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_39" class="md-nav__link">
    <span class="md-ellipsis">
      卷积详解
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_40" class="md-nav__link">
    <span class="md-ellipsis">
      图像处理中的卷积
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_41" class="md-nav__link">
    <span class="md-ellipsis">
      深度网络中的卷积
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#-" class="md-nav__link">
    <span class="md-ellipsis">
      多通道卷积的矩阵-向量形式
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#102-elements-of-practical-convolutions" class="md-nav__link">
    <span class="md-ellipsis">
      10.2 实用卷积的要素 (Elements of practical convolutions)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="10.2 实用卷积的要素 (Elements of practical convolutions)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#padding" class="md-nav__link">
    <span class="md-ellipsis">
      Padding
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#strided-convolutions-pooling" class="md-nav__link">
    <span class="md-ellipsis">
      步幅卷积 (Strided Convolutions) / 池化 (Pooling)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#grouped-convolutions" class="md-nav__link">
    <span class="md-ellipsis">
      分组卷积 (Grouped Convolutions)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dilations" class="md-nav__link">
    <span class="md-ellipsis">
      扩张卷积 (Dilations)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#103-differentiating-convolutions" class="md-nav__link">
    <span class="md-ellipsis">
      10.3 卷积的微分 (Differentiating convolutions)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="10.3 卷积的微分 (Differentiating convolutions)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_42" class="md-nav__link">
    <span class="md-ellipsis">
      微分需要什么？
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_43" class="md-nav__link">
    <span class="md-ellipsis">
      回顾矩阵乘法的微分
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1_1" class="md-nav__link">
    <span class="md-ellipsis">
      将卷积视为矩阵乘法：版本 1
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adjoint" class="md-nav__link">
    <span class="md-ellipsis">
      卷积的伴随算子 (Adjoint)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2_2" class="md-nav__link">
    <span class="md-ellipsis">
      将卷积视为矩阵乘法：版本 2
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lec11" class="md-nav__link">
    <span class="md-ellipsis">
      Lec11 硬件加速
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lec11 硬件加速">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#111" class="md-nav__link">
    <span class="md-ellipsis">
      11.1 通用加速技术
    </span>
  </a>
  
    <nav class="md-nav" aria-label="11.1 通用加速技术">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_44" class="md-nav__link">
    <span class="md-ellipsis">
      机器学习框架中的层次
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vectorization" class="md-nav__link">
    <span class="md-ellipsis">
      Vectorization (向量化)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#data-layout-strides" class="md-nav__link">
    <span class="md-ellipsis">
      数据布局 (Data layout) 和步幅 (strides)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#strides" class="md-nav__link">
    <span class="md-ellipsis">
      关于 Strides 的讨论
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#parallelization" class="md-nav__link">
    <span class="md-ellipsis">
      Parallelization (并行化)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#112" class="md-nav__link">
    <span class="md-ellipsis">
      11.2 案例学习：矩阵乘法
    </span>
  </a>
  
    <nav class="md-nav" aria-label="11.2 案例学习：矩阵乘法">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vanilla-matrix-multiplication" class="md-nav__link">
    <span class="md-ellipsis">
      朴素矩阵乘法 (Vanilla matrix multiplication)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cpu-memory-hierarchy" class="md-nav__link">
    <span class="md-ellipsis">
      现代 CPU 的内存层次结构 (Memory hierarchy)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#architecture-aware-analysis" class="md-nav__link">
    <span class="md-ellipsis">
      体系结构感知分析 (Architecture aware analysis)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#register-tiled-matrix-multiplication" class="md-nav__link">
    <span class="md-ellipsis">
      寄存器分块矩阵乘法 (Register tiled matrix multiplication)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cache-line-aware-tiling" class="md-nav__link">
    <span class="md-ellipsis">
      缓存行感知分块 (Cache line aware tiling)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_45" class="md-nav__link">
    <span class="md-ellipsis">
      整合两种分块方法
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#memory-load-reuse" class="md-nav__link">
    <span class="md-ellipsis">
      核心思想：内存加载复用 (Memory load reuse)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_46" class="md-nav__link">
    <span class="md-ellipsis">
      常见的复用模式
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_47" class="md-nav__link">
    <span class="md-ellipsis">
      讨论：卷积中可能的复用模式
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lec12-gpu" class="md-nav__link">
    <span class="md-ellipsis">
      Lec12 GPU 加速
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lec12 GPU 加速">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#121-gpu" class="md-nav__link">
    <span class="md-ellipsis">
      12.1 GPU 编程
    </span>
  </a>
  
    <nav class="md-nav" aria-label="12.1 GPU 编程">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#gpu" class="md-nav__link">
    <span class="md-ellipsis">
      什么是 GPU
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpu-simt" class="md-nav__link">
    <span class="md-ellipsis">
      GPU 编程模型: SIMT
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_48" class="md-nav__link">
    <span class="md-ellipsis">
      示例: 向量加法
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpu_1" class="md-nav__link">
    <span class="md-ellipsis">
      其他 GPU 编程模型示例
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpu_2" class="md-nav__link">
    <span class="md-ellipsis">
      GPU 内存层级
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#window-sum" class="md-nav__link">
    <span class="md-ellipsis">
      示例: 窗口求和 (Window Sum)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_49" class="md-nav__link">
    <span class="md-ellipsis">
      核心要点
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#122-gpu" class="md-nav__link">
    <span class="md-ellipsis">
      12.2 案例学习: GPU 上的矩阵乘法
    </span>
  </a>
  
    <nav class="md-nav" aria-label="12.2 案例学习: GPU 上的矩阵乘法">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#register-tiling" class="md-nav__link">
    <span class="md-ellipsis">
      线程级别优化: Register Tiling
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#shared-memory-tiling" class="md-nav__link">
    <span class="md-ellipsis">
      块级别优化: Shared Memory Tiling
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpu_3" class="md-nav__link">
    <span class="md-ellipsis">
      更多 GPU 优化技巧
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
    <a href="https://github.com/yJader/JinBlog/edit/main/docs/csdiy/CMU 10-414 Deep Learning Systems/10-414_深度学习系统课程笔记.md" title="编辑此页" class="md-content__button md-icon" rel="edit">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg>
    </a>
  
  


  <h1>Course Notes</h1>

<p>笔记内容由课程ppt+llm生成, 并进行一定修改</p>
<h2 id="lec2-softmax">Lec2 机器学习回顾/Softmax回归<a class="headerlink" href="#lec2-softmax" title="Permanent link">&para;</a></h2>
<h3 id="21">2.1 机器学习基础<a class="headerlink" href="#21" title="Permanent link">&para;</a></h3>
<h4 id="_1">数据驱动编程范式<a class="headerlink" href="#_1" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>传统编程困境</strong>：手工编写分类逻辑困难（如手写数字识别）</li>
<li><strong>监督学习解决方案</strong>：<ol>
<li>收集带标签的训练集 <span class="arithmatex">\({(x^{(i)}, y^{(i)})}_{i=1}^m\)</span></li>
<li>通过机器学习算法自动生成分类程序</li>
</ol>
</li>
</ul>
<h4 id="_2">机器学习三要素<a class="headerlink" href="#_2" title="Permanent link">&para;</a></h4>
<ol>
<li><strong>假设类 (Hypothesis Class)</strong> : 参数化映射函数 <span class="arithmatex">\(h_\theta: \mathbb{R}^n \rightarrow \mathbb{R}^k\)</span>，描述输入到输出的关系</li>
<li><strong>损失函数 (Loss Function)</strong> : 量化模型预测效果：<span class="arithmatex">\(\ell(h(x), y) \rightarrow \mathbb{R}^+\)</span></li>
<li><strong>​优化方法 (Optimization Method) ​</strong>: 最小化经验风险<span class="arithmatex">\(\min_\theta \frac{1}{m}\sum_{i=1}^m \ell(h_\theta(x^{(i)}), y^{(i)})\)</span></li>
</ol>
<hr />
<h3 id="22-softmax">2.2 Softmax回归示例<a class="headerlink" href="#22-softmax" title="Permanent link">&para;</a></h3>
<h4 id="_3">多类分类问题设定<a class="headerlink" href="#_3" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>输入空间</strong>: <span class="arithmatex">\(x^{(i)} \in \mathbb{R}^n\)</span>（如MNIST图像 <span class="arithmatex">\(n=784\)</span>）</li>
<li><strong>输出空间</strong>: <span class="arithmatex">\(y^{(i)} \in \{1,...,k\}\)</span> （如MNIST有 <span class="arithmatex">\(k=10\)</span> 类）</li>
<li><strong>训练集规模</strong>: <span class="arithmatex">\(m\)</span> 个样本（如MNIST <span class="arithmatex">\(m=60,000\)</span>）</li>
</ul>
<h4 id="_4">线性假设函数<a class="headerlink" href="#_4" title="Permanent link">&para;</a></h4>
<p><span class="arithmatex">\(h_\theta(x) = \theta^T x\)</span>
其中</p>
<ul>
<li>参数矩阵 <span class="arithmatex">\(\theta \in \mathbb{R}^{n \times k}\)</span>, 输入参数x为n维列向量(n×1)</li>
<li>将输入的n维列向量映射为k维输出(如每个类别的概率)</li>
</ul>
<p><strong>批量计算形式</strong>：</p>
<div class="arithmatex">\[
\mathbf X \in \mathbb{R}^{m \times n} = \begin{bmatrix} -x^{(1)T}- \\ \vdots \\ -x^{(m)T}- \end{bmatrix}, \quad
y\in\{1,\dots,k\}^m=\begin{bmatrix}y^{(1)}\\ \vdots \\ y^{(m)}
\end{bmatrix}
\]</div>
<ul>
<li><span class="arithmatex">\(-x^{(1)T}-\)</span>意为<strong>强调</strong>这个向量占一行
这样, 线性假设可以用批量计算形式写成:</li>
</ul>
<div class="arithmatex">\[
h_\theta(X) = \mathbf X\theta
  =\begin{bmatrix} -x^{(1)T}\theta- \\ \vdots \\ -x^{(m)T}\theta- \end{bmatrix}
  =\begin{bmatrix} -h_{\theta}(x^{(1)})^{T}- \\ \vdots \\ -h_{\theta}(x^{(m)})^T- \end{bmatrix}
\]</div>
<h4 id="_5">损失函数设计<a class="headerlink" href="#_5" title="Permanent link">&para;</a></h4>
<ol>
<li><strong>分类错误率</strong>​（不可导，不适于优化）：
   <span class="arithmatex">\(<span class="arithmatex">\(\ell_{err}(h(x),y) = \begin{cases}
   0 &amp; \text{if } \arg\max_i h_i(x) = y \\
   1 &amp; \text{otherwise}
   \end{cases}\)</span>\)</span></li>
<li><strong>交叉熵损失</strong>​（Softmax损失）：</li>
</ol>
<div class="arithmatex">\[
 \begin{aligned}
    z_i &amp;= p(label=i) = \frac{\exp(h_i(x))}{\sum_{j=1}^k \exp(h_j(x))} \\
    \ell_{ce}(h(x),y) &amp;= -\log z_y = -h_y(x) + \log\sum_{j=1}^k \exp\left(h_j(x)\right)
 \end{aligned}
\]</div>
<ul>
<li>注: 这里的log默认为e为底, 即ln</li>
</ul>
<h3 id="23">2.3 优化方法<a class="headerlink" href="#23" title="Permanent link">&para;</a></h3>
<h4 id="_6">梯度下降算法<a class="headerlink" href="#_6" title="Permanent link">&para;</a></h4>
<p>对于函数<span class="arithmatex">\(f: \mathbb R^{n\times k}\rightarrow \mathbb R\)</span>, 梯度定义为</p>
<div class="arithmatex">\[
\nabla_\theta f(\theta)\in \mathbb R^{n\times k}=
\begin{bmatrix}
\frac{\partial f(\theta)}{\partial \theta_{11}} &amp; \dots &amp; \frac{\partial f(\theta)}{\partial \theta_{1k}} \\
\vdots &amp;  &amp; \vdots \\
\frac{\partial f(\theta)}{\partial \theta_{n1}} &amp; \dots &amp; \frac{\partial f(\theta)}{\partial \theta_{nk}}
\end{bmatrix}
\]</div>
<ul>
<li><img alt="" src="10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/IMG-10-414%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-20250304210853564.png" /></li>
<li>在θ点处的梯度表示在局部上, 哪个方向能使函数增长最快
为了最小化函数(如损失函数), 梯度下降算法使用负梯度方向, 逐步逼近最小值
<span class="arithmatex">\(\theta := \theta -\alpha \nabla_{\theta}f(\theta)\)</span></li>
<li>α为步长/学习率, α的大小对收敛性/收敛速度有很大影响 <img alt="" src="10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/IMG-10-414%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-20250304211947988.png" /></li>
</ul>
<p>参数更新规则：
<span class="arithmatex">\(\theta := \theta - \alpha \nabla_\theta \left( \frac{1}{m}\sum_{i=1}^m \ell_{ce}(\theta^T x^{(i)}, y^{(i)}) \right)\)</span></p>
<h4 id="sgd">随机梯度下降 (SGD)<a class="headerlink" href="#sgd" title="Permanent link">&para;</a></h4>
<p>每次都使用所有数据来计算梯度开销过大, 可以使用采样构成一个minibatch, 将这个minibatch计算的结果作为真实梯度的<strong>近似</strong></p>
<div class="arithmatex">\[
\begin{array}{l}{{\mathrm{Repeat}:~~}}\\ {{\text{Samplea~a~minibatch~of~data~}X\in\mathbb{R}^{B\times n},y\in\{1,\dots,k\}^{B}}}\\ {{\text{更新参数 }\theta:=\theta-{\frac{\alpha}{B}}\sum_{i=1}^{B}\nabla_{\theta}\ell{\big(}h_{\theta}(x^{(i)}),y^{(i)}{\big)}}}\end{array}
\]</div>
<ul>
<li>这个近似操作可以理解为引入一些"噪声", 这在某些情况下甚至能带来一些优势</li>
</ul>
<h4 id="softmax">softmax的梯度<a class="headerlink" href="#softmax" title="Permanent link">&para;</a></h4>
<p>对向量<span class="arithmatex">\(h \in \mathbb R^k\)</span>:</p>
<div class="arithmatex">\[
\begin{aligned}
\frac{\partial\ell_{ce}(h,y)}{\partial h_{i}}&amp;=\frac{\partial}{\partial h_{i}}\left(-h_{y}+\log\sum_{j=1}^{k}\exp h_{j}\right)\\
&amp;=-1\{i=y\}+\frac{\exp h_{i}}{\sum_{j=1}^{k}\exp h_{j}}
\end{aligned}
\]</div>
<ul>
<li>log实际上是ln</li>
<li>用向量形式表示: <span class="arithmatex">\(\nabla_{h}\ell(h, y)=z-e_{y}, z=normalize(exp(h))\)</span></li>
</ul>
<p>回到softmax: <span class="arithmatex">\(\nabla_{\theta}\ell_{ce}(\theta^Tx, y)\)</span> , θ是n×k的<strong>矩阵</strong>, <span class="arithmatex">\(\theta^Tx\)</span>是k维列<strong>向量</strong>, 如何计算矩阵/向量的导数和链式法则?</p>
<ul>
<li>假装一切都是标量(无视转置), 使用链式法则, 然后重新排列 / 转置矩阵or向量, 让计算结果的维度大小正确
<img alt="" src="10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/IMG-10-414%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-20250304222514624.png" />
<span class="arithmatex">\(\nabla_\theta\ell_{ce}(\theta^Tx,y)\in\mathbb{R}^{n\times k}=x(z-e_y)^T\)</span>
用<strong>批量计算</strong>形式<span class="arithmatex">\(\nabla_\theta\ell_{ce}(\mathbf X\theta,y)\in\mathbb{R}^{n\times k}=\mathbf X^T(Z-I_y),\quad Z=\mathrm{normalize}(\exp(\mathbf X\Theta))\)</span>
重复, 直到参数/loss收敛</li>
</ul>
<ol>
<li>Iterative over minibatches <span class="arithmatex">\(X\in\mathbb{R}^{B\times n},y\in\{1,...,k\}^B\)</span> of training set</li>
<li>Update the parameters <span class="arithmatex">\(\theta:=\theta-\frac{\alpha}{B}X^T(Z-I_y)\)</span></li>
</ol>
<h2 id="lec3">Lec3 手动构建神经网络<a class="headerlink" href="#lec3" title="Permanent link">&para;</a></h2>
<h3 id="31">3.1 从线性到非线性假设类<a class="headerlink" href="#31" title="Permanent link">&para;</a></h3>
<h4 id="_7">线性分类器的局限性<a class="headerlink" href="#_7" title="Permanent link">&para;</a></h4>
<ul>
<li>线性假设类基本形式：
  <span class="arithmatex">\(h_{\theta}(x)=\theta^{T} x,\quad\theta\in \mathbb{R}^{n\times k}\)</span><ul>
<li>通过k个线性函数划分输入空间</li>
<li>无法处理非线性分类边界（如螺旋形数据）</li>
</ul>
</li>
</ul>
<figure style="display: flex; justify-content: space-around; align-items: center;">
 <img src="10-414_深度学习系统课程笔记.assets/IMG-10-414%20深度学习系统课程笔记-20250305211219799.png" style="width: 33%; height: auto; margin: 0 10px;">
 <img src="10-414_深度学习系统课程笔记.assets/IMG-10-414%20深度学习系统课程笔记-20250305211634230.png" style="width: 33%; height: auto; margin: 0 10px;">

</figure>

<h4 id="_8">非线性特征<a class="headerlink" href="#_8" title="Permanent link">&para;</a></h4>
<h5 id="_9">核心思想<a class="headerlink" href="#_9" title="Permanent link">&para;</a></h5>
<ul>
<li>通过特征映射提升维度：
  $$
  \begin{array}{c}
  h_{\theta}(x)=\theta^{T}\phi(x) \
  \phi:\mathbb{R}<sup d="d">{n}\rightarrow\mathbb{R}</sup>
  \end{array}
  $$}, \theta\in\mathbb{R}^{d\times k</li>
</ul>
<h5 id="_10">特征构造方法对比<a class="headerlink" href="#_10" title="Permanent link">&para;</a></h5>
<table>
<thead>
<tr>
<th>方法</th>
<th>描述</th>
<th>数学形式</th>
<th>局限性</th>
</tr>
</thead>
<tbody>
<tr>
<td>线性特征</td>
<td>简单线性组合</td>
<td><span class="arithmatex">\(\phi(x)=W^T x\)</span></td>
<td>等价于单层线性分类器</td>
</tr>
<tr>
<td>​<strong>非线性特征</strong>​</td>
<td>激活函数变换</td>
<td><span class="arithmatex">\(\phi(x)=\sigma(W^T x)\)</span></td>
<td>可表达复杂边界</td>
</tr>
</tbody>
</table>
<h5 id="_11">关键非线性变换示例<a class="headerlink" href="#_11" title="Permanent link">&para;</a></h5>
<ul>
<li><strong>随机傅里叶特征</strong>：
  <span class="arithmatex">\(W\sim\mathcal{N}(0,1)\text{(随机高斯采样)},\ \sigma=\cos(\cdot)\)</span></li>
<li><strong>可学习非线性特征</strong>：（需通过反向传播训练<span class="arithmatex">\(W\)</span>）
  <span class="arithmatex">\(\phi(x)=\text{ReLU}(W^T x)\)</span></li>
</ul>
<hr />
<h3 id="32">3.2 神经网络<a class="headerlink" href="#32" title="Permanent link">&para;</a></h3>
<h4 id="_12">基本定义<a class="headerlink" href="#_12" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>神经网络</strong>：由多个可微参数化函数（层）组成的假设类</li>
<li><strong>深度网络</strong>：多层级联的非线性变换（现代网络通常&gt;3层）</li>
</ul>
<h4 id="_13">神经网络结构<a class="headerlink" href="#_13" title="Permanent link">&para;</a></h4>
<h5 id="_14">两层神经网络<a class="headerlink" href="#_14" title="Permanent link">&para;</a></h5>
<blockquote>
<p>也称为单隐藏层网络</p>
</blockquote>
<div style="text-align: center;">
    <img src="10-414%20深度学习系统课程笔记.assets/IMG-10-414%20深度学习系统课程笔记-20250305214646239.png" style="width: 200px;">
</div>

<div class="arithmatex">\[
\begin{aligned}
h_{\theta}(x) &amp;= W_{2}^{T}\sigma\left(W_{1}^{T} x\right) \\
\theta &amp;= \{W_{1}\in \mathbb{R}^{n\times d}, W_{2}\in \mathbb{R}^{d\times k}\}
\end{aligned}
\]</div>
<ul>
<li>批量矩阵形式：<span class="arithmatex">\(h_\theta(\mathbf X)=\sigma\left(\mathbf X \mathbf W_1\right) \mathbf W_2\)</span><ul>
<li><span class="arithmatex">\(\mathbf X\in \mathbb R^{m\times n}, W_{1}\in \mathbb{R}^{n\times d}, W_{2}\in \mathbb{R}^{d\times k}\)</span></li>
</ul>
</li>
</ul>
<h5 id="mlp">多层感知机（MLP）<a class="headerlink" href="#mlp" title="Permanent link">&para;</a></h5>
<div style="text-align: center;">
 <img src="10-414%20深度学习系统课程笔记.assets/IMG-10-414%20深度学习系统课程笔记-20250305221004492.png" width="300">
</div>
<ul>
<li>L层通用结构：</li>
</ul>
<div class="arithmatex">\[
\begin{aligned}
\mathbf Z_{i+1} &amp;= \sigma_{i}\left(\mathbf Z_{i} \mathbf W_{i}\right),\quad i=1,\ldots,L \\
\mathbf Z_{1} &amp;= \mathbf X \\
h_{\theta}(\mathbf X) &amp;= \mathbf Z_{L+1}
\end{aligned}
\]</div>
<ul>
<li>维度关系：<span class="arithmatex">\(\mathbf Z_i\in\mathbb{R}^{m\times n_i},\ \mathbf W_i\in\mathbb{R}^{n_i\times n_{i+1}}\)</span><ul>
<li>Z被称为layer, activation, neuron, 是网络在不同阶段生成的中间特征</li>
</ul>
</li>
<li>典型激活函数：ReLU, Sigmoid, Tanh</li>
</ul>
<h4 id="_15">深度网络优势分析<a class="headerlink" href="#_15" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>观点</th>
<th>支持/反对</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>生物相似性</td>
<td>❌</td>
<td>实际与大脑工作机制差异显著</td>
</tr>
<tr>
<td>电路效率</td>
<td>⚠️</td>
<td>理论优势难以实际验证</td>
</tr>
<tr>
<td>经验有效性</td>
<td>✅</td>
<td>固定参数量下表现更优</td>
</tr>
</tbody>
</table>
<hr />
<h3 id="33">3.3 反向传播<a class="headerlink" href="#33" title="Permanent link">&para;</a></h3>
<h4 id="_16">梯度计算框架<a class="headerlink" href="#_16" title="Permanent link">&para;</a></h4>
<ul>
<li>优化目标：  <span class="arithmatex">\(\min_{\theta} \frac{1}{m}\sum_{i=1}^{m}\ell_{ce}\left(h_{\theta}(x^{(i)}), y^{(i)}\right)\)</span><ul>
<li>核心需求：计算<span class="arithmatex">\(\nabla_\theta\ell_{ce}(h_{\theta}(x^{(i)}), y^{(i)})\)</span></li>
</ul>
</li>
</ul>
<h4 id="_17">两层网络梯度推导<a class="headerlink" href="#_17" title="Permanent link">&para;</a></h4>
<blockquote>
<p>批量矩阵形式：<span class="arithmatex">\(h_\theta(\mathbf X)=\sigma\left(\mathbf X \mathbf W_1\right) \mathbf W_2\)</span></p>
</blockquote>
<ul>
<li><span class="arithmatex">\(\mathbf X\in \mathbb R^{m\times n}, W_{1}\in \mathbb{R}^{n\times d}, W_{2}\in \mathbb{R}^{d\times k}\)</span></li>
</ul>
<p>对于简单的两层网络, 梯度用批量矩阵形式写为: <span class="arithmatex">\(\nabla_{W_{1}, W_{2}}\ell_{ce}(h_{\theta}(XW_1)W_{2}, y^{(i)})\)</span></p>
<div class="arithmatex">\[
\begin{aligned}
  \frac{\partial\ell_{ce}(\sigma(XW_1)W_2,y)}{\partial W_2} &amp; =\frac{\partial\ell_{ce}(\sigma(XW_1)W_2,y)}{\partial\sigma(XW_1)W_2}\cdot\frac{\partial\sigma(XW_1)W_2}{\partial W_2} \\
 &amp;  =(S-I_y)\cdot\sigma(XW_1),\quad[S=\mathrm{~softmax}(\sigma(XW_1)W_2)] \\ \\
 \frac{\partial\ell_{ce}(\sigma(XW_1)W_2,y)}{\partial W_1} &amp;
 =\frac{\partial\ell_{ce}(\sigma(XW_1)W_2,y)}{\partial\sigma(XW_1)W_2} \cdot \frac{\partial\sigma(XW_1)W_2}{\partial \sigma(XW_{1})} \cdot \frac{\partial \sigma(XW_{1})}{\partial XW_{1}} \cdot \frac{{\partial XW_{1}}}{\partial W_{1}}\\
 &amp; =
 \underbrace{(S-I_{y})}_{m \times k}\cdot \underbrace{W_{2}}_{d\times k}\cdot \underbrace{\sigma'(XW_{1})}_{m\times d} \cdot \underbrace{X}_{m\times n}
\end{aligned}
\]</div>
<p>对<span class="arithmatex">\(W_2\)</span>的梯度</p>
<div class="arithmatex">\[
\nabla_{W_2}\ell = \sigma(XW_1)^T(S-I_y)
\]</div>
<ul>
<li><span class="arithmatex">\(S=\text{softmax}(\sigma(XW_1)W_2)\)</span></li>
<li><span class="arithmatex">\(I_y\)</span>为one-hot标签矩阵</li>
</ul>
<p>对<span class="arithmatex">\(W_1\)</span>的梯度</p>
<div class="arithmatex">\[
\nabla_{W_1}\ell = \underbrace{X^T}_{n\times m}\left[ \underbrace{(S-I_y)W_2^T}_{m\times d} \circ \underbrace{\sigma'(XW_1)}_{m\times d} \right] \in \mathbb{R}^{n\times d}
\]</div>
<ul>
<li><span class="arithmatex">\(\circ\)</span>表示逐元素乘法(标量乘法)</li>
<li><span class="arithmatex">\(\sigma'\)</span>为激活函数导数</li>
</ul>
<h4 id="_18">通用反向传播算法<a class="headerlink" href="#_18" title="Permanent link">&para;</a></h4>
<blockquote>
<p>尽管用了简化, 但是计算梯度还是有些麻烦</p>
</blockquote>
<h5 id="_19">推导过程<a class="headerlink" href="#_19" title="Permanent link">&para;</a></h5>
<p>前向传播递推式: <span class="arithmatex">\(Z_{i+1} = \sigma_i(Z_iW_i) \in \mathbb R^{m\times n_{i+1}}\quad (i=1,...,L)\)</span></p>
<ul>
<li>注意: 前面个的两层网络的最后一层没有激活函数, 和此处不相同</li>
</ul>
<p>前向传播最终输出<span class="arithmatex">\(Z_{L+1}\)</span>损失函数对于<span class="arithmatex">\(W_{i}\)</span>的梯度, 由链式法则可得:
<img alt="" src="10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/IMG-10-414%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-20250306153440999.png" /></p>
<ul>
<li><span class="arithmatex">\(G_{i}\)</span>为损失对中间输出<span class="arithmatex">\(Z_{i}\)</span>的导数
可得</li>
</ul>
<div class="arithmatex">\[
\begin{aligned}
G_i
  &amp;=G_{i+1}\cdot\frac{\partial Z_{i+1}}{\partial Z_i}=G_{i+1}\cdot\frac{\partial\sigma_i(Z_iW_i)}{\partial Z_iW_i}\cdot\frac{\partial Z_iW_i}{\partial Z_i}\\
  &amp;= \underbrace{G_{i+1}}_{m\times n_{i+1}}\cdot\underbrace{\sigma^{\prime}(Z_iW_i)}_{m\times n_{i+1}}\cdot \underbrace{W_i}_{n_{i}\times n_{i+1}}\\
G_{i}&amp;=\frac{{\partial \ell(Z_{L+1}, y)}}{\partial Z_{i}}= \nabla_{Z_{i}}\ell(Z_{L+1}, y)\in \mathbb R^{m\times n_{i}}\\
\text{整理得:}\\
  G_i &amp;= (G_{i+1} \circ \sigma_i'(Z_iW_i)) W_i^T \\
\end{aligned}
\]</div>
<p>回到参数<span class="arithmatex">\(W_{i}\)</span>的梯度更新:</p>
<div class="arithmatex">\[
\begin{aligned}
\frac{\partial\ell(Z_{L+1},y)}{\partial W_i}&amp;=G_{i+1}\cdot \frac{\partial Z_{i+1}}{\partial W_{i}}=G_{i+1}\cdot\frac{\partial\sigma_i(Z_iW_i)}{\partial Z_iW_i}\cdot\frac{\partial Z_iW_i}{\partial W_i}\\
&amp;=\underbrace{G_{i+1}}_{m\times n_{i+1}}\cdot\underbrace{\sigma^{\prime}(Z_iW_i)}_{m\times n_{i+1}}\cdot \underbrace {Z_i}_{m\times n_{i}}\\
\Longrightarrow &amp;\nabla_{W_i}\ell(Z_{L+1},y)=Z_i^T\left(G_{i+1}\circ\sigma^{\prime}(Z_iW_i)\right) \in \mathbb R^{n_i\times n_{i+1}}
\end{aligned}
\]</div>
<h5 id="_20">前向传播<a class="headerlink" href="#_20" title="Permanent link">&para;</a></h5>
<div class="arithmatex">\[
\begin{aligned}
Z_1 &amp;= X \in \mathbb R^{m\times n} \\
Z_{i+1} &amp;= \sigma_i(Z_iW_i) \in \mathbb R^{m\times n_{i+1}}\quad (i=1,...,L)
\end{aligned}
\]</div>
<h5 id="_21">反向传播<a class="headerlink" href="#_21" title="Permanent link">&para;</a></h5>
<ol>
<li>初始化梯度：   <span class="arithmatex">\(G_{L+1} = \nabla_{Z_{L+1}}\ell = S-I_y\)</span></li>
<li>反向迭代：<span class="arithmatex">\(G_i = (G_{i+1} \circ \sigma_i'(Z_iW_i)) W_i^T\)</span></li>
<li>参数梯度计算： <span class="arithmatex">\(\nabla_{W_i}\ell = Z_i^T(G_{i+1} \circ \sigma_i'(Z_iW_i))\)</span></li>
</ol>
<h4 id="_22">计算图视角<a class="headerlink" href="#_22" title="Permanent link">&para;</a></h4>
<p><img alt="" src="10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/IMG-10-414%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-20250306171653489.png" /></p>
<ul>
<li><strong>向量-雅可比积</strong>vector Jacobian product：核心计算单元
  <span class="arithmatex">\(\frac{\partial Z_{i+1}}{\partial W_i} = \sigma_i'(Z_iW_i) \cdot Z_i\)</span></li>
<li>内存优化：缓存前向传播结果<span class="arithmatex">\(Z_i, W_i\)</span></li>
</ul>
<h3 id="_23">关键公式总结<a class="headerlink" href="#_23" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>公式类型</th>
<th>表达式</th>
</tr>
</thead>
<tbody>
<tr>
<td>前向传播</td>
<td><span class="arithmatex">\(Z_{i+1} = \sigma_i(Z_iW_i)\)</span></td>
</tr>
<tr>
<td>损失梯度</td>
<td><span class="arithmatex">\(G_{L+1} = S - I_y\)</span></td>
</tr>
<tr>
<td>反向梯度</td>
<td><span class="arithmatex">\(G_i = (G_{i+1} \circ \sigma_i') W_i^T\)</span></td>
</tr>
<tr>
<td>权重梯度</td>
<td><span class="arithmatex">\(\nabla_{W_i} = Z_i^T(G_{i+1} \circ \sigma_i')\)</span></td>
</tr>
</tbody>
</table>
<h2 id="lec4">Lec4 自动微分<a class="headerlink" href="#lec4" title="Permanent link">&para;</a></h2>
<p>本讲内容分为两个主要部分：</p>
<ol>
<li>不同微分方法的总体介绍</li>
<li>反向模式自动微分</li>
</ol>
<h3 id="40">4.0 微分在机器学习中的应用<a class="headerlink" href="#40" title="Permanent link">&para;</a></h3>
<p>每个机器学习算法都由三个不同的元素组成：</p>
<ol>
<li><strong>假设类</strong>：<span class="arithmatex">\(h_\theta(x)\)</span></li>
<li><strong>损失函数</strong>：<span class="arithmatex">\(l(h_\theta(x), y) = -h_y(x) + \log \sum_{j=1}^{k} \exp h_j(x)\)</span></li>
<li><strong>优化方法</strong>：<span class="arithmatex">\(\theta := \theta - \frac{\alpha}{B} \sum_{i=1}^{B} \nabla_\theta \ell(h_\theta(x_i), y_i)\)</span></li>
</ol>
<p>计算损失函数对假设类参数的梯度是机器学习中最常见的操作。</p>
<h3 id="41">4.1 手动微分<a class="headerlink" href="#41" title="Permanent link">&para;</a></h3>
<h4 id="_24">数值微分<a class="headerlink" href="#_24" title="Permanent link">&para;</a></h4>
<p><strong>基于定义的直接计算</strong>:
<span class="arithmatex">\(\frac{\partial f(\theta)}{\partial \theta_i} = \lim_{\epsilon \to 0} \frac{f(\theta + \epsilon e_i) - f(\theta)}{\epsilon}\)</span>
<strong>更准确的数值近似方法</strong>:</p>
<p><span class="arithmatex">\(\frac{\partial f(\theta)}{\partial \theta_i} = \frac{f(\theta + \epsilon e_i) - f(\theta - \epsilon e_i)}{2\epsilon} + o(\epsilon^2)\)</span></p>
<ul>
<li>数值微分存在数值误差，计算效率较低(要计算两次f())。</li>
<li>但它是检验自动微分算法实现的有力工具，特别是在单元测试中。
<strong>数值梯度检查</strong>:
从单位球中选取 <span class="arithmatex">\(\delta\)</span>，检验以下不变性：</li>
</ul>
<p><span class="arithmatex">\(\delta^T \nabla_\theta f(\theta) = \frac{f(\theta + \epsilon\delta) - f(\theta - \epsilon\delta)}{2\epsilon} + o(\epsilon^2)\)</span></p>
<h4 id="_25">符号微分<a class="headerlink" href="#_25" title="Permanent link">&para;</a></h4>
<p>通过写下公式，利用 和规则、积规则和链式法则推导梯度。</p>
<ul>
<li>和规则：<span class="arithmatex">\(\frac{\partial(f(\theta) + g(\theta))}{\partial \theta} = \frac{\partial f(\theta)}{\partial \theta} + \frac{\partial g(\theta)}{\partial \theta}\)</span></li>
<li>积规则：<span class="arithmatex">\(\frac{\partial(f(\theta)g(\theta))}{\partial \theta} = g(\theta)\frac{\partial f(\theta)}{\partial \theta} + f(\theta)\frac{\partial g(\theta)}{\partial \theta}\)</span></li>
<li>链式法则：<span class="arithmatex">\(\frac{\partial f(g(\theta))}{\partial \theta} = \frac{\partial f(g(\theta))}{\partial g(\theta)}\frac{\partial g(\theta)}{\partial \theta}\)</span>
但简单地这样做可能导致计算浪费。对于函数 <span class="arithmatex">\(f(\theta) = \prod_{i=1}^{n} \theta_i\)</span>，计算所有偏导数需要 <span class="arithmatex">\(n(n-2)\)</span> 次乘法操作，效率较低。</li>
</ul>
<h3 id="42">4.2 自动微分<a class="headerlink" href="#42" title="Permanent link">&para;</a></h3>
<h4 id="_26">计算图<a class="headerlink" href="#_26" title="Permanent link">&para;</a></h4>
<blockquote>
<p><img alt="" src="10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/IMG-10-414%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-20250309225625857.png" />
以函数 <span class="arithmatex">\(y = f(x_1, x_2) = \ln(x_1) + x_1 x_2 - \sin(x_2)\)</span> 为例：</p>
</blockquote>
<p><strong>前向计算轨迹</strong></p>
<ul>
<li><span class="arithmatex">\(v_1 = x_1 = 2\)</span></li>
<li><span class="arithmatex">\(v_2 = x_2 = 5\)</span></li>
<li><span class="arithmatex">\(v_3 = \ln(v_1) = \ln(2) = 0.693\)</span></li>
<li><span class="arithmatex">\(v_4 = v_1 \times v_2 = 10\)</span></li>
<li><span class="arithmatex">\(v_5 = \sin(v_2) = \sin(5) = -0.959\)</span></li>
<li><span class="arithmatex">\(v_6 = v_3 + v_4 = 10.693\)</span></li>
<li><span class="arithmatex">\(v_7 = v_6 - v_5 = 10.693 + 0.959 = 11.652\)</span></li>
<li><span class="arithmatex">\(y = v_7 = 11.652\)</span></li>
</ul>
<h4 id="forward-ad">前向模式自动微分 (Forward AD)<a class="headerlink" href="#forward-ad" title="Permanent link">&para;</a></h4>
<p><img alt="" src="10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/IMG-10-414%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-20250309225625857.png" />
定义 <span class="arithmatex">\(\dot{v}_i = \frac{\partial v_i}{\partial x_1}\)</span>，然后按照计算图的前向拓扑顺序迭代计算 <span class="arithmatex">\(\dot{v}_i\)</span>：</p>
<p>前向 AD 轨迹:</p>
<ul>
<li><span class="arithmatex">\(\dot{v}_1 =\frac{\partial v_1}{\partial x_1}=\frac{\partial x_1}{\partial x_1}= 1 \quad (v_{1}=x_{1})\)</span></li>
<li><span class="arithmatex">\(\dot{v}_2 = 0\)</span></li>
<li><span class="arithmatex">\(\dot{v}_3 =\frac{\partial v_{3}}{\partial x_{1}}=\frac{\partial \ln(v_{1})}{\partial x_{1}}=\frac{1}{v_{1}}\frac{\partial v_1}{\partial x_1}= \frac{\dot{v}_1}{v_1} = 0.5\)</span></li>
<li><span class="arithmatex">\(\dot{v}_4 = \dot{v}_1 v_2 + \dot{v}_2 v_1 = 1 \times 5 + 0 \times 2 = 5\)</span></li>
<li><span class="arithmatex">\(\dot{v}_5 = \dot{v}_2 \cos(v_2) = 0 \times \cos(5) = 0\)</span></li>
<li><span class="arithmatex">\(\dot{v}_6 = \dot{v}_3 + \dot{v}_4 = 0.5 + 5 = 5.5\)</span></li>
<li><span class="arithmatex">\(\dot{v}_7 = \dot{v}_6 - \dot{v}_5 = 5.5 - 0 = 5.5\)</span>
最终得到 <span class="arithmatex">\(\frac{\partial y}{\partial x_1} = \dot{v}_7 = 5.5\)</span></li>
<li>本质上就是计算链式法则过程的中间结果</li>
</ul>
<h5 id="ad">前向模式 AD 的局限性<a class="headerlink" href="#ad" title="Permanent link">&para;</a></h5>
<p>对于函数 <span class="arithmatex">\(f: \mathbb{R}^n \to \mathbb{R}^k\)</span>，我们需要 <span class="arithmatex">\(n\)</span> 次前向 AD 传递来获取相对于每个输入的梯度。在机器学习中，我们通常关注 <span class="arithmatex">\(k = 1\)</span> 但 <span class="arithmatex">\(n\)</span> 很大的情况，这使得前向模式 AD 效率低下。</p>
<h4 id="reverse-ad">反向模式自动微分 (Reverse AD)<a class="headerlink" href="#reverse-ad" title="Permanent link">&para;</a></h4>
<p><img alt="" src="10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/IMG-10-414%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-20250309225625857.png" />
定义伴随值adjoint <span class="arithmatex">\(\bar{v}_i = \frac{\partial y}{\partial v_i}\)</span>，然后按照计算图的反向拓扑顺序迭代计算 <span class="arithmatex">\(\bar{v}_i\)</span>：</p>
<ul>
<li>伴随值就可以用来更新参数权重<span class="arithmatex">\(\omega \leftarrow \omega - \eta \cdot \overline{\omega}\)</span></li>
</ul>
<p><strong>反向 AD 评估轨迹</strong></p>
<ul>
<li><span class="arithmatex">\(\bar{v}_7 = \frac{\partial y}{\partial v_7} = 1\)</span></li>
<li><span class="arithmatex">\(\bar{v}_6 = \frac{\partial y}{\partial v_7} \cdot \frac{\partial v_7}{\partial v_6}= \bar{v}_7  \frac{\partial v_6-v_{5}}{\partial v_6} = \bar{v}_7 \times 1 = 1\)</span></li>
<li><span class="arithmatex">\(\bar{v}_5 = \bar{v}_7 \frac{\partial v_7}{\partial v_5} = \bar{v}_7 \times -1 = -1\)</span></li>
<li><span class="arithmatex">\(\bar{v}_4 = \bar{v}_6 \frac{\partial v_6}{\partial v_4} = \bar{v}_6 \times 1 = 1\)</span></li>
<li><span class="arithmatex">\(\bar{v}_3 = \bar{v}_6 \frac{\partial v_6}{\partial v_3} = \bar{v}_6 \times 1 = 1\)</span></li>
<li><span class="arithmatex">\(\bar{v}_2 = \bar{v}_5 \frac{\partial v_5}{\partial v_2} + \bar{v}_4 \frac{\partial v_4}{\partial v_2} = \bar{v}_5 \times \cos(v_2) + \bar{v}_4 \times v_1 = -0.284 + 2 = 1.716\)</span></li>
<li><span class="arithmatex">\(\bar{v}_1 = \bar{v}_4 \frac{\partial v_4}{\partial v_1} + \bar{v}_3 \frac{\partial v_3}{\partial v_1} = \bar{v}_4 \times v_2 + \bar{v}_3 \frac{1}{v_1} = 5 + \frac{1}{2} = 5.5\)</span></li>
</ul>
<h5 id="_27"><strong>多路径情况的推导</strong><a class="headerlink" href="#_27" title="Permanent link">&para;</a></h5>
<p><img alt="" src="10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/IMG-10-414%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-20250309233851510.png" />
当节点在多个路径中使用时（如 <span class="arithmatex">\(v_1\)</span> 在 <span class="arithmatex">\(v_2\)</span> 和 <span class="arithmatex">\(v_3\)</span> 路径中使用）：</p>
<p><span class="arithmatex">\(\bar{v}_1 = \frac{\partial y}{\partial v_1} = \frac{\partial f(v_2, v_3)}{\partial v_2}\frac{\partial v_2}{\partial v_1} + \frac{\partial f(v_2, v_3)}{\partial v_3}\frac{\partial v_3}{\partial v_1} = \bar{v}_2\frac{\partial v_2}{\partial v_1} + \bar{v}_3\frac{\partial v_3}{\partial v_1}\)</span></p>
<p>定义"部分伴随值" <span class="arithmatex">\(\overline{v_{i \to j}} = \bar{v}_j \frac{\partial v_j}{\partial v_i}\)</span> 用于每对输入-输出节点，则：</p>
<p><span class="arithmatex">\(\bar{v}_i = \sum_{j \in next(i)} \overline{{v}_{i \to j}}\)</span></p>
<ul>
<li><span class="arithmatex">\(\bar{v}_{1}=\overline{v_{1\rightarrow 2}}+\overline{v_{1\rightarrow 3}}\)</span>
so, 我们可以分别计算下游节点-&gt;上游节点的多有部分伴随值，然后将它们<strong>相加</strong>, 然后得到上游变量的伴随值。</li>
</ul>
<h5 id="ad_1">反向 AD 算法<a class="headerlink" href="#ad_1" title="Permanent link">&para;</a></h5>
<p><img alt="" src="10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/IMG-10-414%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-20250309235052794.png" /></p>
<h5 id="ad_2">通过扩展计算图实现反向模式 AD<a class="headerlink" href="#ad_2" title="Permanent link">&para;</a></h5>
<p>反向模式 AD 可以通过构建一个计算图来计算伴随值实现。这个过程中，我们扩展原始计算图(算法中的<code>append Vktoj to node_to_grad[k]</code>)，添加用于计算伴随值的节点。
<img alt="" src="10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/IMG-10-414%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-20250310000050555.png" /></p>
<h5 id="ad_3">反向模式 AD 与反向传播的比较<a class="headerlink" href="#ad_3" title="Permanent link">&para;</a></h5>
<p><img alt="" src="10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/IMG-10-414%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-20250310000555493.png" /></p>
<p><strong>反向传播</strong>：</p>
<ul>
<li>在相同的前向图上运行反向操作</li>
<li>用于第一代深度学习框架（如 caffe, cuda-convnet）</li>
</ul>
<p><strong>扩展计算图的反向模式 AD</strong>：</p>
<ul>
<li>为<strong>伴随值</strong>构造单独的图节点, 可以对反向传播过程进行专门的优化</li>
<li>用于现代深度学习框架</li>
</ul>
<h5 id="ad_4">张量上的反向模式 AD<a class="headerlink" href="#ad_4" title="Permanent link">&para;</a></h5>
<p>对于张量值，定义<strong>伴随张量</strong> <span class="arithmatex">\(\bar{Z}=\begin{bmatrix}\frac{\partial y}{\partial Z_{1,1}}&amp;...&amp;\frac{\partial y}{\partial Z_{1,n}}\\...&amp;...&amp;...\\\frac{\partial y}{\partial Z_{m,1}}&amp;...&amp;\frac{\partial y}{\partial Z_{m,n}}\end{bmatrix}\)</span>
<img alt="" src="10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/IMG-10-414%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-20250310001620078.png" />
例如，对于矩阵乘法 <span class="arithmatex">\(Z = XW\)</span>，<span class="arithmatex">\(v = f(Z)\)</span>：</p>
<p><strong>标量形式的反向评估</strong>：
<span class="arithmatex">\(\overline{X_{i,k}} = \sum_j \frac{\partial Z_{i,j}}{\partial X_{i,k}} \bar{Z}_{i,j} = \sum_j W_{k,j} \bar{Z}_{i,j}\)</span></p>
<p><strong>矩阵形式</strong>：
<span class="arithmatex">\(\bar{X} = \bar{Z}W^T\)</span></p>
<h5 id="_28">梯度的梯度<a class="headerlink" href="#_28" title="Permanent link">&para;</a></h5>
<p>反向模式 AD 的结果仍然是计算图。我们可以通过组合更多操作来进一步扩展该图，并在梯度上再次运行反向模式 AD。</p>
<h5 id="ad_5">数据结构上的反向模式 AD<a class="headerlink" href="#ad_5" title="Permanent link">&para;</a></h5>
<p>对于数据结构，我们也可以定义相应的伴随数据结构。
<img alt="" src="10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/IMG-10-414%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-20250310002737506.png" />
例如，对于字典定义<span class="arithmatex">\(\hat{d}=\left\{ \text{"cat"}: \frac{\partial{y}}{\partial a_{0}}, \text{"dog"}: \frac{\partial{y}}{\partial a_{1}}  \right\}\)</span>：
<img alt="" src="10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/IMG-10-414%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-20250310002745471.png" /></p>
<p><strong>关键思想</strong>：定义与前向值相同数据类型的"伴随值"和伴随传播规则，然后应用相同的算法。</p>
<h2 id="lec6">Lec6 全连接网络、优化与初始化<a class="headerlink" href="#lec6" title="Permanent link">&para;</a></h2>
<h3 id="61-fully-connected-networks">6.1 全连接网络（Fully Connected Networks）<a class="headerlink" href="#61-fully-connected-networks" title="Permanent link">&para;</a></h3>
<h4 id="mlp_1">多层感知机（MLP）的数学定义<a class="headerlink" href="#mlp_1" title="Permanent link">&para;</a></h4>
<p><strong>L 层</strong>全连接网络（又称 MLP）的迭代公式：
  <span class="arithmatex">\(z_{i+1} = \sigma_i(W_i^T z_i + b_i),\quad i = 1,\dots,L\)</span>
 <span class="arithmatex">\(h_\theta(x) \equiv z_{L+1},\quad z_1 \equiv x\)</span></p>
<ul>
<li>参数集：<span class="arithmatex">\(\theta = \{W_{1:L}, b_{1:L}\}\)</span></li>
<li><span class="arithmatex">\(\sigma_i(x)\)</span>：非线性激活函数，通常最后一层 <span class="arithmatex">\(\sigma_L(x)=x\)</span>。</li>
</ul>
<h4 id="broadcasting">矩阵形式与广播（Broadcasting）<a class="headerlink" href="#broadcasting" title="Permanent link">&para;</a></h4>
<p>考虑到 batch 大小 <span class="arithmatex">\(m\)</span> 时的矩阵形式：
<span class="arithmatex">\(Z_{i+1} = \sigma_i(Z_i W_i + \mathbf{1}b_i^T)\)</span></p>
<ul>
<li>其中 <span class="arithmatex">\(Z_i \in \mathbb{R}^{m\times n}\)</span>，<span class="arithmatex">\(\mathbf{1}b_i^T \in \mathbb{R}^{m\times n}\)</span>。</li>
<li>实践中不真正构造 <span class="arithmatex">\(\mathbf{1}b_i^T\)</span>，而是利用「广播」机制：<ul>
<li>将 <span class="arithmatex">\(n\times 1\)</span> 向量<span class="arithmatex">\(b_i\)</span>按列复制 <span class="arithmatex">\(p\)</span> 次，<strong>不额外复制数据</strong>。</li>
<li>这样, 这个式子可以简写为<span class="arithmatex">\(Z_{i+1} = \sigma_i(Z_i W_i + b_i^T)\)</span>  (在矩阵加法时自动应用广播)</li>
</ul>
</li>
</ul>
<h4 id="_29">训练前必须回答的关键问题<a class="headerlink" href="#_29" title="Permanent link">&para;</a></h4>
<ol>
<li>如何选网络的宽度和深度？</li>
<li>如何优化目标函数？（SGD 是答案之一，但实践中常用改进版本）</li>
<li>如何初始化权重？</li>
<li>如何确保网络在多次迭代后仍易于训练？</li>
</ol>
<p>这些问题会相互影响, 且没有特定的回答</p>
<p>但对于dl,  他们最终服务于场景, 之后会介绍一些基本原则</p>
<h3 id="62-optimization">6.2 优化算法（Optimization）<a class="headerlink" href="#62-optimization" title="Permanent link">&para;</a></h3>
<h4 id="gradient-descent">梯度下降（Gradient Descent）<a class="headerlink" href="#gradient-descent" title="Permanent link">&para;</a></h4>
<p>通用更新规则：</p>
<p><span class="arithmatex">\(\theta_{t+1} := \theta_t - \alpha \nabla_\theta f(\theta_t)\)</span></p>
<ul>
<li><span class="arithmatex">\(\alpha&gt;0\)</span>：步长（学习率）, t: 迭代次数</li>
<li>局部最快下降方向，但大尺度可能出现震荡。</li>
</ul>
<h4 id="newtons-method">牛顿法（Newton’s Method）<a class="headerlink" href="#newtons-method" title="Permanent link">&para;</a></h4>
<p>利用 Hessian 矩阵 <span class="arithmatex">\(H_t=\nabla_\theta^2 f(\theta_t) = \begin{bmatrix} \dfrac{\partial^2 f}{\partial \theta_1^2} &amp; \dfrac{\partial^2 f}{\partial \theta_1 \partial \theta_2} &amp; \cdots &amp; \dfrac{\partial^2 f}{\partial \theta_1 \partial \theta_n} \\[1em] \dfrac{\partial^2 f}{\partial \theta_2 \partial \theta_1} &amp; \dfrac{\partial^2 f}{\partial \theta_2^2} &amp; \cdots &amp; \dfrac{\partial^2 f}{\partial \theta_2 \partial \theta_n} \\[1em] \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\[1em] \dfrac{\partial^2 f}{\partial \theta_n \partial \theta_1} &amp; \dfrac{\partial^2 f}{\partial \theta_n \partial \theta_2} &amp; \cdots &amp; \dfrac{\partial^2 f}{\partial \theta_n^2} \end{bmatrix}\)</span>：</p>
<div class="arithmatex">\[
\theta_{t+1} = \theta_t - \alpha H_t^{-1}\nabla_\theta f(\theta_t)
\]</div>
<p><strong>特点</strong>：</p>
<ul>
<li>对二次函数一次迭代即可收敛（<span class="arithmatex">\(\alpha=1\)</span>）。</li>
<li>深度学习中<strong>不实用</strong>：① 求逆 Hessian 代价高；② 非凸时不一定好。</li>
</ul>
<h4 id="momentum">动量法（Momentum）<a class="headerlink" href="#momentum" title="Permanent link">&para;</a></h4>
<p>在梯度基础上引入动量：</p>
<div class="arithmatex">\[
\begin{aligned}
u_{t+1} &amp;= \beta u_t + (1-\beta)\nabla_\theta f(\theta_t) \\ \theta_{t+1} &amp;= \theta_t - \alpha u_{t+1}
\end{aligned}
\]</div>
<ul>
<li><span class="arithmatex">\(\beta\)</span>：动量系数, 通常<span class="arithmatex">\(0\leq \beta &lt;1\)</span>，平滑更新方向，减少震荡。</li>
</ul>
<div class="arithmatex">\[
\begin{aligned}
u_{t+1} &amp;= \beta u_t + (1-\beta)\nabla_\theta f(\theta_t) \\&amp;= \beta u_{t} +\beta(1-\beta)\nabla_\theta f(\theta_{t-1}) + \beta^2(1-\beta)\nabla_\theta f(\theta_{t-2})+\dots
\end{aligned}
\]</div>
<ul>
<li>
<p><strong>无偏修正</strong>（unbiased）：<span class="arithmatex">\(\theta_{t+1} = \theta_t - \alpha \frac{u_{t+1}}{1-\beta^{t+1}}\)</span></p>
<ul>
<li>在训练的初期, 历史梯度不够大, 导致动量不够有效, 加上修正能够提高收敛速度</li>
<li>(左: 修正前, 右: 修正后), 随着t增加, 分母趋近于1, 去除偏差</li>
</ul>
<p><img alt="image-20250817181507556" src="10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20250817181507556.png" /></p>
</li>
</ul>
<h4 id="nesterov">Nesterov 动量<a class="headerlink" href="#nesterov" title="Permanent link">&para;</a></h4>
<p>「未来点」计算梯度：</p>
<div class="arithmatex">\[
\begin{aligned}
u_{t+1} &amp;= \beta u_t + (1-\beta)\nabla_\theta f(\theta_t - \alpha u_t)\\ \theta_{t+1} &amp;= \theta_t - \alpha u_{t+1}
\end{aligned}
\]</div>
<h4 id="adam">Adam 优化器<a class="headerlink" href="#adam" title="Permanent link">&para;</a></h4>
<p>结合动量 + 自适应学习率：</p>
<div class="arithmatex">\[
\begin{aligned}
u_{t+1} &amp;= \beta_1 u_t + (1-\beta_1)\nabla_\theta f(\theta_t)\\
v_{t+1} &amp;= \beta_2 v_t + (1-\beta_2)(\nabla_\theta f(\theta_t))^{2(逐元素平方)} \\
\theta_{t+1} &amp;= \theta_t - \alpha \frac{\hat u_{t+1}}{\sqrt{\hat v_{t+1}}+\varepsilon}
\end{aligned}
\]</div>
<ul>
<li><span class="arithmatex">\(\hat u, \hat v\)</span>：偏差修正后的动量</li>
<li>实践中 Adam 几乎成为默认选择，但仍存在争议</li>
</ul>
<h4 id="sgd_1">随机梯度下降（SGD）<a class="headerlink" href="#sgd_1" title="Permanent link">&para;</a></h4>
<p><strong>核心思想</strong>：用 mini-batch 估计梯度：</p>
<p><span class="arithmatex">\(\theta_{t+1} = \theta_t - \alpha \frac{1}{|B|}\sum_{i\in B}\nabla_\theta \ell(h_\theta(x_i),y_i)\)</span></p>
<ul>
<li>优点：计算快，噪声大但收敛好</li>
<li><strong>重要提醒</strong>：<ul>
<li>所有前述算法（Momentum、Adam、Newton 等）在实际中都使用<strong>随机版本</strong></li>
<li>简单凸二次实验带来的直觉有限，必须在真实网络上大量实验</li>
</ul>
</li>
</ul>
<h3 id="63-initialization">6.3 权重初始化（Initialization）<a class="headerlink" href="#63-initialization" title="Permanent link">&para;</a></h3>
<h4 id="0">为什么不能初始化为 0？<a class="headerlink" href="#0" title="Permanent link">&para;</a></h4>
<div class="arithmatex">\[
\begin{aligned}
&amp;Z_{i+1}=\sigma_i(Z_iW_i)\\
&amp;G_i=\left(G_{i+1}\circ\sigma_i^{\prime}\left(Z_iW_i\right)\right)W_i^T\end{aligned}
\]</div>
<p>若 <span class="arithmatex">\(W_i=0\)</span>，则：</p>
<ul>
<li>前向：<span class="arithmatex">\(Z_{i+1}=0\)</span>；</li>
<li>反向：<span class="arithmatex">\(G_i=0\)</span>，导致 <span class="arithmatex">\(\nabla_{W_i}\ell=0\)</span>。</li>
</ul>
<ul>
<li>结果：所有层陷入鞍点，无法学习。</li>
</ul>
<h4 id="_30">随机初始化的影响<a class="headerlink" href="#_30" title="Permanent link">&para;</a></h4>
<p>随机初始化, 以 <span class="arithmatex">\(W_i \sim \mathcal{N}(0,\sigma^2 I)\)</span> 为例：</p>
<ul>
<li><strong>方差 <span class="arithmatex">\(\sigma^2\)</span> 决定</strong>：<ol>
<li>前向激活的范数 <span class="arithmatex">\(\|Z_i\|\)</span>；</li>
<li>反向梯度的范数 <span class="arithmatex">\(\|\nabla_{W_i}\ell\|\)</span>。</li>
</ol>
</li>
</ul>
<div class="arithmatex">\[
L_{2}范数: ||X||_{2}=\sqrt{ x_{1}^2+x_{2}^2\dots x_{n}^2 }
\]</div>
<ul>
<li><strong>实验现象</strong>（50 层 ReLU，隐藏层 100 单元）：<ul>
<li><span class="arithmatex">\(\sigma^2=\frac{1}{n}\)</span>：梯度指数级消失；</li>
<li><span class="arithmatex">\(\sigma^2=\frac{2}{n}\)</span>：近乎常数；</li>
<li><span class="arithmatex">\(\sigma^2=\frac{3}{n}\)</span>：梯度爆炸。</li>
<li><img alt="image-20250817221101976" src="10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20250817221101976.png" /></li>
</ul>
</li>
</ul>
<h4 id="_31">权重移动并不大<a class="headerlink" href="#_31" title="Permanent link">&para;</a></h4>
<ul>
<li>经验观察：训练后权重距离初始点很近，不同初始化收敛到不同区域。</li>
<li><strong>结论</strong>：初始化对最终性能影响巨大。</li>
</ul>
<h4 id="kaiming">方差推导与 Kaiming 初始化<a class="headerlink" href="#kaiming" title="Permanent link">&para;</a></h4>
<p>假设 <span class="arithmatex">\(x\sim\mathcal{N}(0,1)\)</span>，<span class="arithmatex">\(w\sim\mathcal{N}(0,\frac{1}{n})\)</span>，则：</p>
<ul>
<li><span class="arithmatex">\(\begin{aligned}\mathbf{E}[x_iw_i]=\mathbf{E}[x_i]\mathbf{E}[w_i]=0,\quad&amp;\mathrm{Var}[x_iw_i]=\mathbf{Var}[x_i]\mathbf{Var}[w_i]=1/n\end{aligned}\)</span></li>
<li><span class="arithmatex">\(\mathbf{E}[w^T x]=0,\quad \mathbf{Var}(w^T x)=1\)</span>。</li>
<li>如果使用了线性激活层, <span class="arithmatex">\(z_i \sim\mathcal{N}(0,1), W_i\sim\mathcal{N}(0,\frac{1}{n}I)\)</span>, 则<span class="arithmatex">\(z_{i+1}=W_i^Tz_i\sim N(0,I)\)</span></li>
</ul>
<p>对 ReLU（约一半神经元置 0），需将方差放大 2 倍：</p>
<ul>
<li><strong>Kaiming 正态初始化</strong>：<span class="arithmatex">\(W_i\sim\mathcal{N}(0,\frac{2}{n}I)\)</span>。</li>
</ul>
<h4 id="kaiming_1">Kaiming 初始化的推导<a class="headerlink" href="#kaiming_1" title="Permanent link">&para;</a></h4>
<p>在深度学习中，He 初始化（Kaiming 初始化）是一种针对 <strong>ReLU 激活函数</strong> 的权重初始化方法。它的核心目标是保持每一层的输出方差一致，从而避免梯度消失或爆炸问题。以下是推导过程的详细解释：</p>
<hr />
<h5 id="1"><strong>1. 基本设定</strong><a class="headerlink" href="#1" title="Permanent link">&para;</a></h5>
<p>假设我们有一个全连接层（或卷积层），其输入为 <span class="arithmatex">\(x^{(l-1)}\)</span>，权重矩阵为 <span class="arithmatex">\(W \in \mathbb{R}^{n_{\text{out}} \times n_{\text{in}}}\)</span>，输出为：</p>
<div class="arithmatex">\[
y^{(l)} = x^{(l-1)} W^T
\]</div>
<p>然后通过 ReLU 激活函数：</p>
<div class="arithmatex">\[
x^{(l)} = \max(y^{(l)}, 0)
\]</div>
<p>我们的目标是初始化权重 <span class="arithmatex">\(W\)</span>，使得每一层的输出方差 <span class="arithmatex">\(\text{Var}[y^{(l)}]\)</span> 与前一层的输入方差 <span class="arithmatex">\(\text{Var}[y^{(l-1)}]\)</span> 保持一致。</p>
<h5 id="2"><strong>2. 方差推导</strong><a class="headerlink" href="#2" title="Permanent link">&para;</a></h5>
<p>假设权重 <span class="arithmatex">\(W_{i,j}\)</span> 独立同分布，且输入 <span class="arithmatex">\(x_j\)</span> 的均值为 0，方差为 <span class="arithmatex">\(\text{Var}[x_j] = \frac{1}{2} \text{Var}[y^{(l-1)}]\)</span>（这是 ReLU 的特性）。<br />
对于输出<span class="arithmatex">\(y_i\)</span>：</p>
<div class="arithmatex">\[
y_i = \sum_{j=1}^{n_{\text{in}}} x_j W_{i,j}
\]</div>
<p>由于<span class="arithmatex">\(x_j\)</span>和<span class="arithmatex">\(W_{i,j}\)</span>独立，方差为：</p>
<div class="arithmatex">\[
\text{Var}[y_i] = \sum_{j=1}^{n_{\text{in}}} \text{Var}[x_j] \cdot \text{Var}[W_{i,j}] = n_{\text{in}} \cdot \text{Var}[x_j] \cdot \sigma^2
\]</div>
<p>其中<span class="arithmatex">\(\sigma^2 = \text{Var}[W_{i,j}]\)</span>是权重的方差。</p>
<h5 id="3-relu">3. ReLU 对输入分布的影响<a class="headerlink" href="#3-relu" title="Permanent link">&para;</a></h5>
<p>由于<span class="arithmatex">\(x^{(l-1)}\)</span>是 ReLU 的输出，即<span class="arithmatex">\(x^{(l-1)} = \max(y^{(l-2)}, 0)\)</span> ，因此<span class="arithmatex">\(x_j\)</span>的分布是对称的（一半为 0，另一半为正数）。<br />
此时，<span class="arithmatex">\(x_j\)</span>的平方期望为：</p>
<div class="arithmatex">\[
\mathbb{E}[x_j^2] = \mathbb{E}[\text{ReLU}(y^{(l-1)})^2] = \frac{1}{2} \text{Var}[y^{(l-1)}]
\]</div>
<p>（因为 ReLU 将负值设为 0，正值保留，导致平方期望变为原方差的一半。）</p>
<h5 id="4">4. 保持方差一致的条件<a class="headerlink" href="#4" title="Permanent link">&para;</a></h5>
<p>为了使输出方差<span class="arithmatex">\(\text{Var}[y^{(l)}]\)</span>与输入方差<span class="arithmatex">\(\text{Var}[y^{(l-1)}]\)</span>一致，代入上式：</p>
<div class="arithmatex">\[
\text{Var}[y^{(l)}] = n_{\text{in}} \cdot \left( \frac{1}{2} \text{Var}[y^{(l-1)}] \right) \cdot \sigma^2
\]</div>
<p>要求：</p>
<div class="arithmatex">\[
\text{Var}[y^{(l)}] = \text{Var}[y^{(l-1)}]
\]</div>
<p>解得：</p>
<div class="arithmatex">\[
n_{\text{in}} \cdot \frac{1}{2} \sigma^2 = 1 \quad \Rightarrow \quad \sigma^2 = \frac{2}{n_{\text{in}}}
\]</div>
<p>因此，权重<span class="arithmatex">\(W\)</span>应初始化为均值为 0、方差为<span class="arithmatex">\(\sigma^2 = \frac{2}{n_{\text{in}}}\)</span>的分布。</p>
<h3 id="64">6.4 小结与后续<a class="headerlink" href="#64" title="Permanent link">&para;</a></h3>
<ul>
<li>本节课覆盖：<ul>
<li>MLP 的数学与广播实现；</li>
<li>从 GD → Newton → Momentum → Nesterov → Adam 的优化演进；</li>
<li>随机梯度下降的核心地位；</li>
<li>初始化如何影响信号传播与最终解。</li>
</ul>
</li>
<li>下节课将深入更多实践技巧与实验分析。</li>
</ul>
<h2 id="lec7">Lec7 神经网络库抽象<a class="headerlink" href="#lec7" title="Permanent link">&para;</a></h2>
<h3 id="71-programming-abstractions">7.1 编程抽象（Programming Abstractions）<a class="headerlink" href="#71-programming-abstractions" title="Permanent link">&para;</a></h3>
<p>框架的编程抽象定义了实现、扩展和执行模型计算的通用方式。理解设计背后的思考过程有助于：</p>
<ul>
<li>理解抽象设计的原因</li>
<li>学习设计新抽象的经验</li>
</ul>
<h4 id="case-studies">案例研究（Case Studies）<a class="headerlink" href="#case-studies" title="Permanent link">&para;</a></h4>
<p>因时间有限未深入研究的框架：Theano, Torch7, MXNet, Caffe2, Chainer, JAX...</p>
<h4 id="_32">三种核心抽象模式<a class="headerlink" href="#_32" title="Permanent link">&para;</a></h4>
<h5 id="1-forward-and-backward-layer-interface">1. 前向与反向层接口（Forward and backward layer interface）<a class="headerlink" href="#1-forward-and-backward-layer-interface" title="Permanent link">&para;</a></h5>
<blockquote>
<p><em>代表框架：Caffe 1.0</em></p>
</blockquote>
<div class="language-python highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="k">class</span><span class="w"> </span><span class="nc">Layer</span><span class="p">:</span>
</span><span id="__span-0-2"><a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="n">bottom</span><span class="p">,</span> <span class="n">top</span><span class="p">):</span>  <span class="c1"># 前向计算</span>
</span><span id="__span-0-3"><a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>        <span class="k">pass</span>
</span><span id="__span-0-4"><a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="n">top</span><span class="p">,</span> <span class="n">propagate_down</span><span class="p">,</span> <span class="n">bottom</span><span class="p">):</span>  <span class="c1"># 反向传播梯度</span>
</span><span id="__span-0-5"><a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>        <span class="k">pass</span>
</span></code></pre></div>
<ul>
<li><strong>​特点​</strong>​：显式定义前向计算和梯度计算操作</li>
<li><strong>​早期先驱​</strong>​：cuda-convnet（AlexNet框架）</li>
</ul>
<h5 id="2-computational-graph-and-declarative-programming">2. 计算图与声明式编程（Computational graph and declarative programming）<a class="headerlink" href="#2-computational-graph-and-declarative-programming" title="Permanent link">&para;</a></h5>
<blockquote>
<p><em>代表框架：<code>TensorFlow 1.0</code></em></p>
</blockquote>
<div class="language-python highlight"><pre><span></span><code><span id="__span-1-1"><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a><span class="kn">import</span><span class="w"> </span><span class="nn">tensorflow</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">tf</span>
</span><span id="__span-1-2"><a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a>
</span><span id="__span-1-3"><a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a><span class="n">v1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">()</span>
</span><span id="__span-1-4"><a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a><span class="n">v2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">v1</span><span class="p">)</span>
</span><span id="__span-1-5"><a id="__codelineno-1-5" name="__codelineno-1-5" href="#__codelineno-1-5"></a><span class="n">v3</span> <span class="o">=</span> <span class="n">v2</span> <span class="o">+</span> <span class="mi">1</span>
</span><span id="__span-1-6"><a id="__codelineno-1-6" name="__codelineno-1-6" href="#__codelineno-1-6"></a><span class="n">v4</span> <span class="o">=</span> <span class="n">v2</span> <span class="o">*</span> <span class="n">v3</span>
</span><span id="__span-1-7"><a id="__codelineno-1-7" name="__codelineno-1-7" href="#__codelineno-1-7"></a>
</span><span id="__span-1-8"><a id="__codelineno-1-8" name="__codelineno-1-8" href="#__codelineno-1-8"></a><span class="n">sess</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span>
</span><span id="__span-1-9"><a id="__codelineno-1-9" name="__codelineno-1-9" href="#__codelineno-1-9"></a><span class="n">value4</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">v4</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">v1</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">])})</span>
</span></code></pre></div>
<ul>
<li><strong>​特点​</strong>​：先声明计算图 → 通过输入数据(<code>sess.run</code>)执行图</li>
<li><strong>​早期先驱​</strong>​：Theano
计算图定义和计算操作分离, 可以有更多的优化机会</li>
<li>假如计算的不是v4, 而是v3, <code>v4 = v2 * v3</code>j计算可以直接skip</li>
</ul>
<h5 id="3-imperative-automatic-differentiation">3. 命令式自动微分（Imperative automatic differentiation）<a class="headerlink" href="#3-imperative-automatic-differentiation" title="Permanent link">&para;</a></h5>
<blockquote>
<p><em>代表框架：PyTorch (Needle)</em></p>
</blockquote>
<div class="language-python highlight"><pre><span></span><code><span id="__span-2-1"><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a><span class="kn">import</span><span class="w"> </span><span class="nn">needle</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">ndl</span>
</span><span id="__span-2-2"><a id="__codelineno-2-2" name="__codelineno-2-2" href="#__codelineno-2-2"></a>
</span><span id="__span-2-3"><a id="__codelineno-2-3" name="__codelineno-2-3" href="#__codelineno-2-3"></a><span class="n">v1</span> <span class="o">=</span> <span class="n">ndl</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span>
</span><span id="__span-2-4"><a id="__codelineno-2-4" name="__codelineno-2-4" href="#__codelineno-2-4"></a><span class="n">v2</span> <span class="o">=</span> <span class="n">ndl</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">v1</span><span class="p">)</span>
</span><span id="__span-2-5"><a id="__codelineno-2-5" name="__codelineno-2-5" href="#__codelineno-2-5"></a><span class="n">v3</span> <span class="o">=</span> <span class="n">v2</span> <span class="o">+</span> <span class="mi">1</span>
</span><span id="__span-2-6"><a id="__codelineno-2-6" name="__codelineno-2-6" href="#__codelineno-2-6"></a><span class="n">v4</span> <span class="o">=</span> <span class="n">v2</span> <span class="o">*</span> <span class="n">v3</span>
</span><span id="__span-2-7"><a id="__codelineno-2-7" name="__codelineno-2-7" href="#__codelineno-2-7"></a>
</span><span id="__span-2-8"><a id="__codelineno-2-8" name="__codelineno-2-8" href="#__codelineno-2-8"></a><span class="k">if</span> <span class="n">v4</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">:</span> 
</span><span id="__span-2-9"><a id="__codelineno-2-9" name="__codelineno-2-9" href="#__codelineno-2-9"></a>    <span class="n">v5</span> <span class="o">=</span> <span class="n">v4</span> <span class="o">*</span> <span class="mi">2</span> 
</span><span id="__span-2-10"><a id="__codelineno-2-10" name="__codelineno-2-10" href="#__codelineno-2-10"></a><span class="k">else</span><span class="p">:</span> 
</span><span id="__span-2-11"><a id="__codelineno-2-11" name="__codelineno-2-11" href="#__codelineno-2-11"></a>    <span class="n">v5</span> <span class="o">=</span> <span class="n">v4</span>
</span><span id="__span-2-12"><a id="__codelineno-2-12" name="__codelineno-2-12" href="#__codelineno-2-12"></a><span class="n">v5</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>  <span class="c1"># 即时反向传播</span>
</span></code></pre></div>
<ul>
<li><strong>​特点​</strong>​：<strong>动态构建</strong>计算图 + 支持Python控制流</li>
<li><strong>​早期先驱​</strong>​：Chainer</li>
</ul>
<h4 id="discussions">讨论（Discussions）<a class="headerlink" href="#discussions" title="Permanent link">&para;</a></h4>
<p><strong>​不同编程抽象的优缺点分析​</strong>​：</p>
<table>
<thead>
<tr>
<th>抽象类型</th>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td>前向/反向接口 (Caffe)</td>
<td>接口简单，控制明确</td>
<td>模块间耦合度高，扩展性受限</td>
</tr>
<tr>
<td>计算图 (TensorFlow)</td>
<td>支持跨设备优化，部署效率高</td>
<td>调试困难，控制流实现复杂</td>
</tr>
<tr>
<td>命令式自动微分 (PyTorch)</td>
<td>开发调试便捷，支持动态图</td>
<td>运行时计算, 导致优化机会较少</td>
</tr>
</tbody>
</table>
<h3 id="72-high-level-modular-library-components">7.2 高级模块化库组件（High Level Modular Library Components）<a class="headerlink" href="#72-high-level-modular-library-components" title="Permanent link">&para;</a></h3>
<h4 id="elements-of-machine-learning">机器学习核心元素（Elements of Machine Learning）<a class="headerlink" href="#elements-of-machine-learning" title="Permanent link">&para;</a></h4>
<ol>
<li><strong>假设类（Hypothesis Class）</strong>：<img src="./10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20250819200542864.png" alt="image-20250819200542864" style="zoom:25%;" /><span class="arithmatex">\(h_θ(x)\)</span></li>
<li><strong>​损失函数（Loss Function）​</strong>​：  <span class="arithmatex">\(l(h_θ(x),y)=−h_y(x)+log∑_{j=1}^k exp(h_j(x))\)</span></li>
<li><strong>优化方法（Optimization Method）</strong>：  <span class="arithmatex">\(θ:=θ−\fracαB∑_{i=1}^B ∇_θ \ell(h_θ(x(i)),y(i))\)</span></li>
</ol>
<p><strong>​关键问题​</strong>​：如何将上述元素转化为代码中的模块化组件？</p>
<h4 id="_33">深度学习的模块化本质<a class="headerlink" href="#_33" title="Permanent link">&para;</a></h4>
<p>深度学习模型本质是模块的组合（如多层残差网络）：</p>
<p><img alt="image-20250819201040536" src="10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20250819201040536.png" /></p>
<p><em>残差连接结构示意图（摘自ResNet论文）</em></p>
<ul>
<li>上图的每一个方框都可以视为一个模块</li>
</ul>
<h4 id="_34">模块化组件设计<a class="headerlink" href="#_34" title="Permanent link">&para;</a></h4>
<p><img alt="image-20250819210558208" src="10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20250819210558208.png" /></p>
<h5 id="1-nnmodule">1. <code>nn.Module</code>：组合模块<a class="headerlink" href="#1-nnmodule" title="Permanent link">&para;</a></h5>
<p><img src="./10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20250819204639494.png" alt="image-20250819204639494" style="zoom:50%;" /></p>
<p><img src="./10-414_深度学习系统课程笔记.assets/image-20250819204700700.png" alt="image-20250819204700700" style="zoom:50%;" /></p>
<ul>
<li><strong>​核心功能​</strong>​：tensor in → tensor out</li>
<li><strong>​关键要素​</strong>​：<ul>
<li>给定输入计算输出</li>
<li>获取（可训练）参数列表</li>
<li>参数初始化方法</li>
</ul>
</li>
<li><strong>​代码示例​</strong>​：</li>
</ul>
<div class="language-python highlight"><pre><span></span><code><span id="__span-3-1"><a id="__codelineno-3-1" name="__codelineno-3-1" href="#__codelineno-3-1"></a><span class="k">class</span><span class="w"> </span><span class="nc">Linear</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
</span><span id="__span-3-2"><a id="__codelineno-3-2" name="__codelineno-3-2" href="#__codelineno-3-2"></a> <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">):</span>
</span><span id="__span-3-3"><a id="__codelineno-3-3" name="__codelineno-3-3" href="#__codelineno-3-3"></a>  <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>  <span class="c1"># 可训练参数</span>
</span><span id="__span-3-4"><a id="__codelineno-3-4" name="__codelineno-3-4" href="#__codelineno-3-4"></a>  <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
</span><span id="__span-3-5"><a id="__codelineno-3-5" name="__codelineno-3-5" href="#__codelineno-3-5"></a> <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
</span><span id="__span-3-6"><a id="__codelineno-3-6" name="__codelineno-3-6" href="#__codelineno-3-6"></a>  <span class="k">return</span> <span class="n">X</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>
</span></code></pre></div>
<h5 id="2_1">2. 损失函数：特殊模块<a class="headerlink" href="#2_1" title="Permanent link">&para;</a></h5>
<p><img src="./10-414_深度学习系统课程笔记.assets/image-20250819204726660.png" alt="image-20250819204726660" style="zoom:50%;" /></p>
<ul>
<li><strong>​特性​</strong>​：张量输入 → 标量输出  <ul>
<li>例如交叉熵损失：  <span class="arithmatex">\(l(h_\theta(x),y)=−h_y(x)+log∑_{j=1}^k exp(h_j(x))\)</span></li>
</ul>
</li>
</ul>
<ul>
<li><strong>​扩展问题​</strong>​：<ul>
<li>如何组合多目标函数？</li>
<li>训练完成后推理阶段的行为？</li>
</ul>
</li>
</ul>
<h5 id="3-optimizer">3. 优化器（Optimizer）<a class="headerlink" href="#3-optimizer" title="Permanent link">&para;</a></h5>
<p><img alt="image-20250819204744418" src="10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20250819204744418.png" /></p>
<ul>
<li><strong>​功能​</strong>​：根据模型权重执行优化步骤</li>
<li><strong>​状态管理​</strong>​：跟踪辅助状态（如动量）</li>
<li><strong>​常见优化器​</strong>​：<ul>
<li><strong>​SGD​</strong>​：<span class="arithmatex">\(w_i←w_i−αg_i\)</span></li>
<li><strong>​SGD with momentum​</strong>​：  <span class="arithmatex">\(\begin{aligned}u_i​&amp;←βu_i​+(1−β)g_i\\​w_i​&amp;←w_i​−αu_i​​\end{aligned}\)</span></li>
<li><strong>​Adam​</strong>​：<span class="arithmatex">\(\begin{aligned}u_i&amp;←β_1u_i+(1−β_1)g_i\\v_i&amp;←β_2v_i+(1−β_2)g_i^2\\w_i&amp;←w_i−αu_i/(v_i^{1/2}+ϵ)\end{aligned}\)</span></li>
</ul>
</li>
</ul>
<h5 id="4_1">4. 正则化与优化器<a class="headerlink" href="#4_1" title="Permanent link">&para;</a></h5>
<ul>
<li><strong>​实现方式​</strong>​：<ul>
<li>作为损失函数的一部分（如L2正则项）</li>
<li>直接整合到优化器更新中</li>
<li><em>例：带权重衰减的SGD</em>  <span class="arithmatex">\(w_i←(1−αλ)w_i−αg_i\)</span></li>
</ul>
</li>
</ul>
<h5 id="5-initialization">5. 参数初始化（Initialization）<a class="headerlink" href="#5-initialization" title="Permanent link">&para;</a></h5>
<ul>
<li><strong>​策略依赖​</strong>​：模块类型和参数种类</li>
<li><strong>​常见方法​</strong>​：<ul>
<li>权重（weights）：均匀分布（幅度取决于输入/输出维度）</li>
<li>偏置（bias）：零初始化</li>
<li>方差累加项：初始化为1</li>
</ul>
</li>
<li><strong>​执行时机​</strong>​：在<code>nn.Module</code>构建阶段完成</li>
</ul>
<h5 id="6-data-loader-and-preprocessing">6. 数据加载与预处理（Data Loader and Preprocessing）<a class="headerlink" href="#6-data-loader-and-preprocessing" title="Permanent link">&para;</a></h5>
<ul>
<li><strong>流程</strong>：<img src="./10-414_深度学习系统课程笔记.assets/image-20250819210057075.png" alt="image-20250819210057075" style="zoom:50%;" /><ul>
<li>经常通过混洗(shuffle)和变化进行输入的预处理</li>
</ul>
</li>
</ul>
<ul>
<li><strong>​重要性​</strong>​：数据增强对深度学习模型预测精度提升有显著贡献</li>
<li><strong>​特性​</strong>​：数据管道本身具有组合性</li>
</ul>
<h4 id="discussions_1">讨论（Discussions）<a class="headerlink" href="#discussions_1" title="Permanent link">&para;</a></h4>
<p><strong>​其他可能的模块化组件示例​</strong>​：</p>
<ul>
<li>学习率调度器（Learning Rate Scheduler）</li>
<li>梯度裁剪模块（Gradient Clipping）</li>
<li>自定义激活函数（如Swish, GELU）</li>
<li>分布式训练通信原语</li>
</ul>
<hr />
<h4 id="revisit-programming-abstraction">Revisit Programming Abstraction<a class="headerlink" href="#revisit-programming-abstraction" title="Permanent link">&para;</a></h4>
<p><strong>分层抽象设计</strong></p>
<p>以PyTorch（Needle）为例：</p>
<ol>
<li><strong>​底层​</strong>​：张量计算图抽象（处理自动微分）</li>
<li><strong>​高层​</strong>​：模块化组合抽象（如<code>nn.Module</code>）</li>
</ol>
<p><strong>对比早期框架（Caffe 1.0）</strong></p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-4-1"><a id="__codelineno-4-1" name="__codelineno-4-1" href="#__codelineno-4-1"></a><span class="k">class</span><span class="w"> </span><span class="nc">Layer</span><span class="p">:</span>
</span><span id="__span-4-2"><a id="__codelineno-4-2" name="__codelineno-4-2" href="#__codelineno-4-2"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="n">bottom</span><span class="p">,</span> <span class="n">top</span><span class="p">):</span> <span class="o">...</span> 
</span><span id="__span-4-3"><a id="__codelineno-4-3" name="__codelineno-4-3" href="#__codelineno-4-3"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="n">top</span><span class="p">,</span> <span class="n">propagate_down</span><span class="p">,</span> <span class="n">bottom</span><span class="p">):</span> <span class="o">...</span>
</span></code></pre></div>
<ul>
<li><strong>​局限​</strong>​：将梯度计算与模块组合紧密耦合</li>
</ul>
<h2 id="lec9">Lec9 深度学习系统：归一化与正则化<a class="headerlink" href="#lec9" title="Permanent link">&para;</a></h2>
<h3 id="91">9.1 引言<a class="headerlink" href="#91" title="Permanent link">&para;</a></h3>
<p>本讲主要讨论深度学习中的两个关键技术：<strong>归一化（Normalization）</strong> 和 <strong>正则化（Regularization）</strong>，以及它们与优化、初始化之间的相互作用。</p>
<ul>
<li><strong>归一化</strong>：用于稳定和加速训练过程，通过调整层间激活值的分布。</li>
<li><strong>正则化</strong>：用于防止过拟合，提升模型在测试集上的泛化能力。</li>
<li>三者（优化、初始化、归一化、正则化）之间存在复杂而深刻的交互关系。</li>
</ul>
<hr />
<h3 id="92">9.2 初始化与优化的交互<a class="headerlink" href="#92" title="Permanent link">&para;</a></h3>
<p><strong>问题提出</strong>：
假设我们初始化权重为 <span class="arithmatex">\(W_i \sim \mathcal{N}(0, \frac{c}{n})\)</span>，其中对于 ReLU 网络通常选择 <span class="arithmatex">\(c = 2\)</span>。那么是否在几次优化迭代后，权重的尺度就会“自动修正”？</p>
<p><strong>答案：不会！</strong></p>
<ul>
<li>如果初始化不当，深层网络即使使用标准 SGD 也无法成功训练。</li>
<li>更根本的问题是：<strong>即使训练成功，初始化时的尺度效应在整个训练过程中仍然持续存在</strong>
  <img alt="" src="10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/IMG-10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-20250820150023447.png" /><ul>
<li>图中, 训练后不同方差的激活值和梯度趋近, 但是权重的分布与训练前变化相差不大</li>
</ul>
</li>
</ul>
<p><strong>实验观察</strong>：
<img alt="" src="10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/IMG-10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-20250820144854604.png" />
<strong>图1：不同初始化方差对激活值和梯度的影响（50层网络）</strong></p>
<table>
<thead>
<tr>
<th>初始化方差</th>
<th>激活值范数（Activation Norm）</th>
<th>梯度范数（Gradient Norm）</th>
<th>训练结果</th>
</tr>
</thead>
<tbody>
<tr>
<td><span class="arithmatex">\(\sigma^2 = \frac{3}{n}\)</span></td>
<td>随层数指数增长（&gt;10³）</td>
<td>极小（~10⁻⁸）</td>
<td>溢出（NaN）</td>
</tr>
<tr>
<td><span class="arithmatex">\(\sigma^2 = \frac{2}{n}\)</span></td>
<td>稳定在 ~1</td>
<td>保持稳定</td>
<td>✅ 成功训练</td>
</tr>
<tr>
<td><span class="arithmatex">\(\sigma^2 = \frac{1}{n}\)</span></td>
<td>指数衰减（&lt;10⁻²）</td>
<td>快速消失</td>
<td>❌ 无进展</td>
</tr>
</tbody>
</table>
<blockquote>
<p><strong>结论</strong>：初始化严重影响训练动态，且其影响贯穿整个训练过程。</p>
</blockquote>
<hr />
<h3 id="93-normalization">9.3 归一化（Normalization）<a class="headerlink" href="#93-normalization" title="Permanent link">&para;</a></h3>
<h4 id="_35">动机<a class="headerlink" href="#_35" title="Permanent link">&para;</a></h4>
<ul>
<li>初始化的影响难以在训练中“自我修复”。</li>
<li>深度网络中每一“层”可以是任意计算操作。</li>
<li>解决方案：<strong>引入额外的层，显式地“修复”每层激活值的分布</strong>。</li>
</ul>
<hr />
<h4 id="931-layer-normalization">9.3.1 层归一化（Layer Normalization）<a class="headerlink" href="#931-layer-normalization" title="Permanent link">&para;</a></h4>
<p><strong>定义</strong>：
对每一层的激活值进行归一化，使其均值为 0，方差为 1。
设第 <span class="arithmatex">\(i\)</span> 层输出为：</p>
<div class="arithmatex">\[
\hat{z}_{i+1} = \sigma_i(W_i^T z_i + b_i)
\]</div>
<ul>
<li>hat表示这是归一化前的值
则层归一化定义为：</li>
</ul>
<div class="arithmatex">\[
z_{i+1} = \frac{\hat{z}_{i+1} - \mathbb{E}[\hat{z}_{i+1}]}{\sqrt{\mathrm{Var}[\hat{z}_{i+1}] + \varepsilon}}
\]</div>
<ul>
<li><span class="arithmatex">\(\mathbb E\)</span>是均值, <span class="arithmatex">\(\mathbb E[\hat{z}_{i+1}]=\frac{1}{n}\sum_{j=1}^n(\hat{z}_{i+1})_{j}\)</span></li>
<li><span class="arithmatex">\(\varepsilon\)</span> 是一个小常数，防止除零</li>
</ul>
<p>常见扩展：
通常还会加入可学习的缩放和平移参数：</p>
<div class="arithmatex">\[
z_{i+1} = \gamma \cdot \frac{\hat{z}_{i+1} - \mu}{\sqrt{\sigma^2 + \varepsilon}} + \beta
\]</div>
<p>其中 <span class="arithmatex">\(\gamma, \beta\)</span> 是可学习的标量参数。</p>
<hr />
<h4 id="932">9.3.2 层归一化效果<a class="headerlink" href="#932" title="Permanent link">&para;</a></h4>
<p><strong>图2：层归一化前后对比</strong>
<img alt="" src="10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/IMG-10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-20250820151144362.png" /></p>
<table>
<thead>
<tr>
<th>指标</th>
<th>归一化前（<span class="arithmatex">\(\sigma^2 = 1.7/n, 2/n, 2.3/n\)</span>）</th>
<th>归一化后</th>
</tr>
</thead>
<tbody>
<tr>
<td>激活值范数</td>
<td>随层数变化剧烈（1.7 → 2.3）</td>
<td>统一为 ~1</td>
</tr>
<tr>
<td>梯度范数</td>
<td>差异明显</td>
<td>更加稳定</td>
</tr>
<tr>
<td>权重方差</td>
<td>初始差异保留</td>
<td>影响减弱</td>
</tr>
</tbody>
</table>
<blockquote>
<p><strong>结论</strong>：层归一化有效“修复”了激活值的范数问题。</p>
</blockquote>
<p><strong>实际表现</strong>：</p>
<ul>
<li>在标准全连接网络（FCN, 或者多层感知机MLP）中，<strong>较难训练到较低损失</strong>。</li>
<li>原因：不同样本之间的相对激活范数可能包含有用的判别信息，而 LayerNorm 抹平了这些差异。</li>
</ul>
<hr />
<h4 id="933-batch-normalization">9.3.3 批归一化（Batch Normalization）<a class="headerlink" href="#933-batch-normalization" title="Permanent link">&para;</a></h4>
<p>核心思想：
从“按行归一化”（LayerNorm）转向“按列归一化”——即在<strong>小批量（minibatch）维度上</strong>对每个特征进行归一化。</p>
<p>设激活矩阵为：</p>
<div class="arithmatex">\[
\hat{Z}_{i+1} = \sigma_i(Z_i W_i + b_i)
\]</div>
<p>其中每行对应一个样本，每列对应一个特征。
<img alt="" src="10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/IMG-10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-20250820160023048.png" /></p>
<p>批归一化对每一列（即每个特征）在 minibatch 上计算均值和方差，并进行归一化：</p>
<div class="arithmatex">\[
(\hat{z}_{i+1})_j = \frac{(z_{i+1})_j - \mu_j}{\sqrt{\sigma_j^2 + \varepsilon}}, \quad \text{其中 } \mu_j = \frac{1}{m}\sum_{k=1}^m (z_{i+1}^{(k)})_j, \quad \sigma_j^2 = \frac{1}{m}\sum_{k=1}^m ((z_{i+1}^{(k)})_j - \mu_j)^2
\]</div>
<ul>
<li>m为每个batch的样本数
同样可加入可学习参数 <span class="arithmatex">\(\gamma, \beta\)</span>。</li>
</ul>
<hr />
<h4 id="934-minibatch-dependence">9.3.4 小批量依赖性（Minibatch Dependence）<a class="headerlink" href="#934-minibatch-dependence" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>问题</strong>：BatchNorm 使得每个样本的输出依赖于整个 minibatch，这在测试(推理)时不可接受, 因为batch size可能为1</li>
<li><strong>解决方案</strong>：<ul>
<li>在训练时维护每个特征的<strong>运行平均（running average）</strong> 的均值 <span class="arithmatex">\(\hat{\mu}_{i+1}\)</span> 和方差 <span class="arithmatex">\(\hat{\sigma}^2_{i+1}\)</span>。</li>
<li>测试时使用这些统计量进行归一化：</li>
</ul>
</li>
</ul>
<div class="arithmatex">\[
(z_{i+1})_j = \frac{(\hat{z}_{i+1})_j - (\hat{\mu}_{i+1})_j}{\sqrt{(\hat{\sigma}^2_{i+1})_j + \varepsilon}}
\]</div>
<hr />
<h3 id="94-regularization">9.4 正则化（Regularization）<a class="headerlink" href="#94-regularization" title="Permanent link">&para;</a></h3>
<h4 id="_36">背景<a class="headerlink" href="#_36" title="Permanent link">&para;</a></h4>
<ul>
<li>深度网络通常是<strong>过参数化（overparameterized）</strong> 的：参数数量远超训练样本数。</li>
<li>传统机器学习理论认为这会导致严重<strong>过拟合overfit</strong>。</li>
<li>然而实践中，许多深度网络<strong>泛化良好</strong>，但并非总是如此（大模型仍可能过拟合）。</li>
</ul>
<h4 id="941">9.4.1 正则化的定义与分类<a class="headerlink" href="#941" title="Permanent link">&para;</a></h4>
<p><strong>正则化</strong>：限制函数类的复杂度，以提升泛化能力。</p>
<p>分为两类：</p>
<ol>
<li>
<p><strong>隐式正则化（Implicit Regularization）</strong>：</p>
<ul>
<li>来自优化算法（如 SGD）或网络结构本身。</li>
<li>例如：我们并非在“所有神经网络”空间中搜索，而是在 SGD + 初始化所能到达的子空间中搜索。</li>
</ul>
</li>
<li>
<p><strong>显式正则化（Explicit Regularization）</strong>：</p>
<ul>
<li>显式修改网络或训练过程以限制复杂度。</li>
<li>例如：<span class="arithmatex">\(\ell_2\)</span> 正则化、out。</li>
</ul>
</li>
</ol>
<hr />
<h4 id="942-ell_2-weight-decay">9.4.2 <span class="arithmatex">\(\ell_2\)</span> 正则化（Weight Decay）<a class="headerlink" href="#942-ell_2-weight-decay" title="Permanent link">&para;</a></h4>
<p>使用 <span class="arithmatex">\(\ell_2\)</span> 正则化的目标函数：</p>
<div class="arithmatex">\[
\min_{W_{1:L}} \frac{1}{m} \sum_{i=1}^m \ell(h_{W_{1:L}}(x_i), y_i) + \frac{\lambda}{2} \sum_{i=1}^L \|W_i\|_F^2
\]</div>
<ul>
<li>F-范数: <span class="arithmatex">\(||W||_{F} = \sqrt{ \sum_{i=1}^m\sum_{j=1}^nw_{ij}^2 }\)</span>, 类似向量的L2范数
梯度下降更新规则：</li>
</ul>
<div class="arithmatex">\[
W_i \leftarrow W_i - \alpha \nabla_{W_i} \ell - \alpha \lambda W_i = (1 - \alpha \lambda) W_i - \alpha \nabla_{W_i} \ell
\]</div>
<p>即：每步先将权重乘以 <span class="arithmatex">\((1 - \alpha \lambda)\)</span>（衰减），再进行梯度更新。</p>
<h4 id="943-ell_2">9.4.3 <span class="arithmatex">\(\ell_2\)</span> 正则化的局限性<a class="headerlink" href="#943-ell_2" title="Permanent link">&para;</a></h4>
<ul>
<li>尽管 <span class="arithmatex">\(\ell_2\)</span> 正则化在深度学习中广泛使用（常称为“weight decay”），但其有效性存疑。</li>
<li><strong>问题</strong>：在深度网络中，<strong>参数大小可能不是复杂度的良好代理</strong>。</li>
<li>如前所示，不同初始化导致的参数尺度差异在整个训练中持续存在，而 <span class="arithmatex">\(\ell_2\)</span> 正则化可能无法有效控制真正的“复杂度”。</li>
</ul>
<hr />
<h4 id="944-dropout">9.4.4 Dropout<a class="headerlink" href="#944-dropout" title="Permanent link">&para;</a></h4>
<p><strong>Dropout方法</strong>：在训练时，以概率 <span class="arithmatex">\(p\)</span> 随机将某些激活值置零。</p>
<p>设：</p>
<div class="arithmatex">\[
\hat{z}_{i+1} = \sigma_i(W_i^T z_i + b_i)
\]</div>
<p>Dropout 操作：</p>
<div class="arithmatex">\[
z_{i+1,j} =
\begin{cases}
\frac{\hat{z}_{i+1,j}}{1 - p}, &amp; \text{以概率 } 1 - p \\
0, &amp; \text{以概率 } p
\end{cases}
\]</div>
<blockquote>
<p><strong>注意</strong>：训练时除以 <span class="arithmatex">\(1-p\)</span> 以保持期望值不变；测试时<strong>不使用 Dropout</strong>，直接使用原始激活值。</p>
</blockquote>
<p>直观解释：</p>
<ul>
<li>常被解释为“增强鲁棒性”或“防止共适应”。</li>
<li>但为何有效？为何测试时不使用？</li>
</ul>
<h4 id="945-dropout">9.4.5 Dropout 作为随机近似<a class="headerlink" href="#945-dropout" title="Permanent link">&para;</a></h4>
<p>Dropout 可类比于 SGD 的随机性：</p>
<ul>
<li>SGD：从完整损失中采样 minibatch 近似梯度：
  $$
  \frac{1}{m}\sum_{i=1}^m \ell(h(x_i), y_i) \Rightarrow \frac{1}{|B|}\sum_{i \in B} \ell(h(x_i), y_i)
  $$</li>
</ul>
<ul>
<li>Dropout：对输入特征进行随机子集采样：
  $$
  z_{i+1} = \sigma_i\left( \sum_{j=1}^n W_{:,j} z_i^{(j)} \right) \Rightarrow z_{i+1} = \sigma_i\left( \sum_{j \in P} \frac{W_{:,j} z_i^{(j)}}{1 - p} \right)
  $$
  其中 <span class="arithmatex">\(P\)</span> 是以概率 <span class="arithmatex">\(1-p\)</span> 采样的特征子集。</li>
</ul>
<blockquote>
<p><strong>类比</strong>：Dropout 在特征空间引入随机性，类似 SGD 在数据空间引入随机性。</p>
</blockquote>
<hr />
<h3 id="95">9.5 各因素的交互作用<a class="headerlink" href="#95" title="Permanent link">&para;</a></h3>
<p>设计选择众多：</p>
<ul>
<li>优化器选择（学习率、动量等）</li>
<li>权重初始化</li>
<li>归一化层（BatchNorm, LayerNorm 等）</li>
<li>正则化方法（Dropout, weight decay）</li>
<li>其他技巧：残差连接、学习率调度等</li>
</ul>
<blockquote>
<p><strong>感受</strong>：深度学习实践看似像是“在大量 GPU 上随机试错”。</p>
</blockquote>
<h4 id="batchnorm-ali-rahimi">BatchNorm 的反思（Ali Rahimi 演讲引用）<a class="headerlink" href="#batchnorm-ali-rahimi" title="Permanent link">&para;</a></h4>
<blockquote>
<p>“我们对 BatchNorm 的理解是：它有效，因为它减少了‘内部协变量偏移（internal covariate shift）’。<br />
但你难道不想知道：  </p>
<ul>
<li>为什么减少内部协变量偏移能加速梯度下降？  </li>
<li>有没有定理或实验证据？  </li>
<li>什么是内部协变量偏移？  </li>
<li>有没有明确定义？”<br />
—— Ali Rahimi（NeurIPS 2017 时间检验奖演讲）</li>
</ul>
</blockquote>
<p><strong>启示</strong>：当前许多深度学习技术仍缺乏坚实的理论基础，更多依赖经验。</p>
<hr />
<h4 id="batchnorm">BatchNorm 的其他观察<a class="headerlink" href="#batchnorm" title="Permanent link">&para;</a></h4>
<p><img alt="" src="10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/IMG-10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-20250820171345792.png" /></p>
<ul>
<li><strong>反常现象</strong>：在<strong>测试时</strong>使用当前 minibatch 的统计量（即“运行 BatchNorm”），反而能提升模型在<strong>分布外（out-of-distribution）数据</strong>上的性能。</li>
<li>这与我们“应使用运行平均”的常规做法相悖，说明 BatchNorm 的机制仍不完全清楚。</li>
</ul>
<h3 id="98">9.8 总结与启示<a class="headerlink" href="#98" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>不要误解</strong>：深度学习并非全是“随机黑科技”，已有大量优秀的实验研究。</li>
<li><strong>现实</strong>：我们尚未完全理解各种经验技巧（如 BatchNorm、Dropout、初始化等）<strong>如何工作以及如何相互作用</strong>。</li>
<li><strong>好消息</strong>：在许多情况下，<strong>不同的架构和方法组合往往能达到相似的良好性能</strong>。</li>
<li>因此，设计模型时可尝试多种路径，不必拘泥于单一“最佳实践”。</li>
</ol>
<h2 id="lec10-convolutional-networks">Lec10 卷积网络 (Convolutional Networks)<a class="headerlink" href="#lec10-convolutional-networks" title="Permanent link">&para;</a></h2>
<h3 id="101-convolutional-operators-in-deep-networks">10.1 深度网络中的卷积算子 (Convolutional operators in deep networks)<a class="headerlink" href="#101-convolutional-operators-in-deep-networks" title="Permanent link">&para;</a></h3>
<h4 id="fully-connected-networks">全连接网络 (fully connected networks) 的问题<a class="headerlink" href="#fully-connected-networks" title="Permanent link">&para;</a></h4>
<p>到目前为止，我们考虑的网络都是将输入图像视为<strong>向量</strong></p>
<p>这在我们试图处理更大图像时会产生一个严重的问题：</p>
<ul>
<li>一个 <span class="arithmatex">\(256 \times 256\)</span> 的 RGB 图像是约 20 万维的输入</li>
<li>将其映射到一个 1000 维的隐藏向量需要 2 亿个参数 (仅单层)</li>
</ul>
<p>此外，这种方式没有捕捉到我们期望在图像中存在的任何“直观”的不变性 (invariances)</p>
<ul>
<li>例如，将图像平移一个像素会导致下一层输出完全不同</li>
</ul>
<p><img alt="" src="10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/IMG-10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-20250825161453768.png" /></p>
<h4 id="_37">卷积如何“简化”深度网络<a class="headerlink" href="#_37" title="Permanent link">&para;</a></h4>
<p>卷积结合了两种非常适合处理图像的思想</p>
<ol>
<li>要求层与层之间的激活仅在“局部” (<code>local</code>) 发生，并将隐藏层本身也视为空间图像</li>
<li>在所有空间位置上共享权重 (<code>share weights</code>)</li>
</ol>
<p><img alt="" src="10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/IMG-10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-20250825161628783.png" /></p>
<h4 id="_38">卷积的优势<a class="headerlink" href="#_38" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>大幅减少参数数量</strong><ul>
<li>一个 <span class="arithmatex">\(256 \times 256\)</span> 的灰度图输入到一个 <span class="arithmatex">\(256 \times 256\)</span> 的单通道隐藏层：<ul>
<li>在全连接网络中需要 40 亿个参数</li>
<li>在一个 <span class="arithmatex">\(3 \times 3\)</span> 的卷积中只需要 9 个参数</li>
</ul>
</li>
</ul>
</li>
<li><strong>捕捉（部分）“自然”的不变性</strong><ul>
<li>将输入图像向右平移一个像素，会使隐藏单元的“图像”也相应平移</li>
</ul>
</li>
</ul>
<p><img alt="" src="10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/IMG-10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-20250825161628783.png" /></p>
<h4 id="_39">卷积详解<a class="headerlink" href="#_39" title="Permanent link">&para;</a></h4>
<p>卷积是许多计算机视觉和图像处理算法中的一个基本操作
其思想是“滑动”一个 <span class="arithmatex">\(k \times k\)</span> 的权重矩阵 <span class="arithmatex">\(w\)</span> (称为 <code>filter</code> 或 <code>kernel</code>) 在图像上，以生成一个新的图像，记为 <span class="arithmatex">\(y = z * w\)</span></p>
<p><img alt="一个 5x5 的输入 z 和一个 3x3 的 filter w，生成一个 3x3 的输出 y" src="10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/IMG-10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-20250825163634045.png" /></p>
<p><strong>计算过程示例</strong>:</p>
<ol>
<li>
<p>计算输出 <span class="arithmatex">\(y_{11}\)</span>:
    <img alt="此处应插入 PPT 第 8 页关于 $y_{11}$ 计算的图示" src="10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/IMG-10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-20250825163728578.png" />
    <span class="arithmatex">\(y_{11} = z_{11}w_{11} + z_{12}w_{12} + z_{13}w_{13} + z_{21}w_{21} + \dots\)</span></p>
</li>
<li>
<p>计算输出 <span class="arithmatex">\(y_{12}\)</span>:  <span class="arithmatex">\(y_{12} = z_{12}w_{11} + z_{13}w_{12} + z_{14}w_{13} + z_{22}w_{21} + \dots\)</span></p>
</li>
<li>计算输出 <span class="arithmatex">\(y_{13}\)</span>:  <span class="arithmatex">\(y_{13} = z_{13}w_{11} + z_{14}w_{12} + z_{15}w_{13} + z_{23}w_{21} + \dots\)</span></li>
<li>计算输出 <span class="arithmatex">\(y_{21}\)</span>:  <span class="arithmatex">\(y_{21} = z_{21}w_{11} + z_{22}w_{12} + z_{23}w_{13} + z_{31}w_{21} + \dots\)</span></li>
<li>计算输出 <span class="arithmatex">\(y_{22}\)</span>:  <span class="arithmatex">\(y_{22} = z_{22}w_{11} + z_{23}w_{12} + z_{24}w_{13} + z_{32}w_{21} + \dots\)</span></li>
<li>计算输出 <span class="arithmatex">\(y_{23}\)</span>:  <span class="arithmatex">\(y_{23} = z_{23}w_{11} + z_{24}w_{12} + z_{25}w_{13} + z_{33}w_{21} + \dots\)</span></li>
</ol>
<h4 id="_40">图像处理中的卷积<a class="headerlink" href="#_40" title="Permanent link">&para;</a></h4>
<p>卷积（通常使用预设定的 <code>filter</code>）是许多计算机视觉应用中的常见操作；卷积网络只是转向了<strong>学习</strong>这些 <code>filter</code></p>
<p><img alt="此处应插入 PPT 第 14 页的图片：左-原图，中-高斯模糊，右-图像梯度" src="10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/IMG-10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-20250825163918744.png" /></p>
<h4 id="_41">深度网络中的卷积<a class="headerlink" href="#_41" title="Permanent link">&para;</a></h4>
<p>深度网络中的卷积几乎总是多通道卷积 (<code>multi-channel convolutions</code>)：将多通道（例如 RGB）输入映射到多通道隐藏单元</p>
<ul>
<li>输入 <span class="arithmatex">\(x \in \mathbb{R}^{h \times w \times c_{in}}\)</span> 表示一个 <span class="arithmatex">\(c_{in}\)</span> 通道，<span class="arithmatex">\(h \times w\)</span> 大小的图像</li>
<li>输出 <span class="arithmatex">\(z \in \mathbb{R}^{h \times w \times c_{out}}\)</span> 表示一个 <span class="arithmatex">\(c_{out}\)</span> 通道，<span class="arithmatex">\(h \times w\)</span> 大小的图像</li>
<li><code>Filter</code> <span class="arithmatex">\(W \in \mathbb{R}^{c_{in} \times c_{out} \times k \times k}\)</span> (一个 4 阶张量) k是卷积核大小</li>
</ul>
<p>多通道卷积为每个输入-输出通道对包含一个卷积 <code>filter</code>，单个输出通道是所有输入通道卷积结果的和</p>
<ul>
<li><span class="arithmatex">\(z[:,:,s] = \sum_{r=1}^{c_{in}} x[:,:,r] * W[r,s,:,:]\)</span></li>
</ul>
<p><img src="10-414_深度学习系统课程笔记.assets/IMG-10-414_深度学习系统课程笔记-20250825164809113.png" alt="此处应插入 PPT 第 15 页的图示，展示多通道输入 x 如何通过一个 4D 的 filter W 卷积生成多通道输出 z" style="zoom:50%;" /></p>
<h4 id="-">多通道卷积的矩阵-向量形式<a class="headerlink" href="#-" title="Permanent link">&para;</a></h4>
<p>有一种更直观的方式来理解多通道卷积：它们是传统卷积的推广，其中标量乘法被替换为矩阵-向量乘积
<img alt="" src="10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/IMG-10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-20250825165346020.png" /></p>
<ul>
<li>输入图像中的每个 <span class="arithmatex">\(x_{ij}\)</span> 都是 <span class="arithmatex">\(\mathbb{R}^{c_{in}}\)</span> 中的向量</li>
<li><code>Filter</code> 中的每个 <span class="arithmatex">\(W_{ij}\)</span> 都是 <span class="arithmatex">\(\mathbb{R}^{c_{out} \times c_{in}}\)</span> 的矩阵</li>
<li>计算 <span class="arithmatex">\(z_{22}\)</span> 的公式：<ul>
<li><span class="arithmatex">\(z_{22} = W_{11}x_{22} + W_{12}x_{23} + W_{13}x_{24} + W_{21}x_{32} + \dots\)</span></li>
<li>这里的乘法是矩阵-向量乘积</li>
</ul>
</li>
</ul>
<hr />
<h3 id="102-elements-of-practical-convolutions">10.2 实用卷积的要素 (Elements of practical convolutions)<a class="headerlink" href="#102-elements-of-practical-convolutions" title="Permanent link">&para;</a></h3>
<h4 id="padding">Padding<a class="headerlink" href="#padding" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>挑战</strong>: “朴素”的卷积产生的输出图像比输入图像小</li>
<li><strong>解决方案</strong>: 对于（奇数）大小为 k 的 <code>kernel</code>，在输入图像的所有边填充 <span class="arithmatex">\((k-1)/2\)</span> 个零，这使得输出与输入大小相同</li>
<li><strong>变体</strong>: <code>circular padding</code>，使用均值填充等
<img alt="" src="10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/IMG-10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-20250825165756784.png" /></li>
</ul>
<h4 id="strided-convolutions-pooling">步幅卷积 (Strided Convolutions) / 池化 (Pooling)<a class="headerlink" href="#strided-convolutions-pooling" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>挑战</strong>: 卷积在每一层都保持输入的分辨率，不能直接产生不同“分辨率”的表示</li>
<li><strong>解决方案 #1</strong>: 引入 <code>max</code> 或 <code>average pooling</code> 层来聚合信息</li>
<li><strong>解决方案 #2</strong>: 以大于 1 的增量（即 <code>stride</code>）在图像上滑动卷积 <code>filter</code>
<img alt="" src="10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/IMG-10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-20250825191402447.png" /></li>
</ul>
<h4 id="grouped-convolutions">分组卷积 (Grouped Convolutions)<a class="headerlink" href="#grouped-convolutions" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>挑战</strong>: 对于大量的输入/输出通道，<code>filter</code> 的权重数量仍然很大，可能导致过拟合 (<code>overfitting</code>) 和计算缓慢</li>
<li><strong>解决方案</strong>: 将通道分组，使得输出中的通道组仅依赖于输入中对应的通道组（等效于强制 <code>filter</code> 权重矩阵为块对角矩阵）
<img alt="" src="10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/IMG-10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-20250825192042204.png" /></li>
</ul>
<h4 id="dilations">扩张卷积 (Dilations)<a class="headerlink" href="#dilations" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>挑战</strong>: 单个卷积的感受野 (<code>receptive field</code>) 相对较小</li>
<li><strong>解决方案</strong>: 扩张 (<code>dilate</code> 或 <code>spread out</code>) 卷积 <code>filter</code>，使其覆盖图像的更大范围</li>
<li><strong>注意</strong>: 为了再次获得相同大小的图像，需要添加更多的 <code>padding</code>
<img alt="" src="10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/IMG-10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-20250825192300811.png" /></li>
</ul>
<hr />
<h3 id="103-differentiating-convolutions">10.3 卷积的微分 (Differentiating convolutions)<a class="headerlink" href="#103-differentiating-convolutions" title="Permanent link">&para;</a></h3>
<h4 id="_42">微分需要什么？<a class="headerlink" href="#_42" title="Permanent link">&para;</a></h4>
<p>回想一下，为了将任何操作集成到深度网络中，我们需要能够乘以其偏导数（伴随操作 <code>adjoint operation</code>）</p>
<p>如果我们定义操作为 <span class="arithmatex">\(z = conv(x, W)\)</span>
那么我们如何乘以其伴随算子？</p>
<div class="arithmatex">\[
\bar{v}\frac{\partial conv(x,W)}{\partial W}\ 和\ \bar{v}\frac{\partial conv(x,W)}{\partial x}
\]</div>
<h4 id="_43">回顾矩阵乘法的微分<a class="headerlink" href="#_43" title="Permanent link">&para;</a></h4>
<p>考虑一个更简单的情况：矩阵-向量乘积 <span class="arithmatex">\(z = Wx\)</span></p>
<ul>
<li><span class="arithmatex">\(\frac{\partial z}{\partial x} = W\)</span>，所以我们需要计算伴随积 <span class="arithmatex">\(\bar{v}^T W \iff W^T \bar{v}\)</span></li>
<li>换句话说，对于矩阵向量乘法操作 <code>Wx</code>，计算反向传播需要乘以转置矩阵 <span class="arithmatex">\(W^T\)</span></li>
</ul>
<p>那么，什么是“卷积的转置”呢？</p>
<h4 id="1_1">将卷积视为矩阵乘法：版本 1<a class="headerlink" href="#1_1" title="Permanent link">&para;</a></h4>
<p>为了回答这个问题，我们先考虑一个 1D 卷积以简化问题
我们可以将一个 1D 卷积 <span class="arithmatex">\(x * w\)</span> (例如，使用零填充) 写成矩阵乘法 <span class="arithmatex">\(\hat{W}x\)</span> 的形式</p>
<p><img alt="" src="10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/IMG-10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-20250825203218940.png" /></p>
<p><span class="arithmatex">\(\begin{bmatrix} z_1 \\ z_2 \\ z_3 \\ z_4 \\ z_5 \end{bmatrix} = x * w = \underbrace{\begin{bmatrix} w_2 &amp; w_3 &amp; 0 &amp; 0 &amp; 0 \\ w_1 &amp; w_2 &amp; w_3 &amp; 0 &amp; 0 \\ 0 &amp; w_1 &amp; w_2 &amp; w_3 &amp; 0 \\ 0 &amp; 0 &amp; w_1 &amp; w_2 &amp; w_3 \\ 0 &amp; 0 &amp; 0 &amp; w_1 &amp; w_2 \end{bmatrix}}_{\hat{W}} \begin{bmatrix} x_1 \\ x_2 \\ x_3 \\ x_4 \\ x_5 \end{bmatrix}\)</span></p>
<h4 id="adjoint">卷积的伴随算子 (Adjoint)<a class="headerlink" href="#adjoint" title="Permanent link">&para;</a></h4>
<p>那么我们如何乘以转置矩阵 <span class="arithmatex">\(\hat{W}^T\)</span> 呢？
<span class="arithmatex">\(\hat{W}^T = \begin{bmatrix} w_2 &amp; w_1 &amp; 0 &amp; 0 &amp; 0 \\ w_3 &amp; w_2 &amp; w_1 &amp; 0 &amp; 0 \\ 0 &amp; w_3 &amp; w_2 &amp; w_1 &amp; 0 \\ 0 &amp; 0 &amp; w_3 &amp; w_2 &amp; w_1 \\ 0 &amp; 0 &amp; 0 &amp; w_3 &amp; w_2 \end{bmatrix}\)</span></p>
<p>请注意，操作 <span class="arithmatex">\(\hat{W}^T v\)</span> 本身就是与一个“翻转”的 <code>filter</code> <span class="arithmatex">\([w_3, w_2, w_1]\)</span> 进行的卷积
因此，伴随算子 <span class="arithmatex">\(\bar{v} \frac{\partial conv(x, W)}{\partial x}\)</span> 只需要用翻转后的 <code>W</code> 对 <span class="arithmatex">\(\bar{v}\)</span> 进行卷积</p>
<h4 id="2_2">将卷积视为矩阵乘法：版本 2<a class="headerlink" href="#2_2" title="Permanent link">&para;</a></h4>
<p>那么另一个伴随算子 <span class="arithmatex">\(\bar{v} \frac{\partial conv(x,W)}{\partial W}\)</span> 呢？
对于这一项，我们可以观察到卷积也可以写成一个矩阵-向量乘积，其中 <code>filter</code> 被视为向量</p>
<p><span class="arithmatex">\(\begin{bmatrix} z_1 \\ z_2 \\ z_3 \\ z_4 \\ z_5 \end{bmatrix} = x * w = \begin{bmatrix} 0 &amp; x_1 &amp; x_2 \\ x_1 &amp; x_2 &amp; x_3 \\ x_2 &amp; x_3 &amp; x_4 \\ x_3 &amp; x_4 &amp; x_5 \\ x_4 &amp; x_5 &amp; 0 \end{bmatrix} \begin{bmatrix} w_1 \\ w_2 \\ w_3 \end{bmatrix}\)</span></p>
<p>因此，其伴随操作需要乘以这个<strong>基于 <code>x</code> 构建的矩阵</strong>的转置（这实际上是一种相当实用的方法，参见未来关于 <code>im2col</code> 操作的讲座）</p>
<h2 id="lec11">Lec11 硬件加速<a class="headerlink" href="#lec11" title="Permanent link">&para;</a></h2>
<h3 id="111">11.1 通用加速技术<a class="headerlink" href="#111" title="Permanent link">&para;</a></h3>
<h5 id="_44">机器学习框架中的层次<a class="headerlink" href="#_44" title="Permanent link">&para;</a></h5>
<p><img alt="" src="10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/IMG-10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-20250825214949352.png" /></p>
<ul>
<li>最上层是机器学习模型 (<code>ML Models</code>)</li>
<li>模型接收输入 <code>x</code> (例如一张图片)</li>
<li>模型内部由一个计算图 (<code>Computational graph</code>) 表示</li>
<li>计算图的节点是张量 (<code>Tensor</code>) 和线性代数库 (<code>Tensor linear algebra libraries</code>) 定义的操作</li>
<li>最终输出结果 <span class="arithmatex">\(h_{\theta}(x)\)</span></li>
<li>这些计算可以运行在各种硬件上，例如：<ul>
<li>Intel Xeon Processor (CPU)</li>
<li>GPU</li>
<li>手机</li>
<li>Raspberry Pi</li>
<li>服务器集群</li>
</ul>
</li>
</ul>
<h5 id="vectorization">Vectorization (向量化)<a class="headerlink" href="#vectorization" title="Permanent link">&para;</a></h5>
<p>向量化是一种利用 <code>SIMD</code> (Single Instruction, Multiple Data) 指令并行处理多个数据元素的技术</p>
<p><strong>例子：两个长度为256的数组相加</strong></p>
<div class="language-c highlight"><pre><span></span><code><span id="__span-5-1"><a id="__codelineno-5-1" name="__codelineno-5-1" href="#__codelineno-5-1"></a><span class="c1">// C 语言示例(伪)代码</span>
</span><span id="__span-5-2"><a id="__codelineno-5-2" name="__codelineno-5-2" href="#__codelineno-5-2"></a><span class="kt">void</span><span class="w"> </span><span class="nf">vecadd</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">B</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">C</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-5-3"><a id="__codelineno-5-3" name="__codelineno-5-3" href="#__codelineno-5-3"></a><span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">&lt;</span><span class="mi">64</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-5-4"><a id="__codelineno-5-4" name="__codelineno-5-4" href="#__codelineno-5-4"></a><span class="w">        </span><span class="c1">// load_float4, add_float4, store_float4 都是向量化指令的抽象</span>
</span><span id="__span-5-5"><a id="__codelineno-5-5" name="__codelineno-5-5" href="#__codelineno-5-5"></a><span class="w">        </span><span class="c1">// 一次处理 4 个浮点数(4个byte)</span>
</span><span id="__span-5-6"><a id="__codelineno-5-6" name="__codelineno-5-6" href="#__codelineno-5-6"></a><span class="w">        </span><span class="n">float4</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">load_float4</span><span class="p">(</span><span class="n">A</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">i</span><span class="o">*</span><span class="mi">4</span><span class="p">);</span>
</span><span id="__span-5-7"><a id="__codelineno-5-7" name="__codelineno-5-7" href="#__codelineno-5-7"></a><span class="w">        </span><span class="n">float4</span><span class="w"> </span><span class="n">b</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">load_float4</span><span class="p">(</span><span class="n">B</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">i</span><span class="o">*</span><span class="mi">4</span><span class="p">);</span>
</span><span id="__span-5-8"><a id="__codelineno-5-8" name="__codelineno-5-8" href="#__codelineno-5-8"></a><span class="w">        </span><span class="n">float4</span><span class="w"> </span><span class="n">c</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">add_float4</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="n">b</span><span class="p">);</span>
</span><span id="__span-5-9"><a id="__codelineno-5-9" name="__codelineno-5-9" href="#__codelineno-5-9"></a><span class="w">        </span><span class="n">store_float4</span><span class="p">(</span><span class="n">C</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">i</span><span class="o">*</span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="n">c</span><span class="p">);</span>
</span><span id="__span-5-10"><a id="__codelineno-5-10" name="__codelineno-5-10" href="#__codelineno-5-10"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-5-11"><a id="__codelineno-5-11" name="__codelineno-5-11" href="#__codelineno-5-11"></a><span class="p">}</span>
</span></code></pre></div>
<ul>
<li><strong>额外要求</strong>: 内存 <code>(A, B, C)</code> 需要是128位对齐的</li>
</ul>
<blockquote>
<p>内存对齐: 内存起始地址必须为32的倍数, 这样可以避免数据跨越内存总线宽度 / cache line, 减少访存次数&amp;防止缓存污染</p>
</blockquote>
<h5 id="data-layout-strides">数据布局 (Data layout) 和步幅 (strides)<a class="headerlink" href="#data-layout-strides" title="Permanent link">&para;</a></h5>
<p><strong>问题：如何在内存中存储一个矩阵</strong></p>
<ul>
<li><strong>行主序 (Row major)</strong>: <span class="arithmatex">\(A[i,j] \Rightarrow Adata[i * A.shape[1] + j]\)</span></li>
<li><strong>列主序 (Column major)</strong>: <span class="arithmatex">\(A[i,j] \Rightarrow Adata[j * A.shape[0] + i]\)</span></li>
<li><strong>步幅格式 (Strides format)</strong>: <span class="arithmatex">\(A[i,j] \Rightarrow Adata[i * A.strides[0] + j * A.strides[1]]\)</span></li>
</ul>
<h5 id="strides">关于 Strides 的讨论<a class="headerlink" href="#strides" title="Permanent link">&para;</a></h5>
<ul>
<li><strong>优点</strong>: 可以用零拷贝 (<code>zero copy</code>) 的方式实现某些变换/切片<ul>
<li><strong>切片 (Slice)</strong>: 改变起始偏移量和形状 (<code>shape</code>)</li>
<li><strong>转置 (Transpose)</strong>: 交换 (<code>swap</code>) 步幅</li>
<li><strong>广播 (Broadcast)</strong>: 插入一个等于 0 的步幅</li>
</ul>
</li>
<li><strong>缺点</strong>: 内存访问可能变得不连续,  因为执行计算时操作的是底层的原始数组<ul>
<li>这使得向量化 (<code>vectorization</code>) 更加困难</li>
<li>许多线性代数操作可能需要先将数组紧凑化 (<code>compact</code>):
    - 需要在执行计算前检查是否compact, 如果不是, 进行compact处理, 根据stride创建一个数组
    - 或者直接实现一个迭代器根据stride进行计算的版本</li>
</ul>
</li>
</ul>
<h5 id="parallelization">Parallelization (并行化)<a class="headerlink" href="#parallelization" title="Permanent link">&para;</a></h5>
<p>利用多线程并行执行计算</p>
<div class="language-c highlight"><pre><span></span><code><span id="__span-6-1"><a id="__codelineno-6-1" name="__codelineno-6-1" href="#__codelineno-6-1"></a><span class="c1">// 使用 OpenMP 的并行化 vecadd 示例</span>
</span><span id="__span-6-2"><a id="__codelineno-6-2" name="__codelineno-6-2" href="#__codelineno-6-2"></a><span class="kt">void</span><span class="w"> </span><span class="nf">vecadd</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">B</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">C</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-6-3"><a id="__codelineno-6-3" name="__codelineno-6-3" href="#__codelineno-6-3"></a><span class="w">    </span><span class="c1">// #pragma omp parallel for 指令告诉编译器用多个线程并行执行这个 for 循环</span>
</span><span id="__span-6-4"><a id="__codelineno-6-4" name="__codelineno-6-4" href="#__codelineno-6-4"></a><span class="w">    </span><span class="cp">#pragma omp parallel for</span>
</span><span id="__span-6-5"><a id="__codelineno-6-5" name="__codelineno-6-5" href="#__codelineno-6-5"></a><span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">&lt;</span><span class="mi">64</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-6-6"><a id="__codelineno-6-6" name="__codelineno-6-6" href="#__codelineno-6-6"></a><span class="w">        </span><span class="n">float4</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">load_float4</span><span class="p">(</span><span class="n">A</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">i</span><span class="o">*</span><span class="mi">4</span><span class="p">);</span>
</span><span id="__span-6-7"><a id="__codelineno-6-7" name="__codelineno-6-7" href="#__codelineno-6-7"></a><span class="w">        </span><span class="n">float4</span><span class="w"> </span><span class="n">b</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">load_float4</span><span class="p">(</span><span class="n">B</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">i</span><span class="o">*</span><span class="mi">4</span><span class="p">);</span>
</span><span id="__span-6-8"><a id="__codelineno-6-8" name="__codelineno-6-8" href="#__codelineno-6-8"></a><span class="w">        </span><span class="n">float4</span><span class="w"> </span><span class="n">c</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">add_float4</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="n">b</span><span class="p">);</span>
</span><span id="__span-6-9"><a id="__codelineno-6-9" name="__codelineno-6-9" href="#__codelineno-6-9"></a><span class="w">        </span><span class="n">store_float4</span><span class="p">(</span><span class="n">C</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">i</span><span class="o">*</span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="n">c</span><span class="p">);</span>
</span><span id="__span-6-10"><a id="__codelineno-6-10" name="__codelineno-6-10" href="#__codelineno-6-10"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-6-11"><a id="__codelineno-6-11" name="__codelineno-6-11" href="#__codelineno-6-11"></a><span class="p">}</span>
</span></code></pre></div>
<ul>
<li>该指令会在多个线程上执行计算</li>
</ul>
<h3 id="112">11.2 案例学习：矩阵乘法<a class="headerlink" href="#112" title="Permanent link">&para;</a></h3>
<h5 id="vanilla-matrix-multiplication">朴素矩阵乘法 (Vanilla matrix multiplication)<a class="headerlink" href="#vanilla-matrix-multiplication" title="Permanent link">&para;</a></h5>
<p>计算 <span class="arithmatex">\(C = dot(A, B.T)\)</span></p>
<div class="language-c highlight"><pre><span></span><code><span id="__span-7-1"><a id="__codelineno-7-1" name="__codelineno-7-1" href="#__codelineno-7-1"></a><span class="kt">float</span><span class="w"> </span><span class="n">A</span><span class="p">[</span><span class="n">n</span><span class="p">][</span><span class="n">n</span><span class="p">],</span><span class="w"> </span><span class="n">B</span><span class="p">[</span><span class="n">n</span><span class="p">][</span><span class="n">n</span><span class="p">],</span><span class="w"> </span><span class="n">C</span><span class="p">[</span><span class="n">n</span><span class="p">][</span><span class="n">n</span><span class="p">];</span>
</span><span id="__span-7-2"><a id="__codelineno-7-2" name="__codelineno-7-2" href="#__codelineno-7-2"></a>
</span><span id="__span-7-3"><a id="__codelineno-7-3" name="__codelineno-7-3" href="#__codelineno-7-3"></a><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">&lt;</span><span class="n">n</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">i</span><span class="p">)</span>
</span><span id="__span-7-4"><a id="__codelineno-7-4" name="__codelineno-7-4" href="#__codelineno-7-4"></a><span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">j</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="o">&lt;</span><span class="n">n</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">j</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-7-5"><a id="__codelineno-7-5" name="__codelineno-7-5" href="#__codelineno-7-5"></a><span class="w">        </span><span class="n">C</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
</span><span id="__span-7-6"><a id="__codelineno-7-6" name="__codelineno-7-6" href="#__codelineno-7-6"></a><span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">k</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">n</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">k</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-7-7"><a id="__codelineno-7-7" name="__codelineno-7-7" href="#__codelineno-7-7"></a><span class="w">            </span><span class="n">C</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">k</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">B</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">];</span>
</span><span id="__span-7-8"><a id="__codelineno-7-8" name="__codelineno-7-8" href="#__codelineno-7-8"></a><span class="w">        </span><span class="p">}</span>
</span><span id="__span-7-9"><a id="__codelineno-7-9" name="__codelineno-7-9" href="#__codelineno-7-9"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-7-10"><a id="__codelineno-7-10" name="__codelineno-7-10" href="#__codelineno-7-10"></a><span class="p">}</span>
</span></code></pre></div>
<ul>
<li>时间复杂度为 <span class="arithmatex">\(O(n^3)\)</span></li>
</ul>
<h5 id="cpu-memory-hierarchy">现代 CPU 的内存层次结构 (Memory hierarchy)<a class="headerlink" href="#cpu-memory-hierarchy" title="Permanent link">&para;</a></h5>
<table>
<thead>
<tr>
<th>内存层级</th>
<th>延迟 (Latency)</th>
<th>相对延迟</th>
</tr>
</thead>
<tbody>
<tr>
<td>Registers</td>
<td></td>
<td></td>
</tr>
<tr>
<td>L1 Cache</td>
<td>0.5 ns</td>
<td></td>
</tr>
<tr>
<td>L2 Cache</td>
<td>7ns</td>
<td>14x L1 cache</td>
</tr>
<tr>
<td>DRAM</td>
<td>200ns</td>
<td>20x L2 cache, 200x L1 cache</td>
</tr>
</tbody>
</table>
<blockquote>
<p>(数据来源: Latency numbers every programmer should know)</p>
</blockquote>
<h5 id="architecture-aware-analysis">体系结构感知分析 (Architecture aware analysis)<a class="headerlink" href="#architecture-aware-analysis" title="Permanent link">&para;</a></h5>
<p>分析朴素矩阵乘法中的内存访问开销</p>
<div class="language-c highlight"><pre><span></span><code><span id="__span-8-1"><a id="__codelineno-8-1" name="__codelineno-8-1" href="#__codelineno-8-1"></a><span class="n">dram</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">A</span><span class="p">[</span><span class="n">n</span><span class="p">][</span><span class="n">n</span><span class="p">],</span><span class="w"> </span><span class="n">B</span><span class="p">[</span><span class="n">n</span><span class="p">][</span><span class="n">n</span><span class="p">],</span><span class="w"> </span><span class="n">C</span><span class="p">[</span><span class="n">n</span><span class="p">][</span><span class="n">n</span><span class="p">];</span>
</span><span id="__span-8-2"><a id="__codelineno-8-2" name="__codelineno-8-2" href="#__codelineno-8-2"></a>
</span><span id="__span-8-3"><a id="__codelineno-8-3" name="__codelineno-8-3" href="#__codelineno-8-3"></a><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">&lt;</span><span class="n">n</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-8-4"><a id="__codelineno-8-4" name="__codelineno-8-4" href="#__codelineno-8-4"></a><span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">j</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="o">&lt;</span><span class="n">n</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">j</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-8-5"><a id="__codelineno-8-5" name="__codelineno-8-5" href="#__codelineno-8-5"></a><span class="w">        </span><span class="k">register</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">c</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
</span><span id="__span-8-6"><a id="__codelineno-8-6" name="__codelineno-8-6" href="#__codelineno-8-6"></a><span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">k</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">k</span><span class="o">&lt;</span><span class="n">n</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">k</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-8-7"><a id="__codelineno-8-7" name="__codelineno-8-7" href="#__codelineno-8-7"></a><span class="w">            </span><span class="k">register</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">k</span><span class="p">];</span>
</span><span id="__span-8-8"><a id="__codelineno-8-8" name="__codelineno-8-8" href="#__codelineno-8-8"></a><span class="w">            </span><span class="k">register</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">b</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">B</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">];</span>
</span><span id="__span-8-9"><a id="__codelineno-8-9" name="__codelineno-8-9" href="#__codelineno-8-9"></a><span class="w">            </span><span class="n">c</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">b</span><span class="p">;</span>
</span><span id="__span-8-10"><a id="__codelineno-8-10" name="__codelineno-8-10" href="#__codelineno-8-10"></a><span class="w">        </span><span class="p">}</span>
</span><span id="__span-8-11"><a id="__codelineno-8-11" name="__codelineno-8-11" href="#__codelineno-8-11"></a><span class="w">        </span><span class="n">C</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">c</span><span class="p">;</span>
</span><span id="__span-8-12"><a id="__codelineno-8-12" name="__codelineno-8-12" href="#__codelineno-8-12"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-8-13"><a id="__codelineno-8-13" name="__codelineno-8-13" href="#__codelineno-8-13"></a><span class="p">}</span>
</span></code></pre></div>
<ul>
<li><strong><code>dram-&gt;register</code> 的时间成本</strong>:<ul>
<li>对于 <span class="arithmatex">\(A\)</span> 是 <span class="arithmatex">\(n^3\)</span></li>
<li>对于 <span class="arithmatex">\(B\)</span> 是 <span class="arithmatex">\(n^3\)</span></li>
</ul>
</li>
<li><strong>加载总成本</strong>: <span class="arithmatex">\(2 * dramspeed * n^3\)</span></li>
<li><strong><code>register</code> 内存成本</strong>: 3 (用于 a, b, c)</li>
</ul>
<h5 id="register-tiled-matrix-multiplication">寄存器分块矩阵乘法 (Register tiled matrix multiplication)<a class="headerlink" href="#register-tiled-matrix-multiplication" title="Permanent link">&para;</a></h5>
<p><img alt="图：矩阵 A, B.T, C 被划分为小块。A 的一行块（v3个元素，v1行高）和 B.T 的一列块（v3个元素，v2列宽）相乘，得到 C 中的一个 v1 x v2 的块" src="10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/IMG-10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-20250825234741314.png" /></p>
<div class="language-c highlight"><pre><span></span><code><span id="__span-9-1"><a id="__codelineno-9-1" name="__codelineno-9-1" href="#__codelineno-9-1"></a><span class="c1">// v1, v2, v3 是 tiling (分块) 的大小</span>
</span><span id="__span-9-2"><a id="__codelineno-9-2" name="__codelineno-9-2" href="#__codelineno-9-2"></a><span class="c1">// 假设 A, B, C 矩阵都存储在 DRAM 中</span>
</span><span id="__span-9-3"><a id="__codelineno-9-3" name="__codelineno-9-3" href="#__codelineno-9-3"></a><span class="c1">// 数据布局被重排以匹配分块策略，例如 A 的维度是 (n/v1, n/v3, v1, v3)</span>
</span><span id="__span-9-4"><a id="__codelineno-9-4" name="__codelineno-9-4" href="#__codelineno-9-4"></a><span class="n">dram</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">A</span><span class="p">[</span><span class="n">n</span><span class="o">/</span><span class="n">v1</span><span class="p">][</span><span class="n">n</span><span class="o">/</span><span class="n">v3</span><span class="p">][</span><span class="n">v1</span><span class="p">][</span><span class="n">v3</span><span class="p">];</span>
</span><span id="__span-9-5"><a id="__codelineno-9-5" name="__codelineno-9-5" href="#__codelineno-9-5"></a><span class="n">dram</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">B</span><span class="p">[</span><span class="n">n</span><span class="o">/</span><span class="n">v2</span><span class="p">][</span><span class="n">n</span><span class="o">/</span><span class="n">v3</span><span class="p">][</span><span class="n">v2</span><span class="p">][</span><span class="n">v3</span><span class="p">];</span>
</span><span id="__span-9-6"><a id="__codelineno-9-6" name="__codelineno-9-6" href="#__codelineno-9-6"></a><span class="n">dram</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">C</span><span class="p">[</span><span class="n">n</span><span class="o">/</span><span class="n">v1</span><span class="p">][</span><span class="n">n</span><span class="o">/</span><span class="n">v2</span><span class="p">][</span><span class="n">v1</span><span class="p">][</span><span class="n">v2</span><span class="p">];</span>
</span><span id="__span-9-7"><a id="__codelineno-9-7" name="__codelineno-9-7" href="#__codelineno-9-7"></a>
</span><span id="__span-9-8"><a id="__codelineno-9-8" name="__codelineno-9-8" href="#__codelineno-9-8"></a><span class="c1">// 循环遍历输出矩阵 C 的行块 (block row)</span>
</span><span id="__span-9-9"><a id="__codelineno-9-9" name="__codelineno-9-9" href="#__codelineno-9-9"></a><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">&lt;</span><span class="n">n</span><span class="o">/</span><span class="n">v1</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-9-10"><a id="__codelineno-9-10" name="__codelineno-9-10" href="#__codelineno-9-10"></a><span class="w">    </span><span class="c1">// 循环遍历输出矩阵 C 的列块 (block column)</span>
</span><span id="__span-9-11"><a id="__codelineno-9-11" name="__codelineno-9-11" href="#__codelineno-9-11"></a><span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">j</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="o">&lt;</span><span class="n">n</span><span class="o">/</span><span class="n">v2</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">j</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-9-12"><a id="__codelineno-9-12" name="__codelineno-9-12" href="#__codelineno-9-12"></a><span class="w">        </span><span class="c1">// 在寄存器中分配一个 v1 x v2 的块，用于累加 C[i][j] 的结果</span>
</span><span id="__span-9-13"><a id="__codelineno-9-13" name="__codelineno-9-13" href="#__codelineno-9-13"></a><span class="w">        </span><span class="k">register</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">c</span><span class="p">[</span><span class="n">v1</span><span class="p">][</span><span class="n">v2</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="mi">0</span><span class="p">};</span>
</span><span id="__span-9-14"><a id="__codelineno-9-14" name="__codelineno-9-14" href="#__codelineno-9-14"></a>
</span><span id="__span-9-15"><a id="__codelineno-9-15" name="__codelineno-9-15" href="#__codelineno-9-15"></a><span class="w">        </span><span class="c1">// 沿着 k 维度循环，这是矩阵乘法的缩减维度 (reduction dimension)</span>
</span><span id="__span-9-16"><a id="__codelineno-9-16" name="__codelineno-9-16" href="#__codelineno-9-16"></a><span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">k</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">k</span><span class="o">&lt;</span><span class="n">n</span><span class="o">/</span><span class="n">v3</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">k</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-9-17"><a id="__codelineno-9-17" name="__codelineno-9-17" href="#__codelineno-9-17"></a><span class="w">            </span><span class="c1">// 从 DRAM 加载 A 的一个 v1 x v3 块到寄存器 a 中</span>
</span><span id="__span-9-18"><a id="__codelineno-9-18" name="__codelineno-9-18" href="#__codelineno-9-18"></a><span class="w">            </span><span class="k">register</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">a</span><span class="p">[</span><span class="n">v1</span><span class="p">][</span><span class="n">v3</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">k</span><span class="p">];</span>
</span><span id="__span-9-19"><a id="__codelineno-9-19" name="__codelineno-9-19" href="#__codelineno-9-19"></a><span class="w">            </span><span class="c1">// 从 DRAM 加载 B 的一个 v2 x v3 块到寄存器 b 中</span>
</span><span id="__span-9-20"><a id="__codelineno-9-20" name="__codelineno-9-20" href="#__codelineno-9-20"></a><span class="w">            </span><span class="k">register</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">b</span><span class="p">[</span><span class="n">v2</span><span class="p">][</span><span class="n">v3</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">B</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">];</span>
</span><span id="__span-9-21"><a id="__codelineno-9-21" name="__codelineno-9-21" href="#__codelineno-9-21"></a>
</span><span id="__span-9-22"><a id="__codelineno-9-22" name="__codelineno-9-22" href="#__codelineno-9-22"></a><span class="w">            </span><span class="c1">// 计算两个寄存器块的点积 (a * b.T) 并累加到寄存器 c 中</span>
</span><span id="__span-9-23"><a id="__codelineno-9-23" name="__codelineno-9-23" href="#__codelineno-9-23"></a><span class="w">            </span><span class="n">c</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="n">b</span><span class="p">.</span><span class="n">T</span><span class="p">);</span>
</span><span id="__span-9-24"><a id="__codelineno-9-24" name="__codelineno-9-24" href="#__codelineno-9-24"></a><span class="w">        </span><span class="p">}</span>
</span><span id="__span-9-25"><a id="__codelineno-9-25" name="__codelineno-9-25" href="#__codelineno-9-25"></a>
</span><span id="__span-9-26"><a id="__codelineno-9-26" name="__codelineno-9-26" href="#__codelineno-9-26"></a><span class="w">        </span><span class="c1">// 当k循环结束后，将寄存器c中累加完毕的结果块写回到 DRAM 中的 C[i][j]</span>
</span><span id="__span-9-27"><a id="__codelineno-9-27" name="__codelineno-9-27" href="#__codelineno-9-27"></a><span class="w">        </span><span class="n">C</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">c</span><span class="p">;</span>
</span><span id="__span-9-28"><a id="__codelineno-9-28" name="__codelineno-9-28" href="#__codelineno-9-28"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-9-29"><a id="__codelineno-9-29" name="__codelineno-9-29" href="#__codelineno-9-29"></a><span class="p">}</span>
</span></code></pre></div>
<ul>
<li><strong>分块后的成本分析</strong>:<ul>
<li><code>A</code> 的 <code>dram-&gt;register</code> 时间成本: <span class="arithmatex">\(n^3 / v2 = \frac{n}{v_{1}} * \frac{n}{v_{2}} * \frac{n}{v_{3}}*v_{1}*v_{3}\)</span></li>
<li><code>B</code> 的 <code>dram-&gt;register</code> 时间成本: <span class="arithmatex">\(n^3 / v1\)</span></li>
<li>总加载成本: <span class="arithmatex">\(dramspeed * (n^3/v2 + n^3/v1)\)</span></li>
<li><code>register</code> 内存成本: <span class="arithmatex">\(v1*v3 + v2*v3 + v1*v2\)</span></li>
</ul>
</li>
</ul>
<h5 id="cache-line-aware-tiling">缓存行感知分块 (Cache line aware tiling)<a class="headerlink" href="#cache-line-aware-tiling" title="Permanent link">&para;</a></h5>
<p><img alt="图：矩阵 A, B.T, C 被划分为更大的块（条带）。A 的一个 b1 x n 的行条带和 B.T 的一个 n x b2 的列条带相乘，得到 C 中的一个 b1 x b2 的块" src="10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/IMG-10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-20250825235556874.png" /></p>
<div class="language-c highlight"><pre><span></span><code><span id="__span-10-1"><a id="__codelineno-10-1" name="__codelineno-10-1" href="#__codelineno-10-1"></a><span class="c1">// b1, b2 是缓存分块的大小</span>
</span><span id="__span-10-2"><a id="__codelineno-10-2" name="__codelineno-10-2" href="#__codelineno-10-2"></a><span class="n">dram</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">A</span><span class="p">[</span><span class="n">n</span><span class="o">/</span><span class="n">b1</span><span class="p">][</span><span class="n">b1</span><span class="p">][</span><span class="n">n</span><span class="p">];</span>
</span><span id="__span-10-3"><a id="__codelineno-10-3" name="__codelineno-10-3" href="#__codelineno-10-3"></a><span class="n">dram</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">B</span><span class="p">[</span><span class="n">n</span><span class="o">/</span><span class="n">b2</span><span class="p">][</span><span class="n">b2</span><span class="p">][</span><span class="n">n</span><span class="p">];</span>
</span><span id="__span-10-4"><a id="__codelineno-10-4" name="__codelineno-10-4" href="#__codelineno-10-4"></a><span class="n">dram</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">C</span><span class="p">[</span><span class="n">n</span><span class="o">/</span><span class="n">b1</span><span class="p">][</span><span class="n">n</span><span class="o">/</span><span class="n">b2</span><span class="p">][</span><span class="n">b1</span><span class="p">][</span><span class="n">b2</span><span class="p">];</span>
</span><span id="__span-10-5"><a id="__codelineno-10-5" name="__codelineno-10-5" href="#__codelineno-10-5"></a>
</span><span id="__span-10-6"><a id="__codelineno-10-6" name="__codelineno-10-6" href="#__codelineno-10-6"></a><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">&lt;</span><span class="n">n</span><span class="o">/</span><span class="n">b1</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-10-7"><a id="__codelineno-10-7" name="__codelineno-10-7" href="#__codelineno-10-7"></a><span class="w">    </span><span class="n">l1cache</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">a</span><span class="p">[</span><span class="n">b1</span><span class="p">][</span><span class="n">n</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
</span><span id="__span-10-8"><a id="__codelineno-10-8" name="__codelineno-10-8" href="#__codelineno-10-8"></a><span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">j</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="o">&lt;</span><span class="n">n</span><span class="o">/</span><span class="n">b2</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">j</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-10-9"><a id="__codelineno-10-9" name="__codelineno-10-9" href="#__codelineno-10-9"></a><span class="w">        </span><span class="n">l1cache</span><span class="w"> </span><span class="n">b</span><span class="p">[</span><span class="n">b2</span><span class="p">][</span><span class="n">n</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">B</span><span class="p">[</span><span class="n">j</span><span class="p">];</span>
</span><span id="__span-10-10"><a id="__codelineno-10-10" name="__codelineno-10-10" href="#__codelineno-10-10"></a><span class="w">        </span><span class="c1">// 子过程，可以在这里应用寄存器分块</span>
</span><span id="__span-10-11"><a id="__codelineno-10-11" name="__codelineno-10-11" href="#__codelineno-10-11"></a><span class="w">        </span><span class="n">C</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="n">b</span><span class="p">.</span><span class="n">T</span><span class="p">);</span>
</span><span id="__span-10-12"><a id="__codelineno-10-12" name="__codelineno-10-12" href="#__codelineno-10-12"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-10-13"><a id="__codelineno-10-13" name="__codelineno-10-13" href="#__codelineno-10-13"></a><span class="p">}</span>
</span></code></pre></div>
<ul>
<li><strong>缓存分块成本分析</strong>:<ul>
<li><code>A</code> 的 <code>dram-&gt;l1</code> 时间成本: <span class="arithmatex">\(n^2\)</span></li>
<li><code>B</code> 的 <code>dram-&gt;l1</code> 时间成本: <span class="arithmatex">\(n^3 / b1\)</span></li>
</ul>
</li>
<li><strong>约束条件</strong>:<ul>
<li>加载到 L1 缓存的数据块大小不能超过 L1 缓存的容量: <span class="arithmatex">\(b1*n + b2*n &lt; \text{l1 cache size}\)</span></li>
<li>为了在 <code>dot</code> 子过程中继续应用寄存器分块: <span class="arithmatex">\(b1 \% v1 &lt;mark&gt; 0\)</span>, <span class="arithmatex">\(b2 \% v2 &lt;/mark&gt; 0\)</span></li>
</ul>
</li>
</ul>
<h5 id="_45">整合两种分块方法<a class="headerlink" href="#_45" title="Permanent link">&para;</a></h5>
<div class="language-c highlight"><pre><span></span><code><span id="__span-11-1"><a id="__codelineno-11-1" name="__codelineno-11-1" href="#__codelineno-11-1"></a><span class="c1">// 结合缓存分块和寄存器分块</span>
</span><span id="__span-11-2"><a id="__codelineno-11-2" name="__codelineno-11-2" href="#__codelineno-11-2"></a><span class="n">dram</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">A</span><span class="p">[</span><span class="n">n</span><span class="o">/</span><span class="n">b1</span><span class="p">][</span><span class="n">b1</span><span class="o">/</span><span class="n">v1</span><span class="p">][</span><span class="n">n</span><span class="p">][</span><span class="n">v1</span><span class="p">];</span>
</span><span id="__span-11-3"><a id="__codelineno-11-3" name="__codelineno-11-3" href="#__codelineno-11-3"></a><span class="n">dram</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">B</span><span class="p">[</span><span class="n">n</span><span class="o">/</span><span class="n">b2</span><span class="p">][</span><span class="n">b2</span><span class="o">/</span><span class="n">v2</span><span class="p">][</span><span class="n">n</span><span class="p">][</span><span class="n">v2</span><span class="p">];</span>
</span><span id="__span-11-4"><a id="__codelineno-11-4" name="__codelineno-11-4" href="#__codelineno-11-4"></a>
</span><span id="__span-11-5"><a id="__codelineno-11-5" name="__codelineno-11-5" href="#__codelineno-11-5"></a><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">&lt;</span><span class="n">n</span><span class="o">/</span><span class="n">b1</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-11-6"><a id="__codelineno-11-6" name="__codelineno-11-6" href="#__codelineno-11-6"></a><span class="w">    </span><span class="n">l1cache</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">a</span><span class="p">[</span><span class="n">b1</span><span class="o">/</span><span class="n">v1</span><span class="p">][</span><span class="n">n</span><span class="p">][</span><span class="n">v1</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
</span><span id="__span-11-7"><a id="__codelineno-11-7" name="__codelineno-11-7" href="#__codelineno-11-7"></a><span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">j</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="o">&lt;</span><span class="n">n</span><span class="o">/</span><span class="n">b2</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">j</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-11-8"><a id="__codelineno-11-8" name="__codelineno-11-8" href="#__codelineno-11-8"></a><span class="w">        </span><span class="n">l1cache</span><span class="w"> </span><span class="n">b</span><span class="p">[</span><span class="n">b2</span><span class="o">/</span><span class="n">v2</span><span class="p">][</span><span class="n">n</span><span class="p">][</span><span class="n">v2</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">B</span><span class="p">[</span><span class="n">j</span><span class="p">];</span>
</span><span id="__span-11-9"><a id="__codelineno-11-9" name="__codelineno-11-9" href="#__codelineno-11-9"></a><span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">x</span><span class="o">&lt;</span><span class="n">b1</span><span class="o">/</span><span class="n">v1</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">x</span><span class="p">)</span>
</span><span id="__span-11-10"><a id="__codelineno-11-10" name="__codelineno-11-10" href="#__codelineno-11-10"></a><span class="w">            </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">y</span><span class="o">&lt;</span><span class="n">b2</span><span class="o">/</span><span class="n">v2</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">y</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-11-11"><a id="__codelineno-11-11" name="__codelineno-11-11" href="#__codelineno-11-11"></a><span class="w">                </span><span class="k">register</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">c</span><span class="p">[</span><span class="n">v1</span><span class="p">][</span><span class="n">v2</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="mi">0</span><span class="p">};</span>
</span><span id="__span-11-12"><a id="__codelineno-11-12" name="__codelineno-11-12" href="#__codelineno-11-12"></a><span class="w">                </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">k</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">k</span><span class="o">&lt;</span><span class="n">n</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">k</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-11-13"><a id="__codelineno-11-13" name="__codelineno-11-13" href="#__codelineno-11-13"></a><span class="w">                    </span><span class="k">register</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">ar</span><span class="p">[</span><span class="n">v1</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">a</span><span class="p">[</span><span class="n">x</span><span class="p">][</span><span class="n">k</span><span class="p">][</span><span class="o">:</span><span class="p">];</span>
</span><span id="__span-11-14"><a id="__codelineno-11-14" name="__codelineno-11-14" href="#__codelineno-11-14"></a><span class="w">                    </span><span class="k">register</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">br</span><span class="p">[</span><span class="n">v2</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">b</span><span class="p">[</span><span class="n">y</span><span class="p">][</span><span class="n">k</span><span class="p">][</span><span class="o">:</span><span class="p">];</span>
</span><span id="__span-11-15"><a id="__codelineno-11-15" name="__codelineno-11-15" href="#__codelineno-11-15"></a><span class="w">                    </span><span class="n">c</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">dot</span><span class="p">(</span><span class="n">ar</span><span class="p">,</span><span class="w"> </span><span class="n">br</span><span class="p">.</span><span class="n">T</span><span class="p">);</span>
</span><span id="__span-11-16"><a id="__codelineno-11-16" name="__codelineno-11-16" href="#__codelineno-11-16"></a><span class="w">                </span><span class="p">}</span>
</span><span id="__span-11-17"><a id="__codelineno-11-17" name="__codelineno-11-17" href="#__codelineno-11-17"></a><span class="w">                </span><span class="c1">// 此处省略了将 c 写回 C 的代码</span>
</span><span id="__span-11-18"><a id="__codelineno-11-18" name="__codelineno-11-18" href="#__codelineno-11-18"></a><span class="w">            </span><span class="p">}</span>
</span><span id="__span-11-19"><a id="__codelineno-11-19" name="__codelineno-11-19" href="#__codelineno-11-19"></a><span class="w">        </span><span class="p">}</span>
</span><span id="__span-11-20"><a id="__codelineno-11-20" name="__codelineno-11-20" href="#__codelineno-11-20"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-11-21"><a id="__codelineno-11-21" name="__codelineno-11-21" href="#__codelineno-11-21"></a><span class="p">}</span>
</span></code></pre></div>
<ul>
<li><strong>总加载成本</strong>: <span class="arithmatex">\(l1speed * (n^3/v2 + n^3/v1) + dramspeed * (n^2 + n^3/b1)\)</span></li>
</ul>
<h5 id="memory-load-reuse">核心思想：内存加载复用 (Memory load reuse)<a class="headerlink" href="#memory-load-reuse" title="Permanent link">&para;</a></h5>
<p>回顾寄存器分块的代码：</p>
<div class="language-c highlight"><pre><span></span><code><span id="__span-12-1"><a id="__codelineno-12-1" name="__codelineno-12-1" href="#__codelineno-12-1"></a><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">&lt;</span><span class="n">n</span><span class="o">/</span><span class="n">v1</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-12-2"><a id="__codelineno-12-2" name="__codelineno-12-2" href="#__codelineno-12-2"></a><span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">j</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="o">&lt;</span><span class="n">n</span><span class="o">/</span><span class="n">v2</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">j</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-12-3"><a id="__codelineno-12-3" name="__codelineno-12-3" href="#__codelineno-12-3"></a><span class="w">        </span><span class="k">register</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">c</span><span class="p">[</span><span class="n">v1</span><span class="p">][</span><span class="n">v2</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="mi">0</span><span class="p">};</span>
</span><span id="__span-12-4"><a id="__codelineno-12-4" name="__codelineno-12-4" href="#__codelineno-12-4"></a><span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">k</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">k</span><span class="o">&lt;</span><span class="n">n</span><span class="o">/</span><span class="n">v3</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">k</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-12-5"><a id="__codelineno-12-5" name="__codelineno-12-5" href="#__codelineno-12-5"></a><span class="w">            </span><span class="k">register</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">a</span><span class="p">[</span><span class="n">v1</span><span class="p">][</span><span class="n">v3</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">k</span><span class="p">];</span>
</span><span id="__span-12-6"><a id="__codelineno-12-6" name="__codelineno-12-6" href="#__codelineno-12-6"></a><span class="w">            </span><span class="k">register</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">b</span><span class="p">[</span><span class="n">v2</span><span class="p">][</span><span class="n">v3</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">B</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">];</span>
</span><span id="__span-12-7"><a id="__codelineno-12-7" name="__codelineno-12-7" href="#__codelineno-12-7"></a><span class="w">            </span><span class="n">c</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="n">b</span><span class="p">.</span><span class="n">T</span><span class="p">);</span>
</span><span id="__span-12-8"><a id="__codelineno-12-8" name="__codelineno-12-8" href="#__codelineno-12-8"></a><span class="w">        </span><span class="p">}</span>
</span><span id="__span-12-9"><a id="__codelineno-12-9" name="__codelineno-12-9" href="#__codelineno-12-9"></a><span class="w">        </span><span class="n">C</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">c</span><span class="p">;</span>
</span><span id="__span-12-10"><a id="__codelineno-12-10" name="__codelineno-12-10" href="#__codelineno-12-10"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-12-11"><a id="__codelineno-12-11" name="__codelineno-12-11" href="#__codelineno-12-11"></a><span class="p">}</span>
</span></code></pre></div>
<ul>
<li>在内层的 <code>k</code> 循环中加载的 <code>a</code> 块，会在外层的 <code>j</code> 循环中被复用 <span class="arithmatex">\(v2\)</span> 次</li>
<li>同理，加载的 <code>b</code> 块，会在 <code>i</code> 循环中被复用 <span class="arithmatex">\(v1\)</span> 次</li>
<li>复用大大减少了从 <code>DRAM</code> 到 <code>register</code> 的总数据传输量<ul>
<li><code>A</code> 的 <code>dram-&gt;register</code> 成本从 <span class="arithmatex">\(n^3\)</span> 降至 <span class="arithmatex">\(n^3/v2\)</span></li>
<li><code>B</code> 的 <code>dram-&gt;register</code> 成本从 <span class="arithmatex">\(n^3\)</span> 降至 <span class="arithmatex">\(n^3/v1\)</span></li>
</ul>
</li>
</ul>
<h5 id="_46">常见的复用模式<a class="headerlink" href="#_46" title="Permanent link">&para;</a></h5>
<p>对于如下形式的计算：
<span class="arithmatex">\(C[i][j] = \text{sum}(A[i][k] * B[j][k], \text{axis}=k)\)</span></p>
<ul>
<li>对 <code>A</code> 的访问 <span class="arithmatex">\(A[i][k]\)</span> 与 <code>j</code> 无关</li>
<li>因此，对 <code>j</code> 维度进行大小为 <code>v</code> 的分块，可以使得 <code>A</code> 的数据被复用 <code>v</code> 次</li>
</ul>
<h5 id="_47">讨论：卷积中可能的复用模式<a class="headerlink" href="#_47" title="Permanent link">&para;</a></h5>
<div class="language-c highlight"><pre><span></span><code><span id="__span-13-1"><a id="__codelineno-13-1" name="__codelineno-13-1" href="#__codelineno-13-1"></a><span class="kt">float</span><span class="w"> </span><span class="n">Input</span><span class="p">[</span><span class="n">n</span><span class="p">][</span><span class="n">ci</span><span class="p">][</span><span class="n">h</span><span class="p">][</span><span class="n">w</span><span class="p">];</span>
</span><span id="__span-13-2"><a id="__codelineno-13-2" name="__codelineno-13-2" href="#__codelineno-13-2"></a><span class="kt">float</span><span class="w"> </span><span class="n">Weight</span><span class="p">[</span><span class="n">co</span><span class="p">][</span><span class="n">ci</span><span class="p">][</span><span class="n">K</span><span class="p">][</span><span class="n">K</span><span class="p">];</span>
</span><span id="__span-13-3"><a id="__codelineno-13-3" name="__codelineno-13-3" href="#__codelineno-13-3"></a><span class="kt">float</span><span class="w"> </span><span class="n">Output</span><span class="p">[</span><span class="n">n</span><span class="p">][</span><span class="n">co</span><span class="p">][</span><span class="n">h</span><span class="p">][</span><span class="n">w</span><span class="p">];</span>
</span><span id="__span-13-4"><a id="__codelineno-13-4" name="__codelineno-13-4" href="#__codelineno-13-4"></a>
</span><span id="__span-13-5"><a id="__codelineno-13-5" name="__codelineno-13-5" href="#__codelineno-13-5"></a><span class="c1">// 卷积计算公式</span>
</span><span id="__span-13-6"><a id="__codelineno-13-6" name="__codelineno-13-6" href="#__codelineno-13-6"></a><span class="n">Output</span><span class="p">[</span><span class="n">b</span><span class="p">][</span><span class="n">co</span><span class="p">][</span><span class="n">y</span><span class="p">][</span><span class="n">x</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sum</span><span class="p">(</span><span class="n">Input</span><span class="p">[</span><span class="n">b</span><span class="p">][</span><span class="n">k</span><span class="p">][</span><span class="n">y</span><span class="o">+</span><span class="n">ry</span><span class="p">][</span><span class="n">x</span><span class="o">+</span><span class="n">rx</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">Weight</span><span class="p">[</span><span class="n">co</span><span class="p">][</span><span class="n">k</span><span class="p">][</span><span class="n">ry</span><span class="p">][</span><span class="n">rx</span><span class="p">],</span><span class="w"> </span><span class="n">axis</span><span class="o">=</span><span class="p">[</span><span class="n">k</span><span class="p">,</span><span class="w"> </span><span class="n">ry</span><span class="p">,</span><span class="w"> </span><span class="n">rx</span><span class="p">])</span>
</span></code></pre></div>
<ul>
<li>可以思考在 <code>Input</code> 和 <code>Weight</code> 张量的访问中存在哪些复用模式</li>
</ul>
<h2 id="lec12-gpu">Lec12 GPU 加速<a class="headerlink" href="#lec12-gpu" title="Permanent link">&para;</a></h2>
<h3 id="121-gpu">12.1 GPU 编程<a class="headerlink" href="#121-gpu" title="Permanent link">&para;</a></h3>
<h4 id="gpu">什么是 GPU<a class="headerlink" href="#gpu" title="Permanent link">&para;</a></h4>
<p>与 CPU 相比, GPU 拥有大规模的并行计算单元</p>
<p><img alt="" src="10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/IMG-10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-20250831165257755.png" /></p>
<ul>
<li><strong>CPU (Central Processing Unit)</strong><ul>
<li>拥有较少的 <code>Core</code></li>
<li>每个 <code>Core</code> 配有较大的 <code>Control</code> 单元和 <code>L1 Cache</code></li>
<li>拥有共享的 <code>L2 Cache</code> 和 <code>L3 Cache</code></li>
<li>适用于处理复杂的串行任务</li>
</ul>
</li>
</ul>
<ul>
<li><strong>GPU (Graphics Processing Unit)</strong><ul>
<li>拥有海量的 <code>Core</code> (绿色方块)</li>
<li><code>Control</code> 单元和 <code>Cache</code> 相对较小</li>
<li>通过大量线程并行执行简单任务, 实现高吞吐量</li>
</ul>
</li>
</ul>
<h4 id="gpu-simt">GPU 编程模型: SIMT<a class="headerlink" href="#gpu-simt" title="Permanent link">&para;</a></h4>
<p>GPU 采用 <strong>SIMT (Single Instruction Multiple Threads)</strong> 编程模型</p>
<ul>
<li>所有线程执行相同的代码, 但可以根据数据走不同的执行路径</li>
</ul>
<p><strong>CUDA 编程模型术语</strong>
(注意: 其他 GPU 编程模型如 OpenCL, SYCL, Metal 也有类似概念)</p>
<ol>
<li><strong>Thread</strong>
    - 最基本的执行单元</li>
<li><strong>Thread Block</strong>
    - <code>Thread</code> 被组织成 <code>Block</code>
    - 同一个 <code>Block</code> 内的 <code>Thread</code> 共享一块 <code>Shared Memory</code>, 可以相互协作
    - 例如, 下图展示了包含 <code>block0</code>, <code>block1</code>, <code>block2</code>, <code>block3</code> 的结构
      <img src="10-414_深度学习系统课程笔记.assets/IMG-10-414_深度学习系统课程笔记-20250831171114788.png" style="zoom:33%;" /></li>
<li><strong>Grid</strong>
    - <code>Block</code> 被组织成 <code>Grid</code>
    - 一个 <code>Kernel</code> 函数会启动一个 <code>Grid</code> 来执行</li>
</ol>
<h4 id="_48">示例: 向量加法<a class="headerlink" href="#_48" title="Permanent link">&para;</a></h4>
<p>这是一个在 CPU 和 GPU 上实现向量加法 <span class="arithmatex">\(C[i] = A[i] + B[i]\)</span> 的对比</p>
<p><strong>CPU 实现</strong></p>
<div class="language-cpp highlight"><pre><span></span><code><span id="__span-14-1"><a id="__codelineno-14-1" name="__codelineno-14-1" href="#__codelineno-14-1"></a><span class="kt">void</span><span class="w"> </span><span class="nf">VecAddCPU</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">B</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">C</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">n</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-14-2"><a id="__codelineno-14-2" name="__codelineno-14-2" href="#__codelineno-14-2"></a><span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">n</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-14-3"><a id="__codelineno-14-3" name="__codelineno-14-3" href="#__codelineno-14-3"></a><span class="w">        </span><span class="n">C</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">B</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
</span><span id="__span-14-4"><a id="__codelineno-14-4" name="__codelineno-14-4" href="#__codelineno-14-4"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-14-5"><a id="__codelineno-14-5" name="__codelineno-14-5" href="#__codelineno-14-5"></a><span class="p">}</span>
</span></code></pre></div>
<p><strong>GPU 实现 (Kernel 函数)</strong></p>
<p>在 GPU 上执行的函数称为 Kernel, 使用 <strong>global</strong> 标识符</p>
<p>每个线程负责计算结果向量 C 中的一个元素</p>
<div class="language-cpp highlight"><pre><span></span><code><span id="__span-15-1"><a id="__codelineno-15-1" name="__codelineno-15-1" href="#__codelineno-15-1"></a><span class="n">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">VecAddKernel</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">B</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">C</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">n</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-15-2"><a id="__codelineno-15-2" name="__codelineno-15-2" href="#__codelineno-15-2"></a><span class="w">    </span><span class="c1">// 计算当前线程在全局数据中的索引 i</span>
</span><span id="__span-15-3"><a id="__codelineno-15-3" name="__codelineno-15-3" href="#__codelineno-15-3"></a><span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span><span id="__span-15-4"><a id="__codelineno-15-4" name="__codelineno-15-4" href="#__codelineno-15-4"></a><span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">n</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-15-5"><a id="__codelineno-15-5" name="__codelineno-15-5" href="#__codelineno-15-5"></a><span class="w">        </span><span class="n">C</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">B</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
</span><span id="__span-15-6"><a id="__codelineno-15-6" name="__codelineno-15-6" href="#__codelineno-15-6"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-15-7"><a id="__codelineno-15-7" name="__codelineno-15-7" href="#__codelineno-15-7"></a><span class="p">}</span>
</span></code></pre></div>
<ul>
<li><code>blockIdx.x</code>: 当前 <code>Block</code> 在 <code>Grid</code> 中的索引</li>
<li><code>threadIdx.x</code>: 当前 <code>Thread</code> 在 <code>Block</code> 中的索引</li>
<li><code>blockDim.x</code>: 每个 <code>Block</code> 中 <code>Thread</code> 的数量</li>
</ul>
<p><strong>线程索引计算图解</strong></p>
<p>假设每个 <code>Block</code> 包含 4 个 <code>Thread</code> (<code>blockDim.x = 4</code>)
<img alt="" src="10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/IMG-10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-20250831171735237.png" />
全局索引 <span class="arithmatex">\(i\)</span> 的计算公式为: <span class="arithmatex">\(i = \text{blockDim.x} * \text{blockIdx.x} + \text{threadIdx.x}\)</span></p>
<p>本质上不同Thread执行的都是同一段代码, 因为索引不同而产生区别
这段代码不存在数据依赖, 可以非常简单的实现并行计算</p>
<p><strong>Host 端代码</strong></p>
<p>在 CPU (Host) 端, 我们需要:</p>
<ol>
<li>在 GPU (Device) 上分配内存</li>
<li>将数据从 Host 拷贝到 Device</li>
<li>启动 <code>Kernel</code> 函数</li>
<li>将结果从 Device 拷贝回 Host</li>
<li>释放 Device 上的内存</li>
</ol>
<div class="language-cpp highlight"><pre><span></span><code><span id="__span-16-1"><a id="__codelineno-16-1" name="__codelineno-16-1" href="#__codelineno-16-1"></a><span class="kt">void</span><span class="w"> </span><span class="nf">VecAddCUDA</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">Acpu</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">Bcpu</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">Ccpu</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">n</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-16-2"><a id="__codelineno-16-2" name="__codelineno-16-2" href="#__codelineno-16-2"></a><span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">dA</span><span class="p">,</span><span class="w"> </span><span class="o">*</span><span class="n">dB</span><span class="p">,</span><span class="w"> </span><span class="o">*</span><span class="n">dC</span><span class="p">;</span>
</span><span id="__span-16-3"><a id="__codelineno-16-3" name="__codelineno-16-3" href="#__codelineno-16-3"></a>
</span><span id="__span-16-4"><a id="__codelineno-16-4" name="__codelineno-16-4" href="#__codelineno-16-4"></a><span class="w">    </span><span class="c1">// 1. 在 Device 上分配内存</span>
</span><span id="__span-16-5"><a id="__codelineno-16-5" name="__codelineno-16-5" href="#__codelineno-16-5"></a><span class="w">    </span><span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">dA</span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">));</span>
</span><span id="__span-16-6"><a id="__codelineno-16-6" name="__codelineno-16-6" href="#__codelineno-16-6"></a><span class="w">    </span><span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">dB</span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">));</span>
</span><span id="__span-16-7"><a id="__codelineno-16-7" name="__codelineno-16-7" href="#__codelineno-16-7"></a><span class="w">    </span><span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">dC</span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">));</span>
</span><span id="__span-16-8"><a id="__codelineno-16-8" name="__codelineno-16-8" href="#__codelineno-16-8"></a>
</span><span id="__span-16-9"><a id="__codelineno-16-9" name="__codelineno-16-9" href="#__codelineno-16-9"></a><span class="w">    </span><span class="c1">// 2. 将数据从 Host 拷贝到 Device</span>
</span><span id="__span-16-10"><a id="__codelineno-16-10" name="__codelineno-16-10" href="#__codelineno-16-10"></a><span class="w">    </span><span class="c1">// 瓶颈在这里, 频繁的PCIe传输过于耗时</span>
</span><span id="__span-16-11"><a id="__codelineno-16-11" name="__codelineno-16-11" href="#__codelineno-16-11"></a><span class="w">    </span><span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">dA</span><span class="p">,</span><span class="w"> </span><span class="n">Acpu</span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">),</span><span class="w"> </span><span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>
</span><span id="__span-16-12"><a id="__codelineno-16-12" name="__codelineno-16-12" href="#__codelineno-16-12"></a><span class="w">    </span><span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">dB</span><span class="p">,</span><span class="w"> </span><span class="n">Bcpu</span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">),</span><span class="w"> </span><span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>
</span><span id="__span-16-13"><a id="__codelineno-16-13" name="__codelineno-16-13" href="#__codelineno-16-13"></a>
</span><span id="__span-16-14"><a id="__codelineno-16-14" name="__codelineno-16-14" href="#__codelineno-16-14"></a><span class="w">    </span><span class="c1">// 3. 启动 Kernel</span>
</span><span id="__span-16-15"><a id="__codelineno-16-15" name="__codelineno-16-15" href="#__codelineno-16-15"></a><span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">threads_per_block</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">512</span><span class="p">;</span>
</span><span id="__span-16-16"><a id="__codelineno-16-16" name="__codelineno-16-16" href="#__codelineno-16-16"></a><span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">nblocks</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">n</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threads_per_block</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">threads_per_block</span><span class="p">;</span>
</span><span id="__span-16-17"><a id="__codelineno-16-17" name="__codelineno-16-17" href="#__codelineno-16-17"></a><span class="w">    </span><span class="n">VecAddKernel</span><span class="o">&lt;&lt;&lt;</span><span class="n">nblocks</span><span class="p">,</span><span class="w"> </span><span class="n">threads_per_block</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">dA</span><span class="p">,</span><span class="w"> </span><span class="n">dB</span><span class="p">,</span><span class="w"> </span><span class="n">dC</span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="p">);</span>
</span><span id="__span-16-18"><a id="__codelineno-16-18" name="__codelineno-16-18" href="#__codelineno-16-18"></a>
</span><span id="__span-16-19"><a id="__codelineno-16-19" name="__codelineno-16-19" href="#__codelineno-16-19"></a><span class="w">    </span><span class="c1">// 4. 将结果从 Device 拷贝回 Host</span>
</span><span id="__span-16-20"><a id="__codelineno-16-20" name="__codelineno-16-20" href="#__codelineno-16-20"></a><span class="w">    </span><span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">Ccpu</span><span class="p">,</span><span class="w"> </span><span class="n">dC</span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">),</span><span class="w"> </span><span class="n">cudaMemcpyDeviceToHost</span><span class="p">);</span>
</span><span id="__span-16-21"><a id="__codelineno-16-21" name="__codelineno-16-21" href="#__codelineno-16-21"></a>
</span><span id="__span-16-22"><a id="__codelineno-16-22" name="__codelineno-16-22" href="#__codelineno-16-22"></a><span class="w">    </span><span class="c1">// 5. 释放 Device 内存</span>
</span><span id="__span-16-23"><a id="__codelineno-16-23" name="__codelineno-16-23" href="#__codelineno-16-23"></a><span class="w">    </span><span class="n">cudaFree</span><span class="p">(</span><span class="n">dA</span><span class="p">);</span>
</span><span id="__span-16-24"><a id="__codelineno-16-24" name="__codelineno-16-24" href="#__codelineno-16-24"></a><span class="w">    </span><span class="n">cudaFree</span><span class="p">(</span><span class="n">dB</span><span class="p">);</span>
</span><span id="__span-16-25"><a id="__codelineno-16-25" name="__codelineno-16-25" href="#__codelineno-16-25"></a><span class="w">    </span><span class="n">cudaFree</span><span class="p">(</span><span class="n">dC</span><span class="p">);</span>
</span><span id="__span-16-26"><a id="__codelineno-16-26" name="__codelineno-16-26" href="#__codelineno-16-26"></a><span class="p">}</span>
</span></code></pre></div>
<p>注意: 实际应用中, 为了避免频繁的内存拷贝开销, 数据通常会<strong>尽可能长时间</strong>地保留在 GPU 内存中</p>
<h4 id="gpu_1">其他 GPU 编程模型示例<a class="headerlink" href="#gpu_1" title="Permanent link">&para;</a></h4>
<p><strong>OpenCL (用于 ARM GPU 等)</strong></p>
<div class="language-cpp highlight"><pre><span></span><code><span id="__span-17-1"><a id="__codelineno-17-1" name="__codelineno-17-1" href="#__codelineno-17-1"></a><span class="n">kernel</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">VecAdd</span><span class="p">(</span><span class="n">__global</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">a</span><span class="p">,</span><span class="w"> </span>
</span><span id="__span-17-2"><a id="__codelineno-17-2" name="__codelineno-17-2" href="#__codelineno-17-2"></a><span class="w">                   </span><span class="n">__global</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">b</span><span class="p">,</span><span class="w"> </span>
</span><span id="__span-17-3"><a id="__codelineno-17-3" name="__codelineno-17-3" href="#__codelineno-17-3"></a><span class="w">                   </span><span class="n">__global</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">c</span><span class="p">,</span><span class="w"> </span>
</span><span id="__span-17-4"><a id="__codelineno-17-4" name="__codelineno-17-4" href="#__codelineno-17-4"></a><span class="w">                   </span><span class="kt">int</span><span class="w"> </span><span class="n">n</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-17-5"><a id="__codelineno-17-5" name="__codelineno-17-5" href="#__codelineno-17-5"></a><span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">gid</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">get_global_id</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
</span><span id="__span-17-6"><a id="__codelineno-17-6" name="__codelineno-17-6" href="#__codelineno-17-6"></a><span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">gid</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">n</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-17-7"><a id="__codelineno-17-7" name="__codelineno-17-7" href="#__codelineno-17-7"></a><span class="w">        </span><span class="n">c</span><span class="p">[</span><span class="n">gid</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">a</span><span class="p">[</span><span class="n">gid</span><span class="p">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">b</span><span class="p">[</span><span class="n">gid</span><span class="p">];</span>
</span><span id="__span-17-8"><a id="__codelineno-17-8" name="__codelineno-17-8" href="#__codelineno-17-8"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-17-9"><a id="__codelineno-17-9" name="__codelineno-17-9" href="#__codelineno-17-9"></a><span class="p">}</span>
</span></code></pre></div>
<p><strong>Metal (用于 Apple 设备)</strong></p>
<div class="language-cpp highlight"><pre><span></span><code><span id="__span-18-1"><a id="__codelineno-18-1" name="__codelineno-18-1" href="#__codelineno-18-1"></a><span class="n">kernel</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">VecAdd</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="p">[[</span><span class="n">buffer</span><span class="p">(</span><span class="mi">0</span><span class="p">)]],</span>
</span><span id="__span-18-2"><a id="__codelineno-18-2" name="__codelineno-18-2" href="#__codelineno-18-2"></a><span class="w">                   </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">b</span><span class="w"> </span><span class="p">[[</span><span class="n">buffer</span><span class="p">(</span><span class="mi">1</span><span class="p">)]],</span>
</span><span id="__span-18-3"><a id="__codelineno-18-3" name="__codelineno-18-3" href="#__codelineno-18-3"></a><span class="w">                   </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">c</span><span class="w"> </span><span class="p">[[</span><span class="n">buffer</span><span class="p">(</span><span class="mi">2</span><span class="p">)]],</span><span class="w"> </span><span class="c1">// 应该是 buffer(2)</span>
</span><span id="__span-18-4"><a id="__codelineno-18-4" name="__codelineno-18-4" href="#__codelineno-18-4"></a><span class="w">                   </span><span class="n">uint</span><span class="w"> </span><span class="n">gid</span><span class="w"> </span><span class="p">[[</span><span class="n">thread_position_in_grid</span><span class="p">]],</span>
</span><span id="__span-18-5"><a id="__codelineno-18-5" name="__codelineno-18-5" href="#__codelineno-18-5"></a><span class="w">                   </span><span class="kt">int</span><span class="w"> </span><span class="n">n</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-18-6"><a id="__codelineno-18-6" name="__codelineno-18-6" href="#__codelineno-18-6"></a><span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">gid</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">n</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-18-7"><a id="__codelineno-18-7" name="__codelineno-18-7" href="#__codelineno-18-7"></a><span class="w">        </span><span class="n">c</span><span class="p">[</span><span class="n">gid</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">a</span><span class="p">[</span><span class="n">gid</span><span class="p">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">b</span><span class="p">[</span><span class="n">gid</span><span class="p">];</span>
</span><span id="__span-18-8"><a id="__codelineno-18-8" name="__codelineno-18-8" href="#__codelineno-18-8"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-18-9"><a id="__codelineno-18-9" name="__codelineno-18-9" href="#__codelineno-18-9"></a><span class="p">}</span>
</span></code></pre></div>
<h4 id="gpu_2">GPU 内存层级<a class="headerlink" href="#gpu_2" title="Permanent link">&para;</a></h4>
<p><img alt="" src="10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/IMG-10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-20250831174149845.png" /></p>
<ul>
<li><strong>Registers</strong>:<ul>
<li>每个 <code>Thread</code> 私有</li>
<li>速度最快</li>
<li>生命周期与 <code>Thread</code> 相同</li>
</ul>
</li>
<li><strong>Shared Memory</strong>:<ul>
<li>每个 <code>Thread Block</code> 共享</li>
<li>速度快于 <code>Global Memory</code></li>
<li>用于 <code>Block</code> 内 <code>Thread</code> 间的通信与数据共享</li>
</ul>
</li>
<li><strong>Global Memory</strong>:<ul>
<li>所有 <code>Thread</code> 均可访问 (整个 <code>Grid</code>)</li>
<li>速度最慢, 延迟最高</li>
<li>容量最大</li>
</ul>
</li>
</ul>
<h4 id="window-sum">示例: 窗口求和 (Window Sum)<a class="headerlink" href="#window-sum" title="Permanent link">&para;</a></h4>
<p><strong>任务</strong>: 对输入数组 <code>A</code>, 计算一个滑动窗口内元素的和, 存入 <code>B</code> (窗口半径 <code>RADIUS = 2</code>)</p>
<p><strong>简单实现 (只用 Global Memory)</strong></p>
<p>每个线程独立从 <code>Global Memory</code> 中读取所有需要的元素
<img alt="" src="10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/IMG-10-414_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-20250831215341626.png" /></p>
<div class="language-cpp highlight"><pre><span></span><code><span id="__span-19-1"><a id="__codelineno-19-1" name="__codelineno-19-1" href="#__codelineno-19-1"></a><span class="cp">#define RADIUS 2</span>
</span><span id="__span-19-2"><a id="__codelineno-19-2" name="__codelineno-19-2" href="#__codelineno-19-2"></a><span class="n">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">WindowSumSimpleKernel</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">B</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">n</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-19-3"><a id="__codelineno-19-3" name="__codelineno-19-3" href="#__codelineno-19-3"></a><span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">out_idx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span><span id="__span-19-4"><a id="__codelineno-19-4" name="__codelineno-19-4" href="#__codelineno-19-4"></a><span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">out_idx</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">n</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-19-5"><a id="__codelineno-19-5" name="__codelineno-19-5" href="#__codelineno-19-5"></a><span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="n">sum</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
</span><span id="__span-19-6"><a id="__codelineno-19-6" name="__codelineno-19-6" href="#__codelineno-19-6"></a><span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">dx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">-</span><span class="n">RADIUS</span><span class="p">;</span><span class="w"> </span><span class="n">dx</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="n">RADIUS</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">dx</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-19-7"><a id="__codelineno-19-7" name="__codelineno-19-7" href="#__codelineno-19-7"></a><span class="w">            </span><span class="n">sum</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">A</span><span class="p">[</span><span class="n">dx</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">out_idx</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">RADIUS</span><span class="p">];</span>
</span><span id="__span-19-8"><a id="__codelineno-19-8" name="__codelineno-19-8" href="#__codelineno-19-8"></a><span class="w">        </span><span class="p">}</span>
</span><span id="__span-19-9"><a id="__codelineno-19-9" name="__codelineno-19-9" href="#__codelineno-19-9"></a><span class="w">        </span><span class="n">B</span><span class="p">[</span><span class="n">out_idx</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sum</span><span class="p">;</span>
</span><span id="__span-19-10"><a id="__codelineno-19-10" name="__codelineno-19-10" href="#__codelineno-19-10"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-19-11"><a id="__codelineno-19-11" name="__codelineno-19-11" href="#__codelineno-19-11"></a><span class="p">}</span>
</span></code></pre></div>
<ul>
<li><strong>问题</strong>: 相邻线程读取的数据有大量重叠, 导致对 <code>Global Memory</code> 的重复访问, 效率低下</li>
</ul>
<p><strong>使用 Shared Memory 的优化实现</strong></p>
<p>利用 <code>Block</code> 内线程协作, 将计算所需的数据一次性从 <code>Global Memory</code> 读入 <code>Shared Memory</code>, 然后每个线程从 <code>Shared Memory</code> 中读取数据进行计算</p>
<div class="language-cpp highlight"><pre><span></span><code><span id="__span-20-1"><a id="__codelineno-20-1" name="__codelineno-20-1" href="#__codelineno-20-1"></a><span class="n">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">WindowSumSharedKernel</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">B</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">n</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-20-2"><a id="__codelineno-20-2" name="__codelineno-20-2" href="#__codelineno-20-2"></a><span class="w">    </span><span class="c1">// 1. 在 Shared Memory 中声明一个临时数组</span>
</span><span id="__span-20-3"><a id="__codelineno-20-3" name="__codelineno-20-3" href="#__codelineno-20-3"></a><span class="w">    </span><span class="n">__shared__</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">temp</span><span class="p">[</span><span class="n">THREADS_PER_BLOCK</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">RADIUS</span><span class="p">];</span>
</span><span id="__span-20-4"><a id="__codelineno-20-4" name="__codelineno-20-4" href="#__codelineno-20-4"></a>
</span><span id="__span-20-5"><a id="__codelineno-20-5" name="__codelineno-20-5" href="#__codelineno-20-5"></a><span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">base</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span><span id="__span-20-6"><a id="__codelineno-20-6" name="__codelineno-20-6" href="#__codelineno-20-6"></a><span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">out_idx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">base</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span><span id="__span-20-7"><a id="__codelineno-20-7" name="__codelineno-20-7" href="#__codelineno-20-7"></a>
</span><span id="__span-20-8"><a id="__codelineno-20-8" name="__codelineno-20-8" href="#__codelineno-20-8"></a><span class="w">    </span><span class="c1">// 2. 协作将数据从 Global Memory 载入 Shared Memory</span>
</span><span id="__span-20-9"><a id="__codelineno-20-9" name="__codelineno-20-9" href="#__codelineno-20-9"></a><span class="w">    </span><span class="c1">// 载入核心部分</span>
</span><span id="__span-20-10"><a id="__codelineno-20-10" name="__codelineno-20-10" href="#__codelineno-20-10"></a><span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">base</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">n</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-20-11"><a id="__codelineno-20-11" name="__codelineno-20-11" href="#__codelineno-20-11"></a><span class="w">        </span><span class="n">temp</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">RADIUS</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">A</span><span class="p">[</span><span class="n">base</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">];</span><span class="w"> </span><span class="c1">// 偏移 RADIUS</span>
</span><span id="__span-20-12"><a id="__codelineno-20-12" name="__codelineno-20-12" href="#__codelineno-20-12"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-20-13"><a id="__codelineno-20-13" name="__codelineno-20-13" href="#__codelineno-20-13"></a><span class="w">    </span><span class="c1">// 载入处理边界所需的 &quot;ghost&quot; 元素</span>
</span><span id="__span-20-14"><a id="__codelineno-20-14" name="__codelineno-20-14" href="#__codelineno-20-14"></a><span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">RADIUS</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-20-15"><a id="__codelineno-20-15" name="__codelineno-20-15" href="#__codelineno-20-15"></a><span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">base</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">RADIUS</span><span class="w"> </span><span class="o">&gt;=</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span>
</span><span id="__span-20-16"><a id="__codelineno-20-16" name="__codelineno-20-16" href="#__codelineno-20-16"></a><span class="w">            </span><span class="n">temp</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">A</span><span class="p">[</span><span class="n">base</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">RADIUS</span><span class="p">];</span>
</span><span id="__span-20-17"><a id="__codelineno-20-17" name="__codelineno-20-17" href="#__codelineno-20-17"></a><span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">base</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">THREADS_PER_BLOCK</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">n</span><span class="p">)</span>
</span><span id="__span-20-18"><a id="__codelineno-20-18" name="__codelineno-20-18" href="#__codelineno-20-18"></a><span class="w">            </span><span class="n">temp</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">THREADS_PER_BLOCK</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">RADIUS</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">A</span><span class="p">[</span><span class="n">base</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">THREADS_PER_BLOCK</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">];</span>
</span><span id="__span-20-19"><a id="__codelineno-20-19" name="__codelineno-20-19" href="#__codelineno-20-19"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-20-20"><a id="__codelineno-20-20" name="__codelineno-20-20" href="#__codelineno-20-20"></a>
</span><span id="__span-20-21"><a id="__codelineno-20-21" name="__codelineno-20-21" href="#__codelineno-20-21"></a><span class="w">    </span><span class="c1">// 3. 同步, 确保所有数据都已载入 Shared Memory</span>
</span><span id="__span-20-22"><a id="__codelineno-20-22" name="__codelineno-20-22" href="#__codelineno-20-22"></a><span class="w">    </span><span class="n">__syncthreads</span><span class="p">();</span>
</span><span id="__span-20-23"><a id="__codelineno-20-23" name="__codelineno-20-23" href="#__codelineno-20-23"></a>
</span><span id="__span-20-24"><a id="__codelineno-20-24" name="__codelineno-20-24" href="#__codelineno-20-24"></a><span class="w">    </span><span class="c1">// 4. 从 Shared Memory 中读取数据进行计算</span>
</span><span id="__span-20-25"><a id="__codelineno-20-25" name="__codelineno-20-25" href="#__codelineno-20-25"></a><span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">out_idx</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">n</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-20-26"><a id="__codelineno-20-26" name="__codelineno-20-26" href="#__codelineno-20-26"></a><span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="n">sum</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
</span><span id="__span-20-27"><a id="__codelineno-20-27" name="__codelineno-20-27" href="#__codelineno-20-27"></a><span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">dx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">-</span><span class="n">RADIUS</span><span class="p">;</span><span class="w"> </span><span class="n">dx</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="n">RADIUS</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">dx</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-20-28"><a id="__codelineno-20-28" name="__codelineno-20-28" href="#__codelineno-20-28"></a><span class="w">            </span><span class="n">sum</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">temp</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">RADIUS</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">dx</span><span class="p">];</span>
</span><span id="__span-20-29"><a id="__codelineno-20-29" name="__codelineno-20-29" href="#__codelineno-20-29"></a><span class="w">        </span><span class="p">}</span>
</span><span id="__span-20-30"><a id="__codelineno-20-30" name="__codelineno-20-30" href="#__codelineno-20-30"></a><span class="w">        </span><span class="n">B</span><span class="p">[</span><span class="n">out_idx</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sum</span><span class="p">;</span>
</span><span id="__span-20-31"><a id="__codelineno-20-31" name="__codelineno-20-31" href="#__codelineno-20-31"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-20-32"><a id="__codelineno-20-32" name="__codelineno-20-32" href="#__codelineno-20-32"></a><span class="p">}</span>
</span></code></pre></div>
<ul>
<li>通过协作读取到 <code>Shared Memory</code>, 增加了数据复用, 显著减少了对 <code>Global Memory</code> 的访问次数</li>
</ul>
<h4 id="_49">核心要点<a class="headerlink" href="#_49" title="Permanent link">&para;</a></h4>
<ul>
<li>通过启动 <code>Grid</code> 和 <code>Block</code> 来组织大量线程</li>
<li>通过协作将数据块读取到 <code>Shared Memory</code> 中以增加数据复用, 这是 GPU 优化的关键技巧</li>
</ul>
<h3 id="122-gpu">12.2 案例学习: GPU 上的矩阵乘法<a class="headerlink" href="#122-gpu" title="Permanent link">&para;</a></h3>
<h4 id="register-tiling">线程级别优化: Register Tiling<a class="headerlink" href="#register-tiling" title="Permanent link">&para;</a></h4>
<p><strong>思路</strong>: 每个线程负责计算结果矩阵 <code>C</code> 的一个 <span class="arithmatex">\(V \times V\)</span> 的小块, 这个小块的数据 (<code>c[V][V]</code>) 直接<strong>存储在寄存器</strong>中以获得最高访问速度
<img src="10-414_深度学习系统课程笔记.assets/IMG-10-414_深度学习系统课程笔记-20250901164038876.png" alt="图: Register Tiling 示意图" style="zoom:33%;" /></p>
<ul>
<li>每个线程从 <code>A.T</code> 的一行和 <code>B</code> 的一列中分别读取 <code>V</code> 个元素, 计算出一个外积, 累加到寄存器中的 <span class="arithmatex">\(V \times V\)</span> 块</li>
</ul>
<div class="language-cpp highlight"><pre><span></span><code><span id="__span-21-1"><a id="__codelineno-21-1" name="__codelineno-21-1" href="#__codelineno-21-1"></a><span class="c1">// C = A.T * B</span>
</span><span id="__span-21-2"><a id="__codelineno-21-2" name="__codelineno-21-2" href="#__codelineno-21-2"></a><span class="n">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">mm</span><span class="p">(</span><span class="kt">float</span><span class="w"> </span><span class="n">A</span><span class="p">[</span><span class="n">N</span><span class="p">][</span><span class="n">N</span><span class="p">],</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">B</span><span class="p">[</span><span class="n">N</span><span class="p">][</span><span class="n">N</span><span class="p">],</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">C</span><span class="p">[</span><span class="n">N</span><span class="p">][</span><span class="n">N</span><span class="p">])</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-21-3"><a id="__codelineno-21-3" name="__codelineno-21-3" href="#__codelineno-21-3"></a><span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">ybase</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">blockDim</span><span class="p">.</span><span class="n">y</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
</span><span id="__span-21-4"><a id="__codelineno-21-4" name="__codelineno-21-4" href="#__codelineno-21-4"></a><span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">xbase</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span><span id="__span-21-5"><a id="__codelineno-21-5" name="__codelineno-21-5" href="#__codelineno-21-5"></a>
</span><span id="__span-21-6"><a id="__codelineno-21-6" name="__codelineno-21-6" href="#__codelineno-21-6"></a><span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">c</span><span class="p">[</span><span class="n">V</span><span class="p">][</span><span class="n">V</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="mi">0</span><span class="p">};</span><span class="w"> </span><span class="c1">// V*V 的结果块, 存储在寄存器中</span>
</span><span id="__span-21-7"><a id="__codelineno-21-7" name="__codelineno-21-7" href="#__codelineno-21-7"></a><span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">a</span><span class="p">[</span><span class="n">V</span><span class="p">],</span><span class="w"> </span><span class="n">b</span><span class="p">[</span><span class="n">V</span><span class="p">];</span><span class="w">    </span><span class="c1">// 临时存储从 A 和 B 读取的数据</span>
</span><span id="__span-21-8"><a id="__codelineno-21-8" name="__codelineno-21-8" href="#__codelineno-21-8"></a>
</span><span id="__span-21-9"><a id="__codelineno-21-9" name="__codelineno-21-9" href="#__codelineno-21-9"></a><span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">N</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">k</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-21-10"><a id="__codelineno-21-10" name="__codelineno-21-10" href="#__codelineno-21-10"></a><span class="w">        </span><span class="c1">// 从 Global Memory 读取数据</span>
</span><span id="__span-21-11"><a id="__codelineno-21-11" name="__codelineno-21-11" href="#__codelineno-21-11"></a><span class="w">        </span><span class="n">a</span><span class="p">[</span><span class="o">:</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">A</span><span class="p">[</span><span class="n">k</span><span class="p">,</span><span class="w"> </span><span class="n">ybase</span><span class="o">*</span><span class="n">V</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">ybase</span><span class="o">*</span><span class="n">V</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">V</span><span class="p">];</span>
</span><span id="__span-21-12"><a id="__codelineno-21-12" name="__codelineno-21-12" href="#__codelineno-21-12"></a><span class="w">        </span><span class="n">b</span><span class="p">[</span><span class="o">:</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">,</span><span class="w"> </span><span class="n">xbase</span><span class="o">*</span><span class="n">V</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">xbase</span><span class="o">*</span><span class="n">V</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">V</span><span class="p">];</span>
</span><span id="__span-21-13"><a id="__codelineno-21-13" name="__codelineno-21-13" href="#__codelineno-21-13"></a>
</span><span id="__span-21-14"><a id="__codelineno-21-14" name="__codelineno-21-14" href="#__codelineno-21-14"></a><span class="w">        </span><span class="c1">// 在寄存器中计算外积并累加</span>
</span><span id="__span-21-15"><a id="__codelineno-21-15" name="__codelineno-21-15" href="#__codelineno-21-15"></a><span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">V</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">y</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-21-16"><a id="__codelineno-21-16" name="__codelineno-21-16" href="#__codelineno-21-16"></a><span class="w">            </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">V</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-21-17"><a id="__codelineno-21-17" name="__codelineno-21-17" href="#__codelineno-21-17"></a><span class="w">                </span><span class="n">c</span><span class="p">[</span><span class="n">y</span><span class="p">][</span><span class="n">x</span><span class="p">]</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">a</span><span class="p">[</span><span class="n">y</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">b</span><span class="p">[</span><span class="n">x</span><span class="p">];</span>
</span><span id="__span-21-18"><a id="__codelineno-21-18" name="__codelineno-21-18" href="#__codelineno-21-18"></a><span class="w">            </span><span class="p">}</span>
</span><span id="__span-21-19"><a id="__codelineno-21-19" name="__codelineno-21-19" href="#__codelineno-21-19"></a><span class="w">        </span><span class="p">}</span>
</span><span id="__span-21-20"><a id="__codelineno-21-20" name="__codelineno-21-20" href="#__codelineno-21-20"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-21-21"><a id="__codelineno-21-21" name="__codelineno-21-21" href="#__codelineno-21-21"></a>
</span><span id="__span-21-22"><a id="__codelineno-21-22" name="__codelineno-21-22" href="#__codelineno-21-22"></a><span class="w">    </span><span class="c1">// 将结果写回 Global Memory</span>
</span><span id="__span-21-23"><a id="__codelineno-21-23" name="__codelineno-21-23" href="#__codelineno-21-23"></a><span class="w">    </span><span class="n">C</span><span class="p">[</span><span class="n">ybase</span><span class="o">*</span><span class="n">V</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">ybase</span><span class="o">*</span><span class="n">V</span><span class="o">+</span><span class="n">V</span><span class="p">,</span><span class="w"> </span><span class="n">xbase</span><span class="o">*</span><span class="n">V</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">xbase</span><span class="o">*</span><span class="n">V</span><span class="o">+</span><span class="n">V</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">c</span><span class="p">[</span><span class="o">:</span><span class="p">];</span>
</span><span id="__span-21-24"><a id="__codelineno-21-24" name="__codelineno-21-24" href="#__codelineno-21-24"></a><span class="p">}</span>
</span></code></pre></div>
<h4 id="shared-memory-tiling">块级别优化: Shared Memory Tiling<a class="headerlink" href="#shared-memory-tiling" title="Permanent link">&para;</a></h4>
<p><strong>思路</strong>: 每个 <code>Thread Block</code> 负责计算 <code>C</code> 的一个 <span class="arithmatex">\(L \times L\)</span> 的块, 每个线程依然计算其 <span class="arithmatex">\(V \times V\)</span> 的子块. <code>Block</code> 内的线程协作地将 <code>A</code> 和 <code>B</code> 的子块 (tile) 载入 <code>Shared Memory</code>, 然后所有线程从 <code>Shared Memory</code> 中读取数据进行计算</p>
<p><img src="10-414_深度学习系统课程笔记.assets/IMG-10-414_深度学习系统课程笔记-20250901164012453.png" style="zoom: 33%;" /></p>
<ul>
<li>每个 <code>Block</code> 从 <code>A.T</code> 和 <code>B</code> 中载入 <span class="arithmatex">\(S \times L\)</span> 和 <span class="arithmatex">\(S \times L\)</span> 的数据块到 <code>Shared Memory</code></li>
<li><code>Block</code> 内的每个 <code>Thread</code> 再从 <code>Shared Memory</code> 读取数据, 计算自己的 <span class="arithmatex">\(V \times V\)</span> 结果块</li>
</ul>
<div class="language-cpp highlight"><pre><span></span><code><span id="__span-22-1"><a id="__codelineno-22-1" name="__codelineno-22-1" href="#__codelineno-22-1"></a><span class="n">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">mm</span><span class="p">(</span><span class="kt">float</span><span class="w"> </span><span class="n">A</span><span class="p">[</span><span class="n">N</span><span class="p">][</span><span class="n">N</span><span class="p">],</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">B</span><span class="p">[</span><span class="n">N</span><span class="p">][</span><span class="n">N</span><span class="p">],</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">C</span><span class="p">[</span><span class="n">N</span><span class="p">][</span><span class="n">N</span><span class="p">])</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-22-2"><a id="__codelineno-22-2" name="__codelineno-22-2" href="#__codelineno-22-2"></a><span class="w">    </span><span class="n">__shared__</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">sA</span><span class="p">[</span><span class="n">S</span><span class="p">][</span><span class="n">L</span><span class="p">],</span><span class="w"> </span><span class="n">sB</span><span class="p">[</span><span class="n">S</span><span class="p">][</span><span class="n">L</span><span class="p">];</span><span class="w"> </span><span class="c1">// Shared Memory 中的数据块</span>
</span><span id="__span-22-3"><a id="__codelineno-22-3" name="__codelineno-22-3" href="#__codelineno-22-3"></a>
</span><span id="__span-22-4"><a id="__codelineno-22-4" name="__codelineno-22-4" href="#__codelineno-22-4"></a><span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">c</span><span class="p">[</span><span class="n">V</span><span class="p">][</span><span class="n">V</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="mi">0</span><span class="p">};</span><span class="w"> </span><span class="c1">// 每个线程的寄存器数据块</span>
</span><span id="__span-22-5"><a id="__codelineno-22-5" name="__codelineno-22-5" href="#__codelineno-22-5"></a><span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">a</span><span class="p">[</span><span class="n">V</span><span class="p">],</span><span class="w"> </span><span class="n">b</span><span class="p">[</span><span class="n">V</span><span class="p">];</span>
</span><span id="__span-22-6"><a id="__codelineno-22-6" name="__codelineno-22-6" href="#__codelineno-22-6"></a>
</span><span id="__span-22-7"><a id="__codelineno-22-7" name="__codelineno-22-7" href="#__codelineno-22-7"></a><span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">yblock</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
</span><span id="__span-22-8"><a id="__codelineno-22-8" name="__codelineno-22-8" href="#__codelineno-22-8"></a><span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">xblock</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span><span id="__span-22-9"><a id="__codelineno-22-9" name="__codelineno-22-9" href="#__codelineno-22-9"></a>
</span><span id="__span-22-10"><a id="__codelineno-22-10" name="__codelineno-22-10" href="#__codelineno-22-10"></a><span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">ko</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">ko</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">N</span><span class="p">;</span><span class="w"> </span><span class="n">ko</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">S</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-22-11"><a id="__codelineno-22-11" name="__codelineno-22-11" href="#__codelineno-22-11"></a><span class="w">        </span><span class="c1">// 1. 协作将 Global Memory 数据载入 Shared Memory</span>
</span><span id="__span-22-12"><a id="__codelineno-22-12" name="__codelineno-22-12" href="#__codelineno-22-12"></a><span class="w">        </span><span class="c1">// SA[:,:] = A[ko:ko+S, yblock*L : yblock*L+L];</span>
</span><span id="__span-22-13"><a id="__codelineno-22-13" name="__codelineno-22-13" href="#__codelineno-22-13"></a><span class="w">        </span><span class="c1">// sB[:,:] = B[ko:ko+S, xblock*L : xblock*L+L];</span>
</span><span id="__span-22-14"><a id="__codelineno-22-14" name="__codelineno-22-14" href="#__codelineno-22-14"></a><span class="w">        </span><span class="n">__syncthreads</span><span class="p">();</span><span class="w"> </span><span class="c1">// 等待所有线程完成载入</span>
</span><span id="__span-22-15"><a id="__codelineno-22-15" name="__codelineno-22-15" href="#__codelineno-22-15"></a>
</span><span id="__span-22-16"><a id="__codelineno-22-16" name="__codelineno-22-16" href="#__codelineno-22-16"></a><span class="w">        </span><span class="c1">// 2. 从 Shared Memory 读取数据到寄存器, 进行计算</span>
</span><span id="__span-22-17"><a id="__codelineno-22-17" name="__codelineno-22-17" href="#__codelineno-22-17"></a><span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">ki</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">ki</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">S</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">ki</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-22-18"><a id="__codelineno-22-18" name="__codelineno-22-18" href="#__codelineno-22-18"></a><span class="w">            </span><span class="n">a</span><span class="p">[</span><span class="o">:</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sA</span><span class="p">[</span><span class="n">ki</span><span class="p">,</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="o">*</span><span class="n">V</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="o">*</span><span class="n">V</span><span class="o">+</span><span class="n">V</span><span class="p">];</span>
</span><span id="__span-22-19"><a id="__codelineno-22-19" name="__codelineno-22-19" href="#__codelineno-22-19"></a><span class="w">            </span><span class="n">b</span><span class="p">[</span><span class="o">:</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sB</span><span class="p">[</span><span class="n">ki</span><span class="p">,</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="o">*</span><span class="n">V</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="o">*</span><span class="n">V</span><span class="o">+</span><span class="n">V</span><span class="p">];</span><span class="w"> </span><span class="c1">// PPT原文为 sA, 应为 sB</span>
</span><span id="__span-22-20"><a id="__codelineno-22-20" name="__codelineno-22-20" href="#__codelineno-22-20"></a>
</span><span id="__span-22-21"><a id="__codelineno-22-21" name="__codelineno-22-21" href="#__codelineno-22-21"></a><span class="w">            </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">V</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">y</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-22-22"><a id="__codelineno-22-22" name="__codelineno-22-22" href="#__codelineno-22-22"></a><span class="w">                </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">V</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-22-23"><a id="__codelineno-22-23" name="__codelineno-22-23" href="#__codelineno-22-23"></a><span class="w">                    </span><span class="n">c</span><span class="p">[</span><span class="n">y</span><span class="p">][</span><span class="n">x</span><span class="p">]</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">a</span><span class="p">[</span><span class="n">y</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">b</span><span class="p">[</span><span class="n">x</span><span class="p">];</span>
</span><span id="__span-22-24"><a id="__codelineno-22-24" name="__codelineno-22-24" href="#__codelineno-22-24"></a><span class="w">                </span><span class="p">}</span>
</span><span id="__span-22-25"><a id="__codelineno-22-25" name="__codelineno-22-25" href="#__codelineno-22-25"></a><span class="w">            </span><span class="p">}</span>
</span><span id="__span-22-26"><a id="__codelineno-22-26" name="__codelineno-22-26" href="#__codelineno-22-26"></a><span class="w">        </span><span class="p">}</span>
</span><span id="__span-22-27"><a id="__codelineno-22-27" name="__codelineno-22-27" href="#__codelineno-22-27"></a><span class="w">        </span><span class="n">__syncthreads</span><span class="p">();</span><span class="w"> </span><span class="c1">// 同步, 准备载入下一批数据块</span>
</span><span id="__span-22-28"><a id="__codelineno-22-28" name="__codelineno-22-28" href="#__codelineno-22-28"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-22-29"><a id="__codelineno-22-29" name="__codelineno-22-29" href="#__codelineno-22-29"></a><span class="w">    </span><span class="c1">// ... 将 c 写回 Global Memory ...</span>
</span><span id="__span-22-30"><a id="__codelineno-22-30" name="__codelineno-22-30" href="#__codelineno-22-30"></a><span class="p">}</span>
</span></code></pre></div>
<p><strong>内存复用分析</strong></p>
<ul>
<li><strong>Global -&gt; Shared 拷贝</strong>: <span class="arithmatex">\(2 \cdot N^3 / L\)</span></li>
<li>Shared -&gt; Register 拷贝: <span class="arithmatex">\(2 \cdot N^3 / V\)</span><p>通过增大 L 和 V 的值, 可以显著提高数据在 Shared Memory 和 Register 中的复用率, 从而减少对慢速 Global Memory 的访问</p>
</li>
</ul>
<p>展开协作式数据获取</p>
<p>将数据从 Global Memory 载入 Shared Memory (SA[:,:] = A[...]) 的具体实现如下:</p>
<div class="language-cpp highlight"><pre><span></span><code><span id="__span-23-1"><a id="__codelineno-23-1" name="__codelineno-23-1" href="#__codelineno-23-1"></a><span class="c1">// blockDim.y * blockDim.x</span>
</span><span id="__span-23-2"><a id="__codelineno-23-2" name="__codelineno-23-2" href="#__codelineno-23-2"></a><span class="kt">int</span><span class="w"> </span><span class="n">nthreads</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockDim</span><span class="p">.</span><span class="n">y</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">;</span><span class="w"> </span>
</span><span id="__span-23-3"><a id="__codelineno-23-3" name="__codelineno-23-3" href="#__codelineno-23-3"></a><span class="c1">// threadIdx.y * blockDim.x + threadIdx.x</span>
</span><span id="__span-23-4"><a id="__codelineno-23-4" name="__codelineno-23-4" href="#__codelineno-23-4"></a><span class="kt">int</span><span class="w"> </span><span class="n">tid</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span><span class="w"> </span>
</span><span id="__span-23-5"><a id="__codelineno-23-5" name="__codelineno-23-5" href="#__codelineno-23-5"></a>
</span><span id="__span-23-6"><a id="__codelineno-23-6" name="__codelineno-23-6" href="#__codelineno-23-6"></a><span class="k">for</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">L</span><span class="o">*</span><span class="n">S</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">nthreads</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">j</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-23-7"><a id="__codelineno-23-7" name="__codelineno-23-7" href="#__codelineno-23-7"></a><span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">j</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">nthreads</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">tid</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">L</span><span class="p">;</span>
</span><span id="__span-23-8"><a id="__codelineno-23-8" name="__codelineno-23-8" href="#__codelineno-23-8"></a><span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">j</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">nthreads</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">tid</span><span class="p">)</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="n">L</span><span class="p">;</span>
</span><span id="__span-23-9"><a id="__codelineno-23-9" name="__codelineno-23-9" href="#__codelineno-23-9"></a><span class="w">    </span><span class="n">sA</span><span class="p">[</span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">A</span><span class="p">[</span><span class="n">ko</span><span class="o">+</span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">yblock</span><span class="o">*</span><span class="n">L</span><span class="o">+</span><span class="n">x</span><span class="p">];</span><span class="w"> </span><span class="c1">// sA in ppt, not s</span>
</span><span id="__span-23-10"><a id="__codelineno-23-10" name="__codelineno-23-10" href="#__codelineno-23-10"></a><span class="p">}</span>
</span></code></pre></div>
<h4 id="gpu_3">更多 GPU 优化技巧<a class="headerlink" href="#gpu_3" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Global Memory Coalescing (全局内存连续读取)</strong>: 让同一个 <code>Warp</code> 中的线程连续访问 <code>Global Memory</code> 地址, 可以合并成一次内存事务, 提高带宽利用率</li>
<li><strong>Shared Memory Bank Conflict</strong>: <code>Shared Memory</code> 分为多个 <code>Bank</code>, 如果 <code>Warp</code> 内多个线程同时访问同一个 <code>Bank</code>, 就会发生冲突, 导致访问串行化, 降低性能</li>
<li><strong>Software Pipelining</strong>: 通过软件技术重叠数据加载和计算指令, 隐藏内存访问延迟</li>
<li><strong>Warp Level Optimizations</strong>: 针对 <code>Warp</code>(一组32个线程)的特性进行优化</li>
<li><strong>Tensor Core</strong>: 利用 NVIDIA GPU 中的专用硬件单元加速矩阵乘加运算</li>
</ul>
<blockquote>
<p>更多细节请查阅 CUDA Programming Guide</p>
</blockquote>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  回到页面顶部
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="页脚" >
        
          
          <a href="index.html" class="md-footer__link md-footer__link--prev" aria-label="上一页: Index">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                上一页
              </span>
              <div class="md-ellipsis">
                Index
              </div>
            </div>
          </a>
        
        
          
          <a href="10-414%20Homework%E7%AC%94%E8%AE%B0.html" class="md-footer__link md-footer__link--next" aria-label="下一页: Homework Notes">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                下一页
              </span>
              <div class="md-ellipsis">
                Homework Notes
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.tabs", "navigation.indexes", "navigation.tabs.sticky", "navigation.tracking", "toc.follow", "content.action.edit", "navigation.footer", "navigation.path", "navigation.top", "search.suggest", "search.highlight", "search.share"], "search": "../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
        <script src="../../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>